---
title: "Fast approximate Bayesian inference for the Naomi model"
author: "Adam Howes"
date: "June 2023"
output:
  beamer_presentation:
    latex_engine: pdflatex
    highlight: haddock
    fig_width: 7
    fig_height: 3
    includes:
      in_header: preamble.tex
subtitle: Machine Learning and Global Health Network
bibliography: citations.bib
institute: Imperial College London
---

# Doing precision public health requires granular data

<!-- https://github.com/athowes/elgm-inf/blob/fd4b0bb6b9ba93ac277de7aad47a94581e8fc900/src/docs_18-04-23-explainer/18-04-23-explainer.Rmd -->

1. $\color{lolit}{\text{The}}$ right interventions
2. $\color{lolit}{\text{in the}}$ right place
3. $\color{lolit}{\text{to the}}$ right populations
4. $\color{lolit}{\text{at the}}$ right time

#

```{r, echo=FALSE, fig.cap="Naomi is a district-level model of HIV indicators.", out.width = '90%'}
knitr::include_graphics("depends/naomi_results.png")
```

#

```{r, echo=FALSE, fig.cap="Web interface promotes data ownership, data use, and data quality. From \\texttt{https://naomi.unaids.org/.}", out.width = '70%'}
knitr::include_graphics("naomi_user.png")
```

# Better estimates by triangulating information

<!-- https://github.com/athowes/elgm-inf/blob/fd4b0bb6b9ba93ac277de7aad47a94581e8fc900/src/docs_18-04-23-explainer/18-04-23-explainer.Rmd -->

1. Household surveys $\color{lolit}{\text{infreqent, but gold-standard}}$
2. Antenatal care clinic data $\color{lolit}{\text{frequent, only for pregnant women}}$
3. Treatment service provision data $\color{lolit}{\text{frequent, but hard to interpret}}$ 

# Put together, it's a challenging Bayesian inference problem

We want our inference procedure to be

1. Fast $\color{lolit}{\text{enough for interactive review of estimates}}$
2. Accurate $\color{lolit}{\text{enough for precision public health}}$
3. Flexible $\color{lolit}{\text{enough for compatibility with a complex model}}$

# The model has a big Gaussian latent field

# Approximate the marginal posterior of $\x$ by a Gaussian

Given hyperparameters $\btheta$ we compute this as

$$
\tilde{p}_{\texttt{G}}(\x \, | \, \btheta, \y) = \mathcal{N}(\hat \x(\btheta), \hat{\mathbf{H}} (\btheta))
$$

\lolit{

If you input $\theta_1, \ldots, \theta_{24}$ then it'll return a 467 length mean vector $\x(\btheta)$ and $467 \times 467$ length covariance matrix $\hat{\mathbf{H}} (\btheta)$

}

# Get in touch to chat about

1. Further directions for this research $\color{lolit}{\text{including my suggestions for short masters or PhD projects}}$  
2. Impactful academic or industry jobs using Bayesian statistics $\color{lolit}{\text{to begin around the end of this year when I graduate (I hope!)}}$  

# For more information

* Code and notebooks: `github.com/athowes/elgm-inf`
* Working paper on the way^[For sufficiently vague definition of "on the way"]: Fast approximate Bayesian inference for small-area estimation of HIV indicators using the Naomi model $\color{lolit}{\text{Adam Howes, Alex Stringer, Seth Flaxman, Jeff Eaton}}$  

# 

```{r, echo=FALSE, fig.cap="Much of this work done in Waterloo, Canada visiting Alex Stringer last fall! Would definitely recommend the SAS department.", out.width = '60%'}
knitr::include_graphics("uw.png")
```

# References {.allowframebreaks}

<!-- # Latent Gaussian models -->

<!-- * In an LGM [@rue2009approximate] the conditional mean depends on exactly one structured additive predictor -->
<!-- \begin{align*} -->
<!-- y_i &\sim p(y_i \, | \, \eta_i, \theta_1), \quad i \in [n]\\ -->
<!-- \mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\ -->
<!-- \eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), -->
<!-- \end{align*} -->
<!-- * $\beta_0$, $\{\beta_j\}$ and $\{f_k(\cdot)\}$ have Gaussian priors and can be collected into the latent field $\x$ -->

<!-- # Extended latent Gaussian models -->

<!-- * ELGM remove this requirement such that -->
<!-- $$ -->
<!-- \mu_i = g_i(\eta_{\mathcal{J}_i}) -->
<!-- $$ -->
<!-- where $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$ and $\mathcal{J}_i$ is some set of indices -->
<!-- * Let $\dim(\eta) = N_n$ with $\mathcal{J}_i \subseteq \{1, \ldots, N_n\}$ -->
<!--   * $N_n < n$: more data points than structured additive predictors -->
<!--   * $N_n = n$: as many data points as structured additive predictors (LGM case) -->
<!--   * $N_n > n$: fewer data points than structured additive predictors -->
<!-- * The $g_i$ allow for a higher degree of non-linearity in the model -->

<!-- # Why is Naomi an ELGM? -->

<!-- 1. Incidence depends on adult prevalence and coverage -->
<!-- 2. Incidence linked non-linearly to recent infection -->
<!-- 3. ANC offset from household survey -->
<!-- 4. ART coverage and recent infection are products -->
<!-- 5. Aggregation of finer processes -->
<!-- 6. ART attendance uses a multinomial -->
<!-- 7. Multiple link functions -->

<!-- # 1. Incidence depends on adult prevalence and coverage -->

<!-- * Linear predictor for incidence contains aggregated prevalence and coverage -->
<!-- $$ -->
<!-- \log(\lambda_{x, s, a}) = \beta_0^\lambda + \beta_S^{\lambda, s = \text{M}} + \log(\rho_{x}^{\text{15-49}}) + \log(1 - \omega \cdot \alpha_{x}^{\text{15-49}}) + u_x^\lambda + \eta_{R_x, s, a}^\lambda. -->
<!-- $$ -->
<!-- * Here $\log(\rho_{x}^{\text{15-49}})$ and $\log(1 - \omega \cdot \alpha_{x}^{\text{15-49}})$ are not going to be Gaussian -->

<!-- # 2. Incidence and prevalence linked to recent infection -->

<!-- $$ -->
<!-- \kappa_{x, s, a} = 1 - \exp \left(- \lambda_{x, s, a} \cdot \frac{1 - \rho_{x, s, a}}{\rho_{x, s, a}} \cdot (\Omega_T - \beta_T ) - \beta_T \right) -->
<!-- $$ -->

<!-- * Here $\Omega_T$ and $\beta_T$ are Gaussian and have strong priors depending on the particular survey -->

<!-- # 3. ANC offset from household survey -->

<!-- * Linear predictors for ANC indicators contain nested in them the linear predictors for household survey indicators -->
<!-- \begin{align*} -->
<!-- \text{logit}(\rho_{x, a}^{\text{ANC}}) &= \text{logit}(\rho_{x, F, a}) + \beta^{\rho^{\text{ANC}}} + u_x^{\rho^{\text{ANC}}} + \eta_{R_x, a}^{\rho^{\text{ANC}}}, \\ -->
<!-- \text{logit}(\alpha_{x, a}^{\text{ANC}}) &= \text{logit}(\alpha_{x, F, a}) + \beta^{\alpha^{\text{ANC}}} + u_x^{\alpha^{\text{ANC}}} + \eta_{R_x, a}^{\alpha^{\text{ANC}}}.  -->
<!-- \end{align*} -->
<!-- * Here $\text{logit}(\rho_{x, F, a})$ and $\text{logit}(\alpha_{x, F, a})$ *are* Gaussian, but we have dependency of $\mu_i$ on two $\eta_i$ -->

<!-- # 3. ANC offset from household survey -->

<!-- * Note that `R-INLA` does have the `copy` feature $\eta^\star = A \eta$ where $A$ is $n \times n$^[I've also seen it claimed that it could be $m \times n$ where $m \neq n$?] -->
<!-- $$ -->
<!-- \begin{pmatrix} -->
<!-- \eta_1^\star \\ -->
<!-- \eta_2^\star -->
<!-- \end{pmatrix} -->
<!-- = -->
<!-- \begin{pmatrix} -->
<!-- 1 & 1 \\ -->
<!-- 0 & 1 -->
<!-- \end{pmatrix} -->
<!-- \begin{pmatrix} -->
<!-- \eta_1 \\ -->
<!-- \eta_2 -->
<!-- \end{pmatrix} -->
<!-- = -->
<!-- \begin{pmatrix} -->
<!-- \eta_1 + \eta_2 \\ -->
<!-- \eta_2 -->
<!-- \end{pmatrix} -->
<!-- $$ -->
<!-- * By having effects only apply to a subset of indices (`idx = c(NA, 1)` say) perhaps it can work -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->

<!-- Warning! -->

<!-- \begin{enumerate} -->
<!--     \item Is an LGM $\nRightarrow$ can be fit with \texttt{R-INLA} -->
<!--     \item Can be fit with \texttt{R-INLA} $\nRightarrow$ is an LGM -->
<!-- \end{enumerate} -->

<!-- \end{tcolorbox} -->
<!-- \end{center} -->

<!-- # 4. ART coverage and recent infection are products -->

<!-- * In the household survey, say, individuals who are taking ART or have been recently infected must be HIV positive -->
<!-- \begin{align*} -->
<!-- y^{\hat \alpha}_{x, s, a} &\sim \text{xBin}(m_{x, s, a}, \rho_{x, s, a} \cdot \alpha_{x, s, a}), \\ -->
<!-- y^{\hat \kappa}_{x, s, a} &\sim \text{xBin}(m_{x, s, a}, \rho_{x, s, a} \cdot \kappa_{x, s, a}). -->
<!-- \end{align*} -->
<!-- * $\text{logit}(\rho_{x, s, a})$ and $\text{logit}(\alpha_{x, s, a})$ are Gaussian, but we're taking a product here -->
<!-- * $\kappa_{x, s, a}$ is more complicated: a function of incidence, prevalence, mean duration of recent infection and false recent ratio -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->
<!-- Warning! These equations as written do not appear in Naomi: instead there are aggregated versions, as we will see next slide. -->
<!-- \end{tcolorbox} -->
<!-- \end{center} -->

<!-- # 5. Aggregation of finer processes -->

<!-- * There are many instances of likelihoods being placed on aggregate quantities -->
<!-- \begin{align*} -->
<!-- y^{\hat \theta}_{I} &\sim \text{xBin}(m^{\hat \theta}_{I}, \theta_{I}), \\ -->
<!-- \rho_{I} &= \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}. -->
<!-- \end{align*} -->
<!-- * Here we have $|I|$ linear predictors being informed by one observation -->
<!-- * Known as disaggregation regression -->

<!-- # 6. ART attendance uses the multinomial -->

<!-- # 7. Multiple link functions -->

<!-- * The Naomi model uses both $\text{logit}$ and $\log$ (inverse) link functions (not even considering the constructed quasi-link functions) -->
<!-- * For LGMs there is only one $g$, whereas ELGMs allow $g_i$ -->
<!-- * In `R-INLA` it is possible for `y` to be a matrix where each column contains observations with shared likelihood family (and hyperparameters) and `family = c("family1", "family2", ...)` -->

<!-- # Two deterministic methods -->

<!-- * We use two deterministic^[In contrast to the most famous approximate Bayesian inference method, Markov chain Monte Carlo, which is fundamentally stochastic.] methods to approximate our integral -->

<!-- 1. The Laplace approximation -->
<!-- 2. Quadrature -->

<!-- # The Laplace approximation -->

<!-- Pretend $p(\vartheta \, | \, y)$ is Gaussian -->

<!-- * Mode $\hat \vartheta = \argmax_\vartheta \log p(y, \vartheta)$ -->
<!-- * Hessian $H(\hat \vartheta) = - \partial_\vartheta^2 \log p(y, \vartheta) \rvert_{\vartheta = \hat \vartheta}$ -->
<!-- * Gaussian approximation $\implies \tilde p_{\texttt{G}}(\vartheta \, | \, y) = \mathcal{N}(\vartheta \, | \, \hat \vartheta, H(\hat \vartheta)^{-1})$ -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="A Gamma prior with $a = 3$ and $b = 1$."} -->
<!-- cbpalette <- c("#56B4E9","#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999") -->

<!-- a <- 3 -->
<!-- b <- 1 -->

<!-- prior <- ggplot(data = data.frame(x = c(0, 10)), aes(x)) + -->
<!--   stat_function(fun = dgamma, n = 500, args = list(shape = a, rate = b), col = cbpalette[1]) + -->
<!--   annotate("text", x = 6, y = 0.15, label = "Gamma(3, 1)", col = cbpalette[1], size = 5) + -->
<!--   labs(x = "", y = "") + -->
<!--   theme_minimal() -->

<!-- prior -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="Draw 3 points from $\\text{Poisson}(3)$, then compute the posterior."} -->
<!-- set.seed(2) -->
<!-- y <- rpois(3, lambda = 2) -->

<!-- posterior <- prior + -->
<!--   geom_point(data = data.frame(x = y, y = 0), aes(x = x, y = y), inherit.aes = FALSE, alpha = 0.7, size = 2) + -->
<!--   stat_function(data = data.frame(x = c(0, 10)), aes(x), fun = dgamma, n = 500, args = list(shape = a + sum(y), rate = b + length(y)), col = cbpalette[2]) + -->
<!--   annotate("text", x = 5, y = 0.25, label = "Gamma(9, 4)", col = cbpalette[2], size = 5) -->

<!-- posterior -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE} -->
<!-- fn <- function(x) dgamma(x, a + sum(y), b + length(y), log = TRUE) -->

<!-- # Here we are using numerical derivatives -->
<!-- ff <- list( -->
<!--   fn = fn, -->
<!--   gr = function(x) numDeriv::grad(fn, x), -->
<!--   he = function(x) numDeriv::hessian(fn, x) -->
<!-- ) -->

<!-- opt_bfgs <- aghq::optimize_theta( -->
<!--   ff, 1, control = aghq::default_control(method = "BFGS") -->
<!-- ) -->
<!-- ``` -->

<!-- # Laplace approximation -->

<!-- ```{r} -->
<!-- laplace <- posterior + -->
<!--   stat_function( -->
<!--     data = data.frame(x = c(0, 10)), -->
<!--     aes(x), -->
<!--     fun = dnorm, -->
<!--     n = 500, -->
<!--     args = list(mean = opt_bfgs$mode, sd = sqrt(1 / opt_bfgs$hessian)), -->
<!--     col = cbpalette[3] -->
<!--   ) -->
<!-- ``` -->

<!-- # Laplace approximation -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="The Laplace approximation in this case is good near the mode but not in the tails."} -->
<!-- laplace + -->
<!--   annotate("text", x = 5.5, y = 0.35, label = "Laplace approximation", col = cbpalette[3], size = 5) -->
<!-- ``` -->

<!-- # Computation of the Laplace approximation -->

<!-- * This computation was simple, and involved -->

<!-- 1. Optimising a function -->
<!-- 2. Taking the mode and the Hessian at the mode -->

<!-- # The marginal Laplace approximation -->

<!-- * If we don't want to pretend the whole posterior distribution is Gaussian, another option is to pretend \textcolor{hilit}{some of its marginals are} -->
<!-- * This is how `TMB` works: it's up to the user to choose which parameters should be Gaussian using the `random` option -->

<!-- # Which parameters should we treat as Gaussian? -->

<!-- * In spatio-temporal statistics we have data indexed by space and time -->
<!-- * We use random effects also indexed by space and time to model this data -->
<!-- * Spatio-temporal fields can be big -->
<!-- * Willing to make assumptions about how things vary over spacetime -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->

<!-- $\implies$ treat the latent field parameters as Gaussian! That's the majority of the integral done. -->

<!-- \end{tcolorbox} -->
<!-- \end{center} -->

<!-- # What about the hyperparameters? -->

<!-- * `TMB` uses optimisation to find the hyperparameters which maximise the marginal Laplace approximation -->
<!-- * This is the "outer" optimisation loop, where the "inner" is for computation of the Gaussian distribution -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->

<!-- Inference for the latent field is based on a single value of the hyperparameters (the mode) -- so called empirical Bayes $\implies$ no uncertainty in the hyperparameters taken into account! How can you sleep at night. -->

<!-- \end{tcolorbox} -->
<!-- \end{center} -->

<!-- # Quadrature -->

<!-- * This brings us to our other method for solving integrals deterministically -->
<!-- * Say we have a function, then quadrature has two ingredients -->

<!-- 1. Nodes: points to evaluate the function at -->
<!-- 2. Weights: importance of function evaluation at that point -->

<!-- * You evaluate the function at the nodes, then do a weighted sum to calculate your integral -->

<!-- # Trapezoid rule example -->

<!-- * Let's compute $\int_0^\pi \sin(x) \text{d}x = 2$ using quadrature -->

<!-- ```{r} -->
<!-- trapezoid_rule <- function(x, spacing) { -->
<!--   # Assumes nodes are evenly spaced -->
<!--   w <- rep(spacing, length(x)) # Weights given by space between nodes -->
<!--   w[1] <- w[1] / 2 # Apart from the first which is halved -->
<!--   w[length(x)] <- w[length(x)] / 2 # And the last, also halved -->
<!--   sum(w * x) # Compute the weighted sum -->
<!-- } -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="With 10 nodes it's 0.02 off."} -->
<!-- finegrid <- seq(0, pi, length.out = 1000) -->

<!-- plot <- data.frame(x = finegrid, y = sin(finegrid)) %>% -->
<!--   ggplot(aes(x = x, y = y)) + -->
<!--    geom_line(col = cbpalette[1]) + -->
<!--    theme_minimal() + -->
<!--    labs(x = "x", y = "sin(x)") -->

<!-- trapezoid_plot <- function(N) { -->
<!--   grid <- seq(0, pi, length.out = N) -->
<!--   int <- trapezoid_rule(x = sin(grid), spacing = grid[2] - grid[1]) -->

<!--   plot +  -->
<!--     geom_bar( -->
<!--       data = data.frame(x = grid, y = sin(grid)), -->
<!--       aes(x = x, y = y), alpha = 0.7, stat = "identity", -->
<!--       inherit.aes = FALSE, fill = cbpalette[2]) + -->
<!--     theme_minimal() + -->
<!--     labs( -->
<!--       subtitle = paste0("Number of nodes: ", N, "\nTrapezoid rule estimate: ", round(int, 3), "\nTruth: 2"), -->
<!--       x = "x", y = "sin(x)" -->
<!--     ) -->
<!-- } -->

<!-- trapezoid_plot(N = 10) -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE, fig.cap="With 30 nodes it's 0.002 off."} -->
<!-- trapezoid_plot(N = 30) -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE, fig.cap="With 100 nodes it's pretty much correct."} -->
<!-- trapezoid_plot(N = 100) -->
<!-- ``` -->

<!-- # Adaptive Gauss-Hermite quadrature -->

<!-- * \textcolor{hilit}{Gauss-Hermite quadrature} is a method for picking nodes and weights based on the theory of polynomial interpolation -->
<!-- * It works especially well for statistical problems where the integrand looks like something multiplied by a Gaussian distribution -->
<!-- * The \textcolor{hilit}{adaptive} part means the nodes and weights are changed depending on the integrand -- this makes sense, especially when the integrand is also a function of the data -->
<!-- * Implemented by the `aghq` package [@stringer2021implementing] -->

<!-- #  -->

<!-- ```{r, echo=FALSE, message=FALSE, fig.cap="Unadapted points in two dimensions with $k = 3$."} -->
<!-- mu <- c(1, 1.5) -->
<!-- cov <- matrix(c(2, 1, 1, 1), ncol = 2) -->

<!-- obj <- function(theta) { -->
<!--   mvtnorm::dmvnorm(theta, mean = mu, sigma = cov) -->
<!-- } -->

<!-- grid <- expand.grid( -->
<!--   theta1 = seq(-2, 5, length.out = 700), -->
<!--   theta2 = seq(-2, 5, length.out = 700) -->
<!-- ) -->

<!-- ground_truth <- cbind(grid, pdf = obj(grid)) -->

<!-- plot0 <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) + -->
<!--   geom_contour(col = cbpalette[1]) + -->
<!--   coord_fixed(xlim = c(-2, 4.5), ylim = c(-2, 4.5), ratio = 1) + -->
<!--   labs(x = "", y = "") + -->
<!--   theme_minimal() -->

<!-- gg <- mvQuad::createNIGrid(2, "GHe", 3) -->

<!-- add_points <- function(plot0, gg) { -->
<!--   plot0 + -->
<!--     geom_point( -->
<!--       data = mvQuad::getNodes(gg) %>% -->
<!--               as.data.frame() %>% -->
<!--               mutate(weights = mvQuad::getWeights(gg)),  -->
<!--       aes(x = V1, y = V2, size = weights), -->
<!--       alpha = 0.8, -->
<!--       col = cbpalette[2], -->
<!--       inherit.aes = FALSE -->
<!--     ) + -->
<!--     scale_size_continuous(range = c(1, 2)) -->
<!-- } -->

<!-- add_points(plot0, gg) + -->
<!--   labs(size = "Weight", caption = "") -->
<!-- ``` -->

<!-- # -->

<!-- ```{r, echo=FALSE, message=FALSE, fig.cap="Add the mean $z + \\hat \\theta$."} -->
<!-- gg2 <- gg -->
<!-- mvQuad::rescale(gg2, m = mu, C = diag(c(1, 1)), dec.type = 2) -->

<!-- add_points(plot0, gg2) + -->
<!--   labs(size = "Weight", caption = "") -->
<!-- ``` -->

<!-- # -->

<!-- ```{r, echo=FALSE, message=FALSE, fig.cap="First option: rotate by the lower Cholesky $Lz + \\hat \\theta$."} -->
<!-- gg3 <- gg -->
<!-- mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 2) -->

<!-- add_points(plot0, gg3) + -->
<!--   labs(size = "Weight", caption = "") -->
<!-- ``` -->

<!-- # -->

<!-- ```{r, echo=FALSE, message=FALSE, fig.cap="Second option: rotate using the eigendecomposition $E \\Lambda^{1/2} z + \\hat \\theta$."} -->
<!-- gg3 <- gg -->
<!-- mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 1) -->

<!-- add_points(plot0, gg3) + -->
<!--   labs(size = "Weight", caption = "") -->
<!-- ``` -->
