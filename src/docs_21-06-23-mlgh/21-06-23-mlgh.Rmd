---
title: "Fast approximate Bayesian inference for the Naomi model"
author: "Adam Howes"
date: "June 2023"
output:
  beamer_presentation:
    latex_engine: pdflatex
    highlight: haddock
    fig_width: 7
    fig_height: 3
    includes:
      in_header: preamble.tex
subtitle: Machine Learning and Global Health Network
bibliography: citations.bib
institute: Imperial College London
---

# Doing precision public health requires granular data

<!-- https://github.com/athowes/elgm-inf/blob/fd4b0bb6b9ba93ac277de7aad47a94581e8fc900/src/docs_18-04-23-explainer/18-04-23-explainer.Rmd -->

1. $\color{lolit}{\text{The}}$ right interventions
2. $\color{lolit}{\text{in the}}$ right place
3. $\color{lolit}{\text{to the}}$ right populations
4. $\color{lolit}{\text{at the}}$ right time

#

```{r, echo=FALSE, fig.cap="Naomi is a district-level model of HIV indicators, like prevalence, treatment coverage, and incidence.", out.width = '90%'}
knitr::include_graphics("depends/naomi_results.png")
```

#

```{r, echo=FALSE, fig.cap="Estimate generation via a web interface promotes data ownership, data use, and data quality. From \\texttt{https://naomi.unaids.org/.}", out.width = '70%'}
knitr::include_graphics("naomi_user.png")
```

# Better estimates by integration information from many sources

<!-- https://github.com/athowes/elgm-inf/blob/fd4b0bb6b9ba93ac277de7aad47a94581e8fc900/src/docs_18-04-23-explainer/18-04-23-explainer.Rmd -->

1. Household surveys $\color{lolit}{\text{infreqent, but gold-standard}}$
2. Antenatal care clinic data $\color{lolit}{\text{frequent, only for pregnant women}}$
3. Treatment service provision data $\color{lolit}{\text{frequent, but hard to interpret}}$ 

# Put together, it's a challenging Bayesian inference problem

We want our inference procedure to be

1. Fast $\color{lolit}{\text{enough for interactive review of estimates}}$
2. Accurate $\color{lolit}{\text{enough for precision public health}}$
3. Flexible $\color{lolit}{\text{enough for compatibility with a complex model}}$

# The model has a big, structured, Gaussian latent field $\x$

* Fixed effects $\color{lolit}{\text{Gaussian}}$
* Age structure $\color{lolit}{\text{AR1}}$
* Spatial structure $\color{lolit}{\text{IID, ICAR, BYM2}}$

Concatenate together as $\x$, length 467

# Smaller, non-Gaussian, hyperparameters $\btheta$

* Standard deviations $\color{lolit}{\text{Half-Gaussian}}$
* BYM2 proportion parameters $\color{lolit}{\text{Beta}}$
* AR1 autocorrelation parameters $\color{lolit}{\text{Uniform}}$

Concatenate together as $\btheta$, length 24

# Approximate the marginal posterior of $\x$ by a Gaussian

Given hyperparameters $\btheta$ we compute this as

$$
\tilde{p}_{\texttt{G}}(\x \, | \, \btheta, \y) = \mathcal{N}(\hat \x(\btheta), \hat{\mathbf{H}} (\btheta))
$$

\lolit{

If you input 24 length $\btheta$ then it'll return a 467 length mean vector $\hat \x(\btheta)$ and $467 \times 467$ length covariance matrix $\hat{\mathbf{H}} (\btheta)$.
Mean calculated using gradient based optimisation.
Gradients, and Hessian, obtained using automatic differentiation.

}

# Use adaptive Gauss-Hermite quadrature to integrate over $\btheta$

Quadrature method based on the theory of polynomial interpolation which:

1. Works well when the integrand looks like a polynomial times a Gaussian
2. Adapts to the particular integrand based on the mode and Hessian
3. Is implemented by the `aghq` package [@stringer2021implementing]

#

```{r, echo=FALSE, message=FALSE, fig.cap="Unadapted Gauss-Hermite nodes in two dimensions with $k = 3$."}
figA1
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Add the mode $\\mathbf{z} + \\hat \\btheta$."}
figA2
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="You could rotate by the lower Cholesky $\\mathbf{Lz} + \\hat \\btheta$."}
figA3
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Or you could rotate using the eigendecomposition $\\mathbf{E} \\bm{\\Lambda}^{1/2} \\mathbf{z} + \\hat \\btheta$."}
figA4
```

# 24 hyperparameters is too many for a dense grid

$k = 3$ points in 24 dimensions is not feasible
$$
3 \times 3 \times \cdots \times 3 = 3^{24}
$$
So we need to find something smaller $\color{lolit}{\text{(that still does a good job!)}}$

#

```{r, echo=FALSE, message=FALSE, fig.cap="An obvious thing to try is only keeping points from the largest eigenvectors: we call this PCA-AGHQ. Corresponds to variable choice of $k$ by dimension."}
figA5
```

#

```{r, echo=FALSE, message=FALSE, fig.cap=".", out.width = '90%'}
knitr::include_graphics("depends/tv-plot.png")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap=".", out.width = '90%'}
knitr::include_graphics("depends/reduced-rank-plot.png")
```

# Yes, but did it work?

Run NUTS^[For 3 days!] as gold-standard, then compare to TMB to PCA-AGHQ using:

1. Marginal distributions $\color{lolit}{\text{point estimates, ECDF e.g. KS or AD}}$  
2. Joint distributions $\color{lolit}{\text{PSIS, MMD}}$ 
3. Policy relevant outcomes $\color{lolit}{\text{second 90, high incidence}}$  

#

```{r, echo=FALSE, message=FALSE, fig.cap="PCA-AGHQ modestly improves estimation of the posterior mean.", out.width = '80%'}
knitr::include_graphics("depends/mean.png")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="PCA-AGHQ substantially improves estimation of the posterior standard deviation. TMB systematically underestimates, which you'd expect.", out.width = '80%'}
knitr::include_graphics("depends/sd.png")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Strata probabilities of having greater than 81\\% ART coverage, and as such meeting the second 90 target. Both approximate methods are inaccurate for females.", out.width = '85%'}
knitr::include_graphics("depends/exceedance-second90.png")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Strata probabilities of having greater than 1\\% HIV incidence, and as such being classified high incidence. Again, both approximate methods are inaccurate.", out.width = '85%'}
knitr::include_graphics("depends/exceedance-1inc.png")
```

# Further improvements look possible

1. Fix issues with scaling $\color{lolit}{\text{Logit scaled not uniformly more important than log scaled}}$  
2. Take into account importance for outputs of interest $\color{lolit}{\text{Variance of inputs isn't really what we care about}}$  
3. Take into account marginal skewness $\color{lolit}{\text{The more skewed, the more we should be placing lots of points}}$  

#

```{r, echo=FALSE, message=FALSE, fig.cap="On the real scale, $[0, 1]$ hyperparameters appear to have more marginal variance than $[0, \\infty)$ hyperparameters. This doesn't really make them more important though.", out.width = '80%'}
knitr::include_graphics("depends/marginal-sd.png")
```

# Get in touch to chat about

1. Further directions for this research $\color{lolit}{\text{e.g. suggestions for short masters or PhD projects}}$  
2. Impactful academic or industry jobs using Bayesian statistics $\color{lolit}{\text{to begin around the end of this year when I graduate (hopefully!)}}$

| Method      | Details          |
| ----------- | ---------------- |
| Zulip       | Adam Howes       |
| Email       | `ath19@ic.ac.uk` |
| Calendly    | `adamthowes`     |

# For more information

* Code and notebooks: `github.com/athowes/elgm-inf`
* Working paper on the way^[For sufficiently vague definition of "on the way"], Any early readers greatly appreciated!
  * Fast approximate Bayesian inference for small-area estimation of HIV indicators using the Naomi model $\color{lolit}{\text{Adam Howes, Alex Stringer, Seth Flaxman, Jeff Eaton}}$

# 

```{r, echo=FALSE, fig.cap="Much of this work done in Waterloo, Canada visiting Alex Stringer last fall! Would definitely recommend the SAS department.", out.width = '60%'}
knitr::include_graphics("uw.png")
```

# References {.allowframebreaks}

<!-- # Latent Gaussian models -->

<!-- * In an LGM [@rue2009approximate] the conditional mean depends on exactly one structured additive predictor -->
<!-- \begin{align*} -->
<!-- y_i &\sim p(y_i \, | \, \eta_i, \theta_1), \quad i \in [n]\\ -->
<!-- \mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\ -->
<!-- \eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), -->
<!-- \end{align*} -->
<!-- * $\beta_0$, $\{\beta_j\}$ and $\{f_k(\cdot)\}$ have Gaussian priors and can be collected into the latent field $\x$ -->

<!-- # Extended latent Gaussian models -->

<!-- * ELGM remove this requirement such that -->
<!-- $$ -->
<!-- \mu_i = g_i(\eta_{\mathcal{J}_i}) -->
<!-- $$ -->
<!-- where $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$ and $\mathcal{J}_i$ is some set of indices -->
<!-- * Let $\dim(\eta) = N_n$ with $\mathcal{J}_i \subseteq \{1, \ldots, N_n\}$ -->
<!--   * $N_n < n$: more data points than structured additive predictors -->
<!--   * $N_n = n$: as many data points as structured additive predictors (LGM case) -->
<!--   * $N_n > n$: fewer data points than structured additive predictors -->
<!-- * The $g_i$ allow for a higher degree of non-linearity in the model -->

<!-- # Why is Naomi an ELGM? -->

<!-- 1. Incidence depends on adult prevalence and coverage -->
<!-- 2. Incidence linked non-linearly to recent infection -->
<!-- 3. ANC offset from household survey -->
<!-- 4. ART coverage and recent infection are products -->
<!-- 5. Aggregation of finer processes -->
<!-- 6. ART attendance uses a multinomial -->
<!-- 7. Multiple link functions -->

<!-- # 1. Incidence depends on adult prevalence and coverage -->

<!-- * Linear predictor for incidence contains aggregated prevalence and coverage -->
<!-- $$ -->
<!-- \log(\lambda_{x, s, a}) = \beta_0^\lambda + \beta_S^{\lambda, s = \text{M}} + \log(\rho_{x}^{\text{15-49}}) + \log(1 - \omega \cdot \alpha_{x}^{\text{15-49}}) + u_x^\lambda + \eta_{R_x, s, a}^\lambda. -->
<!-- $$ -->
<!-- * Here $\log(\rho_{x}^{\text{15-49}})$ and $\log(1 - \omega \cdot \alpha_{x}^{\text{15-49}})$ are not going to be Gaussian -->

<!-- # 2. Incidence and prevalence linked to recent infection -->

<!-- $$ -->
<!-- \kappa_{x, s, a} = 1 - \exp \left(- \lambda_{x, s, a} \cdot \frac{1 - \rho_{x, s, a}}{\rho_{x, s, a}} \cdot (\Omega_T - \beta_T ) - \beta_T \right) -->
<!-- $$ -->

<!-- * Here $\Omega_T$ and $\beta_T$ are Gaussian and have strong priors depending on the particular survey -->

<!-- # 3. ANC offset from household survey -->

<!-- * Linear predictors for ANC indicators contain nested in them the linear predictors for household survey indicators -->
<!-- \begin{align*} -->
<!-- \text{logit}(\rho_{x, a}^{\text{ANC}}) &= \text{logit}(\rho_{x, F, a}) + \beta^{\rho^{\text{ANC}}} + u_x^{\rho^{\text{ANC}}} + \eta_{R_x, a}^{\rho^{\text{ANC}}}, \\ -->
<!-- \text{logit}(\alpha_{x, a}^{\text{ANC}}) &= \text{logit}(\alpha_{x, F, a}) + \beta^{\alpha^{\text{ANC}}} + u_x^{\alpha^{\text{ANC}}} + \eta_{R_x, a}^{\alpha^{\text{ANC}}}.  -->
<!-- \end{align*} -->
<!-- * Here $\text{logit}(\rho_{x, F, a})$ and $\text{logit}(\alpha_{x, F, a})$ *are* Gaussian, but we have dependency of $\mu_i$ on two $\eta_i$ -->

<!-- # 3. ANC offset from household survey -->

<!-- * Note that `R-INLA` does have the `copy` feature $\eta^\star = A \eta$ where $A$ is $n \times n$^[I've also seen it claimed that it could be $m \times n$ where $m \neq n$?] -->
<!-- $$ -->
<!-- \begin{pmatrix} -->
<!-- \eta_1^\star \\ -->
<!-- \eta_2^\star -->
<!-- \end{pmatrix} -->
<!-- = -->
<!-- \begin{pmatrix} -->
<!-- 1 & 1 \\ -->
<!-- 0 & 1 -->
<!-- \end{pmatrix} -->
<!-- \begin{pmatrix} -->
<!-- \eta_1 \\ -->
<!-- \eta_2 -->
<!-- \end{pmatrix} -->
<!-- = -->
<!-- \begin{pmatrix} -->
<!-- \eta_1 + \eta_2 \\ -->
<!-- \eta_2 -->
<!-- \end{pmatrix} -->
<!-- $$ -->
<!-- * By having effects only apply to a subset of indices (`idx = c(NA, 1)` say) perhaps it can work -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->

<!-- Warning! -->

<!-- \begin{enumerate} -->
<!--     \item Is an LGM $\nRightarrow$ can be fit with \texttt{R-INLA} -->
<!--     \item Can be fit with \texttt{R-INLA} $\nRightarrow$ is an LGM -->
<!-- \end{enumerate} -->

<!-- \end{tcolorbox} -->
<!-- \end{center} -->

<!-- # 4. ART coverage and recent infection are products -->

<!-- * In the household survey, say, individuals who are taking ART or have been recently infected must be HIV positive -->
<!-- \begin{align*} -->
<!-- y^{\hat \alpha}_{x, s, a} &\sim \text{xBin}(m_{x, s, a}, \rho_{x, s, a} \cdot \alpha_{x, s, a}), \\ -->
<!-- y^{\hat \kappa}_{x, s, a} &\sim \text{xBin}(m_{x, s, a}, \rho_{x, s, a} \cdot \kappa_{x, s, a}). -->
<!-- \end{align*} -->
<!-- * $\text{logit}(\rho_{x, s, a})$ and $\text{logit}(\alpha_{x, s, a})$ are Gaussian, but we're taking a product here -->
<!-- * $\kappa_{x, s, a}$ is more complicated: a function of incidence, prevalence, mean duration of recent infection and false recent ratio -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->
<!-- Warning! These equations as written do not appear in Naomi: instead there are aggregated versions, as we will see next slide. -->
<!-- \end{tcolorbox} -->
<!-- \end{center} -->

<!-- # 5. Aggregation of finer processes -->

<!-- * There are many instances of likelihoods being placed on aggregate quantities -->
<!-- \begin{align*} -->
<!-- y^{\hat \theta}_{I} &\sim \text{xBin}(m^{\hat \theta}_{I}, \theta_{I}), \\ -->
<!-- \rho_{I} &= \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}. -->
<!-- \end{align*} -->
<!-- * Here we have $|I|$ linear predictors being informed by one observation -->
<!-- * Known as disaggregation regression -->

<!-- # 6. ART attendance uses the multinomial -->

<!-- # 7. Multiple link functions -->

<!-- * The Naomi model uses both $\text{logit}$ and $\log$ (inverse) link functions (not even considering the constructed quasi-link functions) -->
<!-- * For LGMs there is only one $g$, whereas ELGMs allow $g_i$ -->
<!-- * In `R-INLA` it is possible for `y` to be a matrix where each column contains observations with shared likelihood family (and hyperparameters) and `family = c("family1", "family2", ...)` -->

<!-- # The Laplace approximation -->

<!-- Pretend $p(\vartheta \, | \, y)$ is Gaussian -->

<!-- * Mode $\hat \vartheta = \argmax_\vartheta \log p(y, \vartheta)$ -->
<!-- * Hessian $H(\hat \vartheta) = - \partial_\vartheta^2 \log p(y, \vartheta) \rvert_{\vartheta = \hat \vartheta}$ -->
<!-- * Gaussian approximation $\implies \tilde p_{\texttt{G}}(\vartheta \, | \, y) = \mathcal{N}(\vartheta \, | \, \hat \vartheta, H(\hat \vartheta)^{-1})$ -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="A Gamma prior with $a = 3$ and $b = 1$."} -->
<!-- cbpalette <- c("#56B4E9","#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999") -->

<!-- a <- 3 -->
<!-- b <- 1 -->

<!-- prior <- ggplot(data = data.frame(x = c(0, 10)), aes(x)) + -->
<!--   stat_function(fun = dgamma, n = 500, args = list(shape = a, rate = b), col = cbpalette[1]) + -->
<!--   annotate("text", x = 6, y = 0.15, label = "Gamma(3, 1)", col = cbpalette[1], size = 5) + -->
<!--   labs(x = "", y = "") + -->
<!--   theme_minimal() -->

<!-- prior -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="Draw 3 points from $\\text{Poisson}(3)$, then compute the posterior."} -->
<!-- set.seed(2) -->
<!-- y <- rpois(3, lambda = 2) -->

<!-- posterior <- prior + -->
<!--   geom_point(data = data.frame(x = y, y = 0), aes(x = x, y = y), inherit.aes = FALSE, alpha = 0.7, size = 2) + -->
<!--   stat_function(data = data.frame(x = c(0, 10)), aes(x), fun = dgamma, n = 500, args = list(shape = a + sum(y), rate = b + length(y)), col = cbpalette[2]) + -->
<!--   annotate("text", x = 5, y = 0.25, label = "Gamma(9, 4)", col = cbpalette[2], size = 5) -->

<!-- posterior -->
<!-- ``` -->

<!-- # -->

<!-- ```{r message=FALSE} -->
<!-- fn <- function(x) dgamma(x, a + sum(y), b + length(y), log = TRUE) -->

<!-- # Here we are using numerical derivatives -->
<!-- ff <- list( -->
<!--   fn = fn, -->
<!--   gr = function(x) numDeriv::grad(fn, x), -->
<!--   he = function(x) numDeriv::hessian(fn, x) -->
<!-- ) -->

<!-- opt_bfgs <- aghq::optimize_theta( -->
<!--   ff, 1, control = aghq::default_control(method = "BFGS") -->
<!-- ) -->
<!-- ``` -->

<!-- # Laplace approximation -->

<!-- ```{r} -->
<!-- laplace <- posterior + -->
<!--   stat_function( -->
<!--     data = data.frame(x = c(0, 10)), -->
<!--     aes(x), -->
<!--     fun = dnorm, -->
<!--     n = 500, -->
<!--     args = list(mean = opt_bfgs$mode, sd = sqrt(1 / opt_bfgs$hessian)), -->
<!--     col = cbpalette[3] -->
<!--   ) -->
<!-- ``` -->

<!-- # Laplace approximation -->

<!-- ```{r message=FALSE, echo=FALSE,  fig.cap="The Laplace approximation in this case is good near the mode but not in the tails."} -->
<!-- laplace + -->
<!--   annotate("text", x = 5.5, y = 0.35, label = "Laplace approximation", col = cbpalette[3], size = 5) -->
<!-- ``` -->

<!-- # Computation of the Laplace approximation -->

<!-- * This computation was simple, and involved -->

<!-- 1. Optimising a function -->
<!-- 2. Taking the mode and the Hessian at the mode -->

<!-- # The marginal Laplace approximation -->

<!-- * If we don't want to pretend the whole posterior distribution is Gaussian, another option is to pretend \textcolor{hilit}{some of its marginals are} -->
<!-- * This is how `TMB` works: it's up to the user to choose which parameters should be Gaussian using the `random` option -->

<!-- # What about the hyperparameters? -->

<!-- * `TMB` uses optimisation to find the hyperparameters which maximise the marginal Laplace approximation -->
<!-- * This is the "outer" optimisation loop, where the "inner" is for computation of the Gaussian distribution -->

<!-- \begin{center} -->
<!-- \begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}] -->

<!-- Inference for the latent field is based on a single value of the hyperparameters (the mode) -- so called empirical Bayes $\implies$ no uncertainty in the hyperparameters taken into account! How can you sleep at night. -->

<!-- \end{tcolorbox} -->
<!-- \end{center} -->
