---
title: "Epilepsy example"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
bibliography: citations.bib
abstract: |
  **Background** @rue2009approximate use the epilespy example from the OpenBUGs manual as demonstration.
  
  **Task** We implement the epilepsy example in Stan, `R-INLA`, `TMB`, `glmmTMB`, `tmbstan` and `aghq`. We then compare the results of the inference methods.
---

```{r setup, class.source = 'fold-hide'}
knitr::opts_chunk$set(
  cache = TRUE,
  autodep = TRUE,
  cache.lazy = FALSE,
  cache.comments = FALSE
)
options(scipen = 999)
cbpalette <- multi.utils::cbpalette()
library(tidyverse)

rstan::rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

# Background

## Model description

Section 5.2. of @rue2009approximate demonstrates the application of `R-INLA` to a generalised linear mixed model.
The data, previously included as an example in the OpenBUGs manual, concern the results of a clinical trial for a new epilepsy drug.
Patients $i = 1, \ldots, 59$ are each either assigned the new drug $\texttt{Trt}_i = 1$ or placebo $\texttt{Trt}_i = 0$.
Each patient makes four visits the clinic $j = 1, \ldots, 4$.
The observations $y_{ij}$ give the number of seizures of the $i$th person in the two weeks preceeding their $j$th visit to the clinic.
The covariates are age $\texttt{Age}_i$, baseline seizure counts $\texttt{Base}_i$ and an indicator for the final clinic visit $\texttt{V}_4$, which are all centered.
The observations are modelling using a Poisson distribution $y_{ij} \sim \text{Poisson}(e^{\eta_{ij}})$ with linear predictor 
\begin{align*}
  \eta_{ij}       &= \beta_0 + \beta_\texttt{Base} \log(\texttt{Baseline}_j / 4) + \beta_\texttt{Trt} \texttt{Trt}_i +
                     \beta_{\texttt{Trt} \times \texttt{Base}} \texttt{Trt}_i \times \log(\texttt{Baseline}_j / 4) \\ 
                  &+ \beta_\texttt{Age} \log(\texttt{Age}_i) + \beta_{\texttt{V}_4} {\texttt{V}_4}_j +
                     \epsilon_i + \nu_{ij}, \quad i=1:59, \quad j=1:4,
\end{align*}
where the prior on each of the regression parameters is $\mathcal{N}(0, 100^2)$.
The random effects are IID $\epsilon_i \sim \mathcal{N}(0, 1/\tau_\epsilon)$ and $\nu_{ij} \sim \mathcal{N}(0, 1/\tau_\nu)$ with precision priors $\tau_\epsilon, \tau_\nu \sim \Gamma(0.001, 0.001)$.

## Data preparation

The data is available within `R-INLA` as follows:

```{r}
data(Epil, package = "INLA")
head(Epil)
```

We modify `Epil` by centering all of the covariates:

```{r}
center <- function(x) (x - mean(x))

Epil <- Epil %>%
  mutate(CTrt    = center(Trt),
         ClBase4 = center(log(Base/4)),
         CV4     = center(V4),
         ClAge   = center(log(Age)),
         CBT     = center(Trt * log(Base/4)))
```

Implement the model based upon a response variable of length $N \times J$, where $N$ is the number of patients and $J$ is the number of visits to the clinic, and a model matrix with $K$ predictors (including the intercept term):

```{r}
N <- 59
J <- 4
K <- 6
X <- model.matrix(formula(~ 1 + CTrt + ClBase4 + CV4 + ClAge + CBT), data = Epil)
y <- Epil$y
```

For the individual specific random effect $\epsilon_i$ we use a transformation matrix `E` which repeats elements of vector of length `N` each `J` times.

```{r}
make_epsilon_matrix <- function(N, J) {
  t(outer(1:N, 1:(N * J), function(r, c) as.numeric((J*(r - 1) < c) & (c <= J*r))))
}
```

For example, with $N = 3$ and $J = 2$:

```{r}
E <- make_epsilon_matrix(N = 3, J = 2)
E
t(E %*% c(1, 2, 3)) # Same as rep(1:3, 2)
```

Multiplying this matrix $E$ by the vector $\epsilon$ allows it to be directly added to the linear predictor $\eta = \beta X + \nu + E \epsilon$, where $\beta$ is the vector of coefficients and $X$ is the model matrix.

# Inference methods

We consider a range of inference methods, described below.

## Stan

First, Hamiltonian Monte Carlo via the No-U-Turn algorithm, with four Markov chains of `iter = 1000` samples, and `warmup = 100` burn-in:

```{r}
dat <- list(N = N, J = J, K = K, X = X, y = y, E = make_epsilon_matrix(N, J))

fit1 <- stan(
  "epil.stan",
  data = dat,
  chains = 4,
  warmup = 100,
  iter = 1000,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

## `R-INLA`

Both $\tau_\epsilon$ and $\tau_\nu$ have the same $\Gamma(0.001, 0.001)$ prior.
In `R-INLA`, the precision is internally represented as log-precision, therefore we must set a `loggamma` prior:

```{r}
tau_prior <- list(prec = list(
  prior = "loggamma",
  param = c(0.001, 0.001),
  initial = 1,
  fixed = FALSE)
)
```

The variable `Epil$rand` is gives the row number for each entry and the variable `Epil$Ind` gives the patient number.
These variables can be used to define the IID random effects $\nu_{ij}$ and $\epsilon_i$ in `R-INLA` as follows.
The usual R formula notation (e.g. `y ~ x1 + x2`) is used for the rest of the linear predictor.

```{r}
formula <- y ~ 1 + CTrt + ClBase4 + CV4 + ClAge + CBT +
  f(rand, model = "iid", hyper = tau_prior) +  #' Nu random effect
  f(Ind,  model = "iid", hyper = tau_prior)    #' Epsilon random effect
```

We can use various settings for the strategy, which controls the approximation used for the latent field posterior marginals:

```{r}
epil_inla <- function(strat) {
  inla(formula,
       control.fixed = list(mean = 0, prec = 1 / 100^2), #' Beta prior
       family = "poisson",
       data = Epil,
       control.inla = list(strategy = strat),
       control.predictor = list(compute = TRUE))
}

fit2 <- epil_inla(strat = "gaussian")
fit3 <- epil_inla(strat = "simplified.laplace")
fit4 <- epil_inla(strat = "laplace")
```

## `TMB`

The objective function `obj$fn` and its gradient `obj$gn` are a function of only the parameters, which in this instance are the six regression coefficients $\beta$ together with the logarithms of $\tau_\epsilon$ and $\tau_\nu$.
This can be checked with `names(obj$par)`.

```{r}
compile("epil.cpp")
dyn.load(dynlib("epil"))
```

These are the initialisation parameters:

```{r}
param <- list(
  beta = rep(0, K),
  epsilon = rep(0, N),
  nu = rep(0, N * J),
  l_tau_epsilon = 0,
  l_tau_nu = 0
)
```

We integrate `random` out, which here corresponds to `c("beta", "epsilon", "nu")`, with a Laplace approximation:

```{r}
obj <- MakeADFun(
  data = dat,
  parameters = param,
  random = c("beta", "epsilon", "nu"),
  DLL = "epil",
  silent = TRUE
)
```

Optimise `obj` using 1000 iterations of the the `nlminb` optimiser, passing in the starting values `start`, objective function `objective` and its derivative `gradient`:

```{r}
its <- 1000 #' May converge before this

opt <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr,
  control = list(iter.max = its, trace = 0)
)

sd_out <- sdreport(
  obj,
  par.fixed = opt$par,
  getJointPrecision = TRUE
)
```

### Check `TMB` objective is the same as Stan

For completeness, we check here that the objective functions as implemented in `TMB` and Stan are identical.
To obtain the `TMB` negative log-likelihood we call `MakeADFun` as before, but now do not specify any parameters to be integrated out.
For Stan, we create an empty model, then use `rstan::log_prob`:

```{r}
tmb_nll <- MakeADFun(data = dat, parameters = param, DLL = "epil")
stan_nll <- stan("epil.stan", data = dat, chains = 0)
```

Now test the NLL of the initialisation parameters `param`:

```{r}
c(
  "TMB" = tmb_nll$fn(unlist(param)),
  "Stan" = -rstan::log_prob(object = stan_nll, unlist(param))
)
```

To be more thorough, we can get all the parameters obtained using the MCMC:

```{r}
pars_mat <- as.matrix(fit1)
pars_list <- apply(pars_mat, 1, function(x) relist(flesh = x, skeleton = fit1@inits[[1]]))
upars_list <- lapply(pars_list, function(x) unconstrain_pars(fit1, x))
```

Using just a few of them, we can see that the difference between the `TMB` and Stan objectives is a constant:

```{r}
test_pars <- upars_list[500:510]
tmb_evals <- sapply(test_pars, tmb_nll$fn)
stan_evals <- -sapply(test_pars, FUN = rstan::log_prob, object = stan_nll)

data.frame(
  "TMB" = tmb_evals,
  "Stan" = stan_evals
) %>%
  mutate(Difference = TMB - Stan) %>%
  pull(Difference)
```

Note: When there are errors in the C++ template code, usually to do with indexing (unlike Stan, in `TMB` there is no requirement when defining variables to give them dimensions), calling the `MakeADFun` function tends to crash the R session.
A workaround (courtesy of Kinh) for debugging without crashing the working R session is to use the following (which creates new R sessions which crash in preference to the working R session):

```{r eval=FALSE}
library(parallel)

testrun <- mcparallel({MakeADFun(data = dat,
                                 parameters = param,
                                 DLL = "epil")})

obj <- mccollect(testrun, wait = TRUE, timeout = 0, intermediate = FALSE)
```

## `glmmTMB`

[`glmmTMB`](https://glmmtmb.github.io/glmmTMB/) is an R package written by Ben Bolker and collaborators which allows fitting generalised linear mixed models in `TMB` without writing the C++ code manually as we have in `epil.cpp`.
For example, rather than writing `epil.cpp`, we could have called the following:

```{r}
formula6 <- y ~ 1 + CTrt + ClBase4 + CV4 + ClAge + CBT + (1 | rand) + (1 | Ind)
fit6 <- glmmTMB(formula6, data = Epil, family = poisson(link = "log"))
```

This won't be exactly the same model, in terms of priors and so on, but it may be close.

## `tmbstan`

[`tmbstan`](https://journals.pl,os.org/plosone/article?id=10.1371/journal.pone.0197954) (Cole Monnahan and Kasper Kristensen) is another helpful `TMB` package which allows you to pass the same C++ template you use in `TMB` to Stan in order to perform NUTS (if you have standard C++ code then this can also likely be done using [`stanc`](https://statmodeling.stat.columbia.edu/2017/03/31/running-stan-external-c-code/)).

```{r}
fit7 <- tmbstan(obj = obj, chains = 4, refresh = 0)
```

## `aghq`

[`aghq`](https://arxiv.org/pdf/2101.04468.pdf) is an R package written by Alex Stringer which implements adaptive Gauss-Hermite quadrature.
It also works smoothly with `TMB` templates.
One (odd!) approach would be to use `glmmTMB` to get the `TMB` template which can be passed the `aghq`.

```{r}
glmm_model_info <- glmmTMB(formula6, data = Epil, family = poisson(link = "log"), doFit = FALSE)

glmm_ff <- with(glmm_model_info, {
  TMB::MakeADFun(
    data = data.tmb,
    parameters = parameters,
    random = names(parameters)[grep("theta", names(parameters), invert = TRUE)],
    DLL = "glmmTMB",
    silent = TRUE
  )
})
```

The parameter `k = 3` determines the number of quadrature points per dimension:

```{r}
glmm_quad <- aghq::marginal_laplace_tmb(glmm_ff, k = 3, startingvalue = glmm_ff$par)
```

Another, more standard, approach is to use the `TMB` template, `obj`, that we already have:

```{r results='hide'}
fit8 <- aghq::marginal_laplace_tmb(
  obj,
  k = 3,
  startingvalue = c(param$l_tau_epsilon, param$l_tau_nu)
)

fit8_samples <- aghq::sample_marginal(fit8, M = 1000)$samps %>%
  t() %>%
  as.data.frame() %>%
  inf.utils::replace_duplicate_colnames()
```

## Custom Laplace method with `aghq`

### Using multiple C++ template

Let's just starting by trying to get the Laplace marginals for $\beta$:

```{r}
compile("epil_beta_index.cpp")
dyn.load(dynlib("epil_beta_index"))
```

We have already obtained the MAP estimate $\hat \theta$ in the `TMB` section above that we can use in our empirical Bayes procedure.

```{r}
opt_theta <- opt
```

Create a function to get the Laplace marginal for the $i$th element of $\beta$:

```{r}
xi_laplace_marginal_beta <- function(i, opt_theta, finegrid) {
  dat$i <- i
  
  theta_names <- unique(names(opt_theta$par))
  map_fixed_theta <- list()

  param_fixed_theta <- param

  for(theta in theta_names) {
    map_fixed_theta[[theta]] <- rep(factor(NA), length(param[[theta]]))
    param_fixed_theta[[theta]] <- opt_theta$par[names(opt_theta$par) == theta]
  }

  #' Prepare the initialisation parameters
  param_fixed_theta[["beta_i"]] <- param_fixed_theta$beta[i]
  param_fixed_theta[["beta_minus_i"]] <- param_fixed_theta$beta[-i]
  param_fixed_theta[["beta"]] <- NULL

  #' Might need this here if you're getting crashes
  # f <- parallel::mcparallel({})
  # x <- parallel::mccollect(f) 

  obj_fixed_theta <- MakeADFun(
    data = dat,
    parameters = param_fixed_theta,
    random = c("beta_minus_i", "nu", "epsilon"),
    DLL = "epil_beta_index",
    silent = TRUE,
    map = map_fixed_theta
  )

  quad <- aghq::aghq(
    ff = obj_fixed_theta,
    k = 3,
    startingvalue = 0,
    control = aghq::default_control_tmb()
  )

  pdf_and_cdf <- aghq::compute_pdf_and_cdf(quad, finegrid = fine_grid)[[1]]
  
  return(pdf_and_cdf)
}
```

Now we can compute the Laplace marginal for all `r dat$K` indices of $\beta$:

```{r}
fine_grid <- seq(-5, 5, length.out = 1000)

laplace_beta <- lapply(1:dat$K, xi_laplace_marginal_beta, opt_theta = opt_theta, finegrid = fine_grid) %>%
  bind_rows(.id = "index") %>%
  mutate(index = as.numeric(index))
```

Here are what the marginals look like:

```{r}
laplace_beta %>%
  ggplot(aes(x = theta, y = pdf)) +
  geom_line() +
  facet_wrap(~index) +
  theme_minimal() +
  labs(x = "beta", y = "Posterior")
```

### Using one C++ template

```{r}
compile("epil_index.cpp")
dyn.load(dynlib("epil_index"))
```

We're going to use a data structure to pass to `epil_index`.
A `toggle` value of `0` corresponds to `beta`, a toggle value of `1` corresponds to `nu`, and a toggle value of `2` corresponds to `epsilon`:

```{r}
index_controller <- data.frame(
  "toggle" = c(rep(0, dat$K), rep(1, dat$N * dat$J),  rep(2, dat$N)),
  "i" = c(1:dat$K, 1:(dat$N * dat$J), 1:dat$N)
)
```

Eventually create function of the form:

```{r}
xi_laplace_marginal <- function(i, opt_theta, finegrid) {
  #' To fill
}
```

Start with `i = 1` for the first marginal of $\beta$:

```{r}
i <- 1

#' I don't know if this is the right grid for this application...
fine_grid <- seq(-5, 5, length.out = 1000)

dat$toggle <- index_controller[i, "toggle"]
dat$i <- index_controller[i, "i"]

theta_names <- unique(names(opt_theta$par))
map_fixed_theta <- list()
param_fixed_theta <- param

for(theta in theta_names) {
  map_fixed_theta[[theta]] <- rep(factor(NA), length(param[[theta]]))
  param_fixed_theta[[theta]] <- opt_theta$par[names(opt_theta$par) == theta]
}

#' Prepare the initialisation parameters if it's beta that we're toggling
#' We fix the beta parameter to zero, as we're using beta_i and beta_minus_i
if(dat$toggle == 0) {
  param_fixed_theta[["beta_i"]] <- param_fixed_theta$beta[i]
  param_fixed_theta[["beta_minus_i"]] <- param_fixed_theta$beta[-i]
  map_fixed_theta[["beta"]] <- rep(factor(NA), length(param[["beta"]]))
}

#' Or if it's epsilon that we're toggling
#' We fix the epsilon parameter to zero, as we're using epsilon_i and epsilon_minus_i
if(dat$toggle == 1) {
  param_fixed_theta[["nu_i"]] <- param_fixed_theta$nu[i]
  param_fixed_theta[["nu_minus_i"]] <- param_fixed_theta$nu[-i]
  map_fixed_theta[["nu"]] <- rep(factor(NA), length(param[["nu"]]))
}

#' Or if it's epsilon that we're toggling
#' We fix the epsilon parameter to zero, as we're using epsilon_i and epsilon_minus_i
if(dat$toggle == 2) {
  param_fixed_theta[["epsilon_i"]] <- param_fixed_theta$epsilon[i]
  param_fixed_theta[["epsilon_minus_i"]] <- param_fixed_theta$epsilon[-i]
  map_fixed_theta[["epsilon"]] <- rep(factor(NA), length(param[["epsilon"]]))
}

get_random <- function(dat) {
  if(dat$toggle == 0) return(c("beta_minus_i", "nu", "epsilon"))
  if(dat$toggle == 1) return(c("beta", "nu_minus_i", "epsilon"))
  if(dat$toggle == 2) return(c("beta", "nu", "epsilon_minus_i"))
}

obj_fixed_theta <- MakeADFun(
 data = dat,
 parameters = param_fixed_theta,
 random = get_random(dat),
 DLL = "epil_index",
 silent = TRUE,
 map = map_fixed_theta
)

quad <- aghq::aghq(
  ff = obj_fixed_theta,
  k = 3,
  startingvalue = 0,
  control = aghq::default_control_tmb()
)

pdf_and_cdf <- aghq::compute_pdf_and_cdf(quad, finegrid = fine_grid)[[1]]

ggplot(pdf_and_cdf, aes(x = theta, y = pdf)) +
  geom_line() +
  theme_minimal() +
  labs(x = "beta[1]", y = "Posterior")
```

# Comparison

## Table

Let's look at the mean and standard deviation of each of the $\beta$ parameters:

```{r}
stan1 <- as.vector(t(summary(fit1)$summary[1:6, c(1, 3)]))
inla2 <- as.vector(t(fit2$summary.fixed[1:6, 1:2]))
inla3 <- as.vector(t(fit3$summary.fixed[1:6, 1:2]))
inla4 <- as.vector(t(fit4$summary.fixed[1:6, 1:2]))
tmb5 <- as.vector(t(data.frame(sd_out$par.random[1:6], sqrt(sd_out$diag.cov.random[1:6]))))
glmmtmb6 <- as.vector(t(summary(fit6)$coefficients$cond[, c("Estimate", "Std. Error")]))
tmbstan7 <- as.vector(t(summary(fit7)$summary[1:6, c(1, 3)]))

fit8_beta_samples <- fit8_samples[, 1:6]
aghq8 <- as.vector(t(data.frame(mean = apply(fit8_beta_samples, 2, mean), sd = apply(fit8_beta_samples, 2, sd))))

df <- cbind(stan1, inla2, inla3, inla4, tmb5, glmmtmb6, tmbstan7, aghq8) %>%
  as.data.frame() %>%
  mutate(type = gl(2, 1, 12, labels = c("Mean", "SD")))

beta_i <- function(i) { c(paste0("beta_", i), paste0("sd(beta_", i, ")")) }
rownames(df) <- c(sapply(0:5, beta_i))
colnames(df) <- c("Stan", "R-INLA_G", "R-INLA_SL", "R-INLA_L", "TMB", "glmmTMB", "tmbstan", "aghq", "type")

df %>%
  select(-type) %>%
  kableExtra::kable()

saveRDS(df, "comparison-results.rds")
```

## Focus on $\beta_0$

```{r}
plot_pdf <- ggplot(data.frame(x = rstan::extract(fit1, pars = "beta[1]")[[1]]), aes(x = x)) +
    geom_histogram(aes(y = ..density..), alpha = 0.8, fill = cbpalette[7], bins = 60) +
    theme_minimal() +
    labs(x = "beta[1]", y = "Posterior PDF")

beta0_inla_marginals <- bind_rows(
  data.frame(fit2$marginals.fixed$`(Intercept)`) %>%
    mutate(method = "G"),
  data.frame(fit3$marginals.fixed$`(Intercept)`) %>%
    mutate(method = "SL"),
  data.frame(fit4$marginals.fixed$`(Intercept)`) %>%
    mutate(method = "L")
)

plot_pdf +
  geom_line(data = beta0_inla_marginals, aes(x = x, y = y, col = method)) +
  scale_color_manual(values = cbpalette)
```

## Scatter plot against `tmbstan`

Assuming `tmbstan` to be the ground truth, we can do two dimensional plots to check the fit

```{r}
ggplot(df) +
  geom_point(aes(x = tmbstan, y = TMB, col = type)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", alpha = 0.5) +
  theme_minimal()
```

## Symmetric KL divergence

We use symmetric KL. From the Google group: "... its the symmetric version between the posterior marginal computed using Gaussian approximation and the one use one of the Laplace based ones.".
For `"gaussian"` the `kld` value is small but non-zero, why?

```{r}
head(fit2$summary.random$rand)$kld
```

For `laplace` and `"simplified.laplace"` we can look for the random effect with the highest SKLD:

```{r}
skld <- function(fit) {
  #' Nu random effect
  id_rand <- which.max(fit$summary.random$rand$kld)
  print(fit$summary.random$rand[id_rand, ])
  #' Epsilon random effect
  id_Ind <- which.max(fit$summary.random$Ind$kld)
  print(fit$summary.random$Ind[id_Ind, ])
  return(list(id_rand = id_rand, id_Ind = id_Ind))
}

skld3 <- skld(fit3)
skld4 <- skld(fit4)
```

```{r}
plot_marginals <- function(random_effect, index) {
  marginal2 <- fit2$marginals.random[[random_effect]][paste0("index.", index)][[1]] %>%
    as.data.frame() %>%
    mutate(method = "G")

  marginal3 <- fit3$marginals.random[[random_effect]][paste0("index.", index)][[1]] %>%
    as.data.frame() %>%
    mutate(method = "SL")

  marginal4 <- fit4$marginals.random[[random_effect]][paste0("index.", index)][[1]] %>%
    as.data.frame() %>%
    mutate(method = "L")

  marginals <- bind_rows(marginal2, marginal3, marginal4)

  ggplot(marginals, aes(x = x, y = y, group = method, col = method)) +
    geom_line() +
    scale_color_manual(values = cbpalette) +
    theme_minimal()
}

plot_marginals("rand", index = skld3$id_rand)
plot_marginals("Ind", index = skld3$id_Ind)
```

# Appendix

## `epil.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex1", code=readLines('epil.cpp')}
```

## `epil.stan`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex2", code=readLines('epil.stan')}
```

## `epil_beta_index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex3", code=readLines('epil_beta_index.cpp')}
```

## `epil_index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex4", code=readLines('epil_index.cpp')}
```

# Original computing environment {-}

```{r}
sessionInfo()
```

# Bibliography {-}
