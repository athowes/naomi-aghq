---
title: "Scaling up the hyperparameter grid for Naomi"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** We have previously fit empirical Bayes style approaches to the Naomi model, fixing hyperparameter values.
  
  **Task** We would like to begin to integrate over the hyperparameters using some kind of grid structure. The problem is that there are too many to use anything dense.
---

# `TMB` fit

Start by fitting the model with `TMB`.
This is quick to do and allows us to get a model we can play with to figure out which hyperparmeters are important.

```{r results="hide"}
source("functions.R")

library(dplyr)
library(forcats)
library(ggplot2)
library(readr)
library(tidyr)
library(purrr)
library(stringr)
library(Matrix)
library(TMB)
library(tmbstan)
library(rstan)
library(aghq)
library(sf)
library(naomi)

area_merged <- read_sf(system.file("extdata/demo_areas.geojson", package = "naomi"))
pop_agesex <- read_csv(system.file("extdata/demo_population_agesex.csv", package = "naomi"))
survey_hiv_indicators <- read_csv(system.file("extdata/demo_survey_hiv_indicators.csv", package = "naomi"))
art_number <- read_csv(system.file("extdata/demo_art_number.csv", package = "naomi"))
anc_testing <- read_csv(system.file("extdata/demo_anc_testing.csv", package = "naomi"))
pjnz <- system.file("extdata/demo_mwi2019.PJNZ", package = "naomi")
spec <- naomi::extract_pjnz_naomi(pjnz)

scope <- "MWI"
level <- 4
calendar_quarter_t1 <- "CY2016Q1"
calendar_quarter_t2 <- "CY2018Q3"
calendar_quarter_t3 <- "CY2019Q4"
prev_survey_ids  <- c("DEMO2016PHIA", "DEMO2015DHS")
artcov_survey_ids  <- "DEMO2016PHIA"
vls_survey_ids <- NULL
recent_survey_ids <- "DEMO2016PHIA"
artnum_calendar_quarter_t1 <- "CY2016Q1"
artnum_calendar_quarter_t2 <- "CY2018Q3"
anc_clients_year2 <- 2018
anc_clients_year2_num_months <- 9
anc_prevalence_year1 <- 2016
anc_prevalence_year2 <- 2018
anc_art_coverage_year1 <- 2016
anc_art_coverage_year2 <- 2018

naomi_mf <- naomi_model_frame(
  area_merged,
  pop_agesex,
  spec,
  scope = scope,
  level = level,
  calendar_quarter_t1,
  calendar_quarter_t2,
  calendar_quarter_t3
)

naomi_data <- select_naomi_data(
  naomi_mf,
  survey_hiv_indicators,
  anc_testing,
  art_number,
  prev_survey_ids,
  artcov_survey_ids,
  recent_survey_ids,
  vls_survey_ids,
  artnum_calendar_quarter_t1,
  artnum_calendar_quarter_t2,
  anc_prevalence_year1,
  anc_prevalence_year2,
  anc_art_coverage_year1,
  anc_art_coverage_year2
)

compile("naomi_simple.cpp")
dyn.load(dynlib("naomi_simple"))

compile("naomi_simple_x_index.cpp")
dyn.load(dynlib("naomi_simple_x_index"))

tmb_inputs <- prepare_tmb_inputs(naomi_data)
tmb_inputs_simple <- local_exclude_inputs(tmb_inputs)

fit <- local_fit_tmb(tmb_inputs_simple, outer_verbose = TRUE, inner_verbose = FALSE, max_iter = 250, progress = NULL, DLL = "naomi_simple")
```

# First grid ideas

The names of the hyperparameters are:

```{r}
hypers <- names(fit$par)
```

There are `r length(hypers)` of them, if you'd like to count.

We can calculate their posterior standard deviations using `TMB::sdreport` which uses the delta method as follows:

```{r}
sd_out <- sdreport(obj = fit$obj, par.fixed = fit$par, getJointPrecision = TRUE)
sd <- sqrt(diag(sd_out$cov.fixed))

data.frame(par = names(sd), sd = unname(sd)) %>%
  ggplot(aes(x = reorder(par, sd), y = sd)) +
    geom_point(alpha = 0.6) +
    coord_flip() +
    lims(y = c(0, 2)) +
    theme_minimal() +
    labs(x = "", y = "SD from TMB")
```

A first suggestion about how to place grid points would be proportional to the standard deviations above.
For example, we could get a product grid with the minimum number of points by choosing to have:

* `k = 2` points in the dimension with the highest SD
* `k = 1` point in every other dimension

This would look like the following:

```{r}
base_grid <- sd_levels_ghe_grid(
  dim = length(hypers),
  level = c(1, 2),
  cut_off = c(0, 1.9),
  sd = sqrt(diag(sd_out$cov.fixed))
)

base_grid
```

The total number of points is 2:

```{r}
prod(base_grid$level)
```

Let's try fitting this, and see how long it takes:

```{r}
start <- Sys.time()
quad <- fit_aghq(tmb_inputs, basegrid = base_grid)
end <- Sys.time()

end - start
```

This was a nice idea!
It's not perfect though.
Some hyperparameters have higher SD than others in part simply due to the scale that they are on.
For example if the hyper is on the logit scale then looks from the plot above that it will have a higher SD than another hyper on the log scale.
Is there a way to work around this?
We would ideally like to know how much variation in each hyperparameter effects variation in model outputs, and then take that into account in the importance we place on each dimension.
Doing this in any good automated ways would be a nice contribution.

What about using the eigenvalues rather than the standard deviation?

```{r}
m <- sd_out$par.fixed
C <- sd_out$cov.fixed
E <- eigen(C)
V <- diag(E$values)
D <- E$vectors
```

The relative contributions of each dimension (similar to a Scree plot) are as follows:

```{r}
tv_df <- data.frame(
  n = 1:length(E$values),
  tv = cumsum(E$values / sum(E$values))
)

ggplot(tv_df, aes(x = n, y = tv)) +
  geom_point() +
  geom_hline(yintercept = 0.9, col = "grey", linetype = "dashed") +
  annotate("text", x = 20, y = 0.875, label = "90% of total variation explained", col = "grey") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Dimensions of PCA included", y = "Total variation explained") +
  theme_minimal()
```

```{r}
create_grid <- function(d, k) {
  mvQuad::createNIGrid(dim = d, type = "GHe", level = k)
}

gg <- create_grid(d = 24, k =)
```

```{r}
#' @param gg Quadrature grid
#' @param D Eigenvectors of the Hessian
#' @param V Eigenvalues of the Hessian
#' @param m Mode
#' @param p The number of dimensions of the PCA included
pca_rescale_grid <- function(gg, D, V, m, p) {
  d <- dim(D)[1]
  
  Dp <- sparseMatrix(
    i = rep(1:p, times = p),
    j = sort(rep(1:p,times = p)),
    x = as.numeric(D[1:p, 1:p]),
    dims = c(d, d)
  )
  
  Vp <- Diagonal(n = d, x = c(sqrt(diag(V)[1:p]), rep(0, (d - p))))
  DVp <- Dp %*% Vp
  
  newnodes <- t(DVp %*% t(mvQuad::getNodes(gg)))
  for(j in 1:d) newnodes[ ,j] <- newnodes[ ,j] + m[j]
  newweights <- sqrt(det(V)) * mvQuad::getWeights(gg)

  list(nodes = newnodes, weights = newweights)
}
```

```{r}
gg5 <- pca_rescale_grid(gg = create_grid(d = 24, k = 2), D = D, V = V, m = m, p = 5)
gg7 <- pca_rescale_grid(gg = create_grid(d = 24, k = 2), D = D, V = V, m = m, p = 7)
gg7 <- pca_rescale_grid(gg = create_grid(d = 24, k = 2), D = D, V = V, m = m, p = 9)

out <- as.data.frame(as.matrix(gg$nodes))
out$lps <- -1 * apply(out, 1, fit$obj)
out$w <- gg$weights
logSumExpWeights(out$lps, out$w)
```
