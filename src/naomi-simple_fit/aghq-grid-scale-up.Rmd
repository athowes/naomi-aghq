---
title: "Scaling up the hyperparameter grid for Naomi"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** We have previously fit empirical Bayes style approaches to the Naomi model, fixing hyperparameter values.
  
  **Task** We would like to begin to integrate over the hyperparameters using some kind of grid structure. The problem is that there are too many to use anything dense.
---

# `TMB` fit

Start by fitting the model with `TMB`.
This is quick to do and allows us to get a model we can play with to figure out which hyperparmeters are important.

```{r results="hide"}
source("functions.R")

library(dplyr)
library(forcats)
library(ggplot2)
library(readr)
library(tidyr)
library(purrr)
library(stringr)
library(Matrix)
library(TMB)
library(tmbstan)
library(rstan)
library(aghq)
library(sf)
library(naomi)

area_merged <- read_sf(system.file("extdata/demo_areas.geojson", package = "naomi"))
pop_agesex <- read_csv(system.file("extdata/demo_population_agesex.csv", package = "naomi"))
survey_hiv_indicators <- read_csv(system.file("extdata/demo_survey_hiv_indicators.csv", package = "naomi"))
art_number <- read_csv(system.file("extdata/demo_art_number.csv", package = "naomi"))
anc_testing <- read_csv(system.file("extdata/demo_anc_testing.csv", package = "naomi"))
pjnz <- system.file("extdata/demo_mwi2019.PJNZ", package = "naomi")
spec <- naomi::extract_pjnz_naomi(pjnz)

scope <- "MWI"
level <- 4
calendar_quarter_t1 <- "CY2016Q1"
calendar_quarter_t2 <- "CY2018Q3"
calendar_quarter_t3 <- "CY2019Q4"
prev_survey_ids  <- c("DEMO2016PHIA", "DEMO2015DHS")
artcov_survey_ids  <- "DEMO2016PHIA"
vls_survey_ids <- NULL
recent_survey_ids <- "DEMO2016PHIA"
artnum_calendar_quarter_t1 <- "CY2016Q1"
artnum_calendar_quarter_t2 <- "CY2018Q3"
anc_clients_year2 <- 2018
anc_clients_year2_num_months <- 9
anc_prevalence_year1 <- 2016
anc_prevalence_year2 <- 2018
anc_art_coverage_year1 <- 2016
anc_art_coverage_year2 <- 2018

naomi_mf <- naomi_model_frame(
  area_merged,
  pop_agesex,
  spec,
  scope = scope,
  level = level,
  calendar_quarter_t1,
  calendar_quarter_t2,
  calendar_quarter_t3
)

naomi_data <- select_naomi_data(
  naomi_mf,
  survey_hiv_indicators,
  anc_testing,
  art_number,
  prev_survey_ids,
  artcov_survey_ids,
  recent_survey_ids,
  vls_survey_ids,
  artnum_calendar_quarter_t1,
  artnum_calendar_quarter_t2,
  anc_prevalence_year1,
  anc_prevalence_year2,
  anc_art_coverage_year1,
  anc_art_coverage_year2
)

compile("naomi_simple.cpp")
dyn.load(dynlib("naomi_simple"))

compile("naomi_simple_x_index.cpp")
dyn.load(dynlib("naomi_simple_x_index"))

tmb_inputs <- prepare_tmb_inputs(naomi_data)
tmb_inputs_simple <- local_exclude_inputs(tmb_inputs)

fit <- local_fit_tmb(tmb_inputs_simple, outer_verbose = TRUE, inner_verbose = FALSE, max_iter = 250, progress = NULL, DLL = "naomi_simple")
```

# First grid ideas

The names of the hyperparameters are:

```{r}
hypers <- names(fit$par)
```

There are `r length(hypers)` of them, if you'd like to count.

We can calculate their posterior standard deviations using `TMB::sdreport` which uses the delta method as follows:

```{r}
sd_out <- sdreport(obj = fit$obj, par.fixed = fit$par, getJointPrecision = TRUE)
sd <- sqrt(diag(sd_out$cov.fixed))

data.frame(par = names(sd), sd = unname(sd)) %>%
  ggplot(aes(x = reorder(par, sd), y = sd)) +
    geom_point(alpha = 0.6) +
    coord_flip() +
    lims(y = c(0, 2)) +
    theme_minimal() +
    labs(x = "", y = "SD from TMB")
```

A first suggestion about how to place grid points would be proportional to the standard deviations above.
For example, we could get a product grid with the minimum number of points by choosing to have:

* `k = 2` points in the dimension with the highest SD
* `k = 1` point in every other dimension

This would look like the following:

```{r}
base_grid <- sd_levels_ghe_grid(
  dim = length(hypers),
  level = c(1, 2),
  cut_off = c(0, 1.9),
  sd = sqrt(diag(sd_out$cov.fixed))
)

base_grid
```

The total number of points is 2:

```{r}
prod(base_grid$level)
```

Let's try fitting this, and see how long it takes:

```{r}
start <- Sys.time()
quad <- fit_aghq(tmb_inputs, basegrid = base_grid)
end <- Sys.time()

end - start
```

This was a nice idea!
It's not perfect though.
Some hyperparameters have higher SD than others in part simply due to the scale that they are on.
For example if the hyper is on the logit scale then looks from the plot above that it will have a higher SD than another hyper on the log scale.
Is there a way to work around this?
We would ideally like to know how much variation in each hyperparameter effects variation in model outputs, and then take that into account in the importance we place on each dimension.
Doing this in any good automated ways would be a nice contribution.
