---
title: Inference methods for extended latent Gaussian models
subtitle: Waterloo SAS Student Seminar Series
author: Adam Howes
institute: Imperial College London
date: October 2022
bibliography: citations.bib
output:
  beamer_presentation:
    latex_engine: pdflatex
    highlight: haddock
    fig_width: 7 
    fig_height: 3
    includes:
        in_header: preamble.tex
---

# Motivation

* Surveillance of the HIV epidemic in sub-Saharan Africa
* Want to estimate indicators used for monitoring and response, including:
  * Prevalence $\rho$: the proportion of people who are HIV positive
  * Incidence $\lambda$: the proportion of people newly infected 
  * Treatment coverage $\alpha$: the proportion of PLHIV on treatment
* We would like to provide them at a local, district-level
* This is a challenging task! Data is noisy, sparse and biased

# A simple small-area model for prevalence

* Consider areas $i = 1, \ldots, n$
* Simple random sample household-survey taken in each area, with sample sizes $m^\text{HS}_i$
* The number of people testing positive is $y^\text{HS}_i$
* Then we can use a binomial logistic regression of the form:
\begin{align*}
y^\text{HS}_i &\sim \text{Bin}(m^\text{HS}_i, \rho^\text{HS}_i), \\
\text{logit}(\rho^\text{HS}_i) &\sim g(\vartheta^\text{HS}), \quad i = 1, \ldots, n,
\end{align*}
* If $g$ is Gaussian then this is a latent Gaussian model in the sense of @rue2009approximate
* We usually set up $g$ as a spatial smoother, because the sample sizes in each area are too small to get reliable direct estimates
* One problem with the above model is that household surveys are expensive to run, so they only happen rarely

# Latent Gaussian models

* Three-stage Bayesian hierarchical model
\begin{alignat*}{2}
  &\text{(Observations)}     &   \y &\sim p(\y \, | \, \x), \\
  &\text{(Latent field)}     &   \x &\sim p(\x \, | \, \btheta), \\
  &\text{(Hyperparameters)}  &   \qquad \btheta &\sim p(\btheta),
\end{alignat*}
where $\y = (y_1, \ldots, y_n)$, $\x = (x_1, \ldots, x_n)$, $\btheta = (\theta_1, \ldots, \theta_m)$
* Interested in learning both $(\btheta, \x)$ from data $\y$
* If the middle layer is Gaussian, then it's a latent Gaussian model
\begin{equation*}
  \text{(Latent field)} \qquad  p(\x \, | \, \btheta) = \mathcal{N}(\x  \, | \, \bm{\mu}(\btheta), \bm{Q}(\btheta)^{-1}).
\end{equation*}
* Covers most of the models commonly used in spatiotemporal statistics
* Latent field is typically indexed by spatiotemporal location, such that $n > m$

# Adding ANC surveillance

* Pregnant women attending antenatal care clinics are routinely tested for HIV, to avoid mother-to-child transmission
* This data source is more real-time than household surveys, but it's also more biased, because attendees are unlikely to be as representative of the population
* But perhaps this bias is consistent, in which case we can still make use of the ANC data to improve our model!

# Adding ANC surveillance

* Suppose of $m^\text{ANC}_i$ women attending ANC, $y^\text{ANC}_i$ are HIV positive, then we can use another binomial logistic regression:
\begin{align*}
y^\text{ANC}_i &\sim \text{Bin}(m^\text{ANC}_i, \rho^\text{ANC}_i), \\
\text{logit}(\rho^\text{ANC}_i) &= \text{logit}(\rho^\text{HS}_i) + b_i, \\
b_i &\sim \mathcal{N}(\beta_b, \sigma_b^2),
\end{align*}
* Similar to using $\rho^\text{ANC}_i$ as a covariate in our model for household survey prevalence, but this way we allow it to vary rather than be fixed

# Adding ART coverage

* Remember that we're also interested in what proportion of PLHIV are receiving treatment
* To estimate this, we need first to know the number of PLHIV
\begin{equation*}
H_i = N_i \rho^\text{HS}_i
\end{equation*}
* Then
\begin{align*}
A_i &\sim \text{Bin}(N_i, \rho^\text{HS}_i \alpha_i), \\
\text{logit}(\alpha_i) &\sim \mathcal{N}(\beta_\alpha, \sigma_\alpha^2),
\end{align*}

# Naomi evidence synthesis model

:::::::::::::: {.columns}

::: {.column width=.65}

* Combining these three modules is the basis of the Naomi evidence synthesis model
* Used by countries (which provide their own data) to produce HIV estimates in a yearly process supported by UNAIDS
* Can't run long MCMC in this setting, requires fast, accurate, approximations 
* It's a complicated model, and requires something more flexible than `R-INLA`
* Currently using Template Model Builder `TMB` [@kristensen2015tmb]

:::

::: {.column width=.35}

```{r, echo=FALSE, fig.cap="A supermodel", out.width = '65%'}
knitr::include_graphics("naomi_hex.png")
```

:::

::::::::::::::

# Template Model Builder

* So what is `TMB`?
* R package which implements the Laplace approximation for latent variable models using automatic differentiation (via `CppAD`)
  * For more about AD see e.g. @griewank2008evaluating
  * Useful for getting the mode, Hessian
* Write an objective function $f(\x, \btheta)$ in C++ ("user template")
    * We select $f(\x, \btheta) = - \log p(\y \, | \, \x, \btheta) p(\x \, | \, \btheta) p(\btheta)$

# Template Model Builder

```{cpp eval=FALSE}
#include <TMB.hpp>

template <class Type>
Type objective_function<Type>::operator()() {
  // Define data e.g.
  DATA_VECTOR(y);
  // Define parameters e.g.
  PARAMETER(mu);
  // Calculate negative log-likelihood e.g.
  nll = Type(0.0);
  nll -= dnorm(y, mu, 1, true).sum()
  return(nll);
}
```

# Template Model Builder

* Performs the Laplace approximation $L_f(\btheta) \approx L^\star_f(\btheta)$ and use R to optimise this with respect to $\btheta$ to give $\hat \btheta$ (the central point in Figure \ref{fig:grid})
  * This is done by specifying the `random` argument to be the parameters that you want to integrate out with a Laplace approximation (the latent field)
* MAP estimate of $\x$ conditional on $\hat \btheta$
* Standard errors calculated using the $\delta$-method (a Gaussian assumption)

# Integrated Nested Laplace Approximation

* Suggested reading: the paper [@rue2009approximate] or a book e.g. @blangiardo2015spatial
* Approximate Bayesian inference for \textcolor{hilit}{latent Gaussian models} (LGMs), which recall are three-stage models with middle layer
\begin{equation*}
  \text{(Latent field)} \qquad  p(\x \, | \, \btheta) = \mathcal{N}(\x  \, | \, \bm{\mu}(\btheta), \bm{Q}(\btheta)^{-1}).
\end{equation*}
* `R-INLA` implementation takes advantage of sparsity properties of $\bm{Q}(\btheta)$, i.e. if $\x$ is a Gaussian Markov random field (GMRF)

# Integrated Nested Laplace Approximation

* Gives approximate \textcolor{hilit}{posterior marginals} $\{\tilde p(x_i \, | \, \y)\}_{i = 1}^n$ and $\{\tilde p(\theta_j \, | \, \y)\}_{j = 1}^m$
* To approximate posterior marginals below requires $\tilde p(\btheta \, | \, \y)$ and $\tilde p(x_i \, | \, \btheta, \y)$
\begin{align}
  p(x_i \, | \, \y) &= \int p(x_i, \btheta \, | \, \y) \text{d} \btheta = \int p(x_i \, | \, \btheta, \y) p(\btheta \, | \, \y) \text{d}\btheta, \quad i = 1, \dots, n, \label{eq:inla1} \\
  p(\theta_j \, | \, \y) &= \int p(\btheta \, | \, \y) \text{d}\btheta_{-j} \quad j = 1, \ldots, m. \label{eq:inla2}
\end{align}

# Integrated Nested Laplace Approximation

1) First Laplace approximate hyperparameter posterior
\begin{equation}
\tilde p(\btheta \, | \, \y) \propto \frac{p(\y, \x, \btheta)}{\tilde p_G(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \bm{\mu}^\star(\btheta)}  \label{eq:hypermarginal}
\end{equation}
which can be marginalised to get $\tilde p(\theta_j \, | \, \y)$
  * Note: this involves integrating out a Gaussian approximation to the latent field, which we denote by $p_G(\x \, | \, \btheta, \y)$

# Integrated Nested Laplace Approximation

2) In both \eqref{eq:inla1} and \eqref{eq:inla2} we want to integrate w.r.t. \eqref{eq:hypermarginal}, so choose integration points and weights $\{ \btheta^{(k)}, \Delta^{(k)} \}$
  * For low $m$ `R-INLA` uses a grid-strategy (illustrated in the next slide)
  * For larger $m$ this becomes too expensive and `R-INLA` uses a CCD design is used
  * Other approaches, like adaptive Gaussian Hermite quadrature (AGHQ) have recently been shown to have theoretical guarantees [@bilodeau2021stochastic] and may be preferable

#

```{r message=FALSE, echo=FALSE, out.height="200px", fig.cap=paste("An illustration of the \\texttt{R-INLA} grid method for selecting integration points using a toy bivariate Gaussian distribution for $\\btheta$. Start at the mode and work outwards along the eigenvectors until the density drops sufficiently low. \\label{fig:grid}")}
knitr::include_graphics("depends/inla-grid.pdf")
```

# Adaptive Gaussian Hermite Quadrature

* `aghq` R package and vignette [@stringer2021implementing]
* Gauss-Hermite quadrature is a way of picking nodes and weights, and is based on the theory of polynomial interpolation
* The adaptive part means that it uses the location (mode) and curvature (Hessian) of the target (posterior)
* Use $k$ quadrature points
  * If $k$ is odd then they include the mode
  * If $k = 1$ then it's a Laplace approximation
  * In the vignette $k = 3$ (for each dimension, so $3^m$ total) is chosen quite often

# Integrated Nested Laplace Approximation

3) Choose approximation for $\tilde p(x_i \, | \, \btheta, \y)$
  * Simplest version [@rue2007approximate] is to marginalise the $p_G(\x \, | \, \btheta, \y)$
\begin{equation}
\tilde p(x_i \, | \, \btheta, \y) = \mathcal{N}(x_i \, | \, \mu^\star_i(\btheta), 1 / q^\star_i(\btheta))
\end{equation}
  * The above is referred to as `method = "gaussian"` in `R-INLA`
  * There are two better, more complex approximations
    * Confusingly called `"simplified laplace"` and `"laplace"`

# Integrated Nested Laplace Approximation

4) Finally (!) use quadrature combining
  * our approximation $p(\btheta \, | \, \y)$ from step 1),
  * our integration points and weights $\{ \btheta^{(k)}, \Delta^{(k)} \}$ from step 2),
  * and our approximation $\tilde p(x_i \, | \, \btheta, \y)$ from step 3)
to give
\begin{equation}
  \tilde p(x_i \, | \, \y) = 
  \sum_{k = 1}^K \tilde p(x_i \, | \, \btheta^{(k)}, \y) \times \tilde p(\btheta^{(k)} \, | \, \y) \times \Delta^{(k)}
\end{equation}

# Experiments

* We would like to find an accurate way to do inference for the Naomi model
* It also needs to be fast enough to use in production!

# Experiments

* We wrote a simplified version of the Naomi model up in `TMB`
* This allowed us to test three inference methods **all using precisely the same model and C++ code**

1. A direct Gaussian approximation via `TMB`
2. Adaptive Gaussian Hermite quadrature via `aghq`
3. No-U-Turn Sampling (NUTS -- a type of Hamiltonian Monte Carlo) via `tmbstan`

* Note: using different software it is often very difficult to ensure the model is precisely the same, so we're very fortunate here

# Comparison approach

* You could look at the summaries like the mean and standard deviation of each of the posterior marginals
  * Any approximation method should be pretty good at getting the mean right
  * Gaussian approximations should be good at getting the second moment right
* It's probably better to compare the whole posterior distributions
* One way to do this is via Kolmogorov-Smirnov statistics, which give the maximum difference between two empirical CDFs

# References {.allowframebreaks}
