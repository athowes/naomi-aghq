---
title: "Scaling up the hyperparameter grid for Naomi"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** We have previously fit empirical Bayes style approaches to the Naomi model, fixing hyperparameter values to a single value.
  
  **Task** We would like to begin to integrate over the hyperparameters using some kind of grid structure. The problem is that there are >20 hyperparameters, and so using any type of dense grid is not practical. In this notebook we investigate approaches to using a smaller number of grid points to cover the space, including 1) allocating points in proporiton to posterior standard deviation 2) using principle components analysis.
---

# `TMB` fit

Start by fitting the model with `TMB`.
This is quick to do and allows us to get a model we can play with to figure out which hyperparmeters are important.

```{r}
compile("naomi_simple.cpp")
dyn.load(dynlib("naomi_simple"))

tmb <- readRDS("depends/tmb.rds")
fit <- tmb$fit
```

# Grid ideas

The names of the hyperparameters are:

```{r}
(hypers <- names(fit$par))
```

There are `r length(hypers)` of them, if you'd like to count.

We can calculate their posterior standard deviations using `TMB::sdreport` which uses the delta method as follows:

```{r}
sd_out <- sdreport(obj = fit$obj, par.fixed = fit$par, getJointPrecision = TRUE)
```

The marginal standard deviations are:

```{r}
sd <- sqrt(diag(sd_out$cov.fixed))

data.frame(par = names(sd), sd = unname(sd)) %>%
  ggplot(aes(x = reorder(par, sd), y = sd)) +
    geom_point(alpha = 0.6) +
    coord_flip() +
    lims(y = c(0, 2)) +
    theme_minimal() +
    labs(x = "", y = "SD from TMB")
```

Or, more generally, the covariance matrix is:

```{r}
C <- sd_out$cov.fixed
C_df <- reshape2::melt(as.matrix(C))

C_plot <- C_df %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  labs(x = "", y = "", fill = "C[i, j]") +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(x = "i", y = "j") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

C_plot
```

## Number of points proportional to standard deviation

A first suggestion about how to place grid points would be proportional to the standard deviations above.
For example, we could get a product grid with the minimum number of points by choosing to have:

* `k = 2` points in the dimension with the highest SD
* `k = 1` point in every other dimension

This would look like the following:

```{r}
base_grid <- sd_levels_ghe_grid(
  dim = length(hypers),
  level = c(1, 2),
  cut_off = c(0, 1.9),
  sd = sqrt(diag(sd_out$cov.fixed))
)

base_grid
```

The total number of points for this grid is 2, by design:

```{r}
prod(base_grid$level)
```

Let's try fitting this, and see how long it takes.
First prepare the data:

```{r results="hide", message=FALSE}
area_merged <- read_sf(system.file("extdata/demo_areas.geojson", package = "naomi"))
pop_agesex <- read_csv(system.file("extdata/demo_population_agesex.csv", package = "naomi"))
survey_hiv_indicators <- read_csv(system.file("extdata/demo_survey_hiv_indicators.csv", package = "naomi"))
art_number <- read_csv(system.file("extdata/demo_art_number.csv", package = "naomi"))
anc_testing <- read_csv(system.file("extdata/demo_anc_testing.csv", package = "naomi"))
pjnz <- system.file("extdata/demo_mwi2019.PJNZ", package = "naomi")
spec <- naomi::extract_pjnz_naomi(pjnz)

scope <- "MWI"
level <- 4
calendar_quarter_t1 <- "CY2016Q1"
calendar_quarter_t2 <- "CY2018Q3"
calendar_quarter_t3 <- "CY2019Q4"
prev_survey_ids  <- c("DEMO2016PHIA", "DEMO2015DHS")
artcov_survey_ids  <- "DEMO2016PHIA"
vls_survey_ids <- NULL
recent_survey_ids <- "DEMO2016PHIA"
artnum_calendar_quarter_t1 <- "CY2016Q1"
artnum_calendar_quarter_t2 <- "CY2018Q3"
anc_clients_year2 <- 2018
anc_clients_year2_num_months <- 9
anc_prevalence_year1 <- 2016
anc_prevalence_year2 <- 2018
anc_art_coverage_year1 <- 2016
anc_art_coverage_year2 <- 2018

naomi_mf <- naomi_model_frame(
  area_merged,
  pop_agesex,
  spec,
  scope = scope,
  level = level,
  calendar_quarter_t1,
  calendar_quarter_t2,
  calendar_quarter_t3
)

naomi_data <- select_naomi_data(
  naomi_mf,
  survey_hiv_indicators,
  anc_testing,
  art_number,
  prev_survey_ids,
  artcov_survey_ids,
  recent_survey_ids,
  vls_survey_ids,
  artnum_calendar_quarter_t1,
  artnum_calendar_quarter_t2,
  anc_prevalence_year1,
  anc_prevalence_year2,
  anc_art_coverage_year1,
  anc_art_coverage_year2
)

tmb_inputs <- prepare_tmb_inputs(naomi_data)
tmb_inputs_simple <- local_exclude_inputs(tmb_inputs)
```

Now fit the model with `aghq`:

```{r}
start <- Sys.time()
quad <- fit_aghq(tmb_inputs, basegrid = base_grid)
end <- Sys.time()

end - start
```

A limitation of this approach is that some hyperparameters have higher SD than others in part due to the different scales that they are on.
For example if the hyper is on the logit scale then (from the plot above) it looks as though it will have a higher SD than another hyper on the log scale.
We would ideally like to know how much variation in each hyperparameter effects variation in model outputs, and then take that into account in the importance we place on each dimension: doing this in any good automated ways would be a nice contribution.

## Eigendecomposition

Another option is to use principle components analysis (PCA) to create a grid on a subspace of $\mathbb{R}^{24}$ which retains most of the variation.

```{r}
m <- sd_out$par.fixed
eigenC <- eigen(C)
lambda <- eigenC$values
Lambda <- diag(lambda)
E <- eigenC$vectors
```

Such that `C` can be obtained by $E \Lambda E^\top$, or in code `E %*% diag(lambda) %*% t(E)`:

```{r}
max(C - (E %*% Lambda %*% t(E))) < 10E-12
```

The relative contributions of each dimension are as follows (this is a Scree plot):

```{r}
tv_df <- data.frame(
  n = 1:length(lambda),
  tv = cumsum(lambda / sum(lambda))
)

ggplot(tv_df, aes(x = n, y = tv)) +
  geom_point() +
  geom_hline(yintercept = 0.9, col = "grey", linetype = "dashed") +
  annotate("text", x = 20, y = 0.875, label = "90% of total variation explained", col = "grey") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "PCA dimensions included", y = "Total variation explained") +
  theme_minimal()
```

So with 5 dimensions included, we can explain `r round(100 * filter(tv_df, n == 5) %>% pull(tv), 1)`% of the total variation.
Or, with 10 dimensions included, that percentage increases to `r round(100 * filter(tv_df, n == 10) %>% pull(tv), 1)`%.

What do the PC loadings look like?

```{r}
reshape2::melt(as.matrix(E)) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  labs(x = "", y = "", fill = "E[i, j]") +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(x = "PC loading", y = "Hyper") +
  theme_minimal()
```

Start with `s = 5` dimensions kept of the PCA.
Then create a dense AGHQ grid with `k = 2` on 5 dimensions.

```{r}
s <- 5
d <- dim(E)[1]
gg_s <- mvQuad::createNIGrid(dim = s, type = "GHe", level = 2) 
```

There are $2^5 = 32$ nodes as follows:

```{r}
(n_nodes <- nrow(mvQuad::getNodes(gg_s)))
```

How does the reconstruction of the covariance matrix with `r s` components look?

```{r}
E_s <- E[, 1:s]
Lambda_s <- Lambda[1:s, 1:s]
C_s <- E_s %*% Lambda_s %*% t(E_s)

C_plot

C_s_plot <- reshape2::melt(as.matrix(C_s)) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  labs(x = "", y = "", fill = "C_s[i, j]") +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(x = "i", y = "j") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

C_plot + C_s_plot
```

Let $a \in \mathbb{R}^d$ and $b \in \mathbb{R}^s$.
The PCA provides a mapping $f: \mathbb{R}^d \to \mathbb{R}^s$ which compresses each $a$ to a smaller $b$ given by $f(a) = (E_s \Lambda_s^{1/2})^\top a = b$.
Then $f^{-1}$ is given by $f^{-1}(b) = E_s \Lambda_s^{-1/2} b = a$, using the fact that $E_s^\top = E_s^{-1}$.
So we need to multiply our nodes by $E_s \Lambda_s^{- 1/2}$ as follows:

```{r}
a_nodes <- mvQuad::getNodes(gg_s)
b_nodes <- t(E_s %*% diag(lambda[1:s]^{-0.5}) %*% t(a_nodes))

dim(a_nodes)
dim(b_nodes)
```

Put this into a function, which also adjusts according to the mean and reweights the nodes:

```{r}
#' @param m Mean vector
#' @param C Covariance matrix
#' @param s Small grid dimension
#' @param k Number of points per small grid dimension
pca_rescale <- function(m, C, s, k) {
  d <- nrow(C)
  stopifnot(d == length(m))
  eigenC <- eigen(C)
  lambda <- eigenC$values
  Lambda <- diag(lambda)
  E <- eigenC$vectors
  E_s <- E[, 1:s] 
  gg_s <- mvQuad::createNIGrid(dim = s, type = "GHe", level = k) 
  nodes_out <- t(E_s %*% diag(lambda[1:s]^{-0.5}, ncol = s) %*% t(mvQuad::getNodes(gg_s)))
  for(j in 1:d) nodes_out[, j] <- nodes_out[, j] + m[j]
  weights_out <- mvQuad::getWeights(gg_s) * as.numeric(mvQuad::getWeights(mvQuad::createNIGrid(dim = d - s, type = "GHe", level = 1)))
  
  # Putting things into a mvQuad format manually
  gg <- mvQuad::createNIGrid(dim = d, type = "GHe", level = 1)
  gg$level <- rep(NA, times = d)
  gg$ndConstruction <- "PCA"
  gg$nodes <- nodes_out
  gg$weights <- weights_out
  return(gg)
}
```

### Testing with simple examples

Check that this works as visually expected for a simple example:

```{r}
mu <- c(1, 1.5)
cov <- matrix(c(2, 1, 1, 1), ncol = 2)

obj <- function(theta) {
  mvtnorm::dmvnorm(theta, mean = mu, sigma = cov)
}

grid <- expand.grid(
  theta1 = seq(-2, 5, length.out = 700),
  theta2 = seq(-2, 5, length.out = 700)
)

ground_truth <- cbind(grid, pdf = obj(grid))

plot <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = multi.utils::cbpalette()[1]) +
  coord_fixed(xlim = c(-2, 4.5), ylim = c(-2, 4.5), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal()

gaussian_2d <- pca_rescale(m = mu, C = cov, s = 1, k = 5)

plot +
  geom_point(
    data = data.frame(mvQuad::getNodes(gaussian_2d)) %>%
      mutate(weight = mvQuad::getWeights(gaussian_2d)),
    aes(x = X1, y = X2, size = weight),
    col = multi.utils::cbpalette()[2],
    inherit.aes = FALSE
  ) +
  scale_size_continuous(range = c(1, 2))
```

Test how this PCA grid works for some simple integrals.
We will use Alex's [unit tests](https://github.com/awstringer1/aghq/blob/master/tests/testthat/setup-01-optimization.R) from the `aghq` package.

```{r}
logfteta2d <- function(eta, y) {
  n <- length(y)
  n1 <- ceiling(n / 2)
  n2 <- floor(n / 2)
  y1 <- y[1:n1]
  y2 <- y[(n1 + 1):(n1 + n2)]
  eta1 <- eta[1]
  eta2 <- eta[2]
  sum(y1) * eta1 - (length(y1) + 1) * exp(eta1) - sum(lgamma(y1+1)) + eta1 +
    sum(y2) * eta2 - (length(y2) + 1) * exp(eta2) - sum(lgamma(y2+1)) + eta2
}

set.seed(84343124)
n1 <- 5
n2 <- 5
n <- n1 + n2
y1 <- rpois(n1, 5)
y2 <- rpois(n2, 5)

truemode2d <- c(log((sum(y1) + 1)/(length(y1) + 1)), log((sum(y2) + 1)/(length(y2) + 1)))

truelogint <- function(y) lgamma(1 + sum(y)) - (1 + sum(y)) * log(length(y) + 1) - sum(lgamma(y + 1))
truelognormconst2d <- truelogint(y1) + truelogint(y2)

objfunc2d <- function(x) logfteta2d(x, c(y1, y2))

funlist2d <- list(
  fn = objfunc2d,
  gr = function(x) numDeriv::grad(objfunc2d,x),
  he = function(x) numDeriv::hessian(objfunc2d,x)
)

opt_bfgs_2d <- aghq::optimize_theta(funlist2d, c(1.5, 1.5), control = default_control(method = "BFGS"))

norm_bfgs_2d_1 <- aghq::normalize_logpost(opt_bfgs_2d, 1, 1)
norm_bfgs_2d_3 <- aghq::normalize_logpost(opt_bfgs_2d, 3, 1)
norm_bfgs_2d_5 <- aghq::normalize_logpost(opt_bfgs_2d, 5, 1)
norm_bfgs_2d_7 <- aghq::normalize_logpost(opt_bfgs_2d, 7, 1)

pca_grid_bfgs_2d_1 <- pca_rescale(m = opt_bfgs_2d$mode, C = solve(opt_bfgs_2d$hessian), s = 1, k = 1)
pca_grid_bfgs_2d_3 <- pca_rescale(m = opt_bfgs_2d$mode, C = solve(opt_bfgs_2d$hessian), s = 1, k = 3)
pca_grid_bfgs_2d_5 <- pca_rescale(m = opt_bfgs_2d$mode, C = solve(opt_bfgs_2d$hessian), s = 1, k = 5)
pca_grid_bfgs_2d_7 <- pca_rescale(m = opt_bfgs_2d$mode, C = solve(opt_bfgs_2d$hessian), s = 1, k = 7)

plot_points <- function(gg) {
  gg$nodes %>%
    as.data.frame() %>%
    mutate(weights = gg$weights) %>%
    ggplot() +
    geom_point(
      aes(x = V1, y = V2, size = weights),
      alpha = 0.8,
      col = multi.utils::cbpalette()[2],
      inherit.aes = FALSE
    ) +
    scale_size_continuous(range = c(1, 2)) +
    labs(x = "", y = "", size = "Weight") +
    theme_minimal()
}

{plot_points(pca_grid_bfgs_2d_1) + plot_points(pca_grid_bfgs_2d_3)} / {plot_points(pca_grid_bfgs_2d_5) + plot_points(pca_grid_bfgs_2d_7)}

norm_pca_bfgs_2d_1 <- local_normalize_logpost(opt_bfgs_2d, basegrid = pca_grid_bfgs_2d_1, adapt = FALSE)
norm_pca_bfgs_2d_3 <- local_normalize_logpost(opt_bfgs_2d, basegrid = pca_grid_bfgs_2d_3, adapt = FALSE)
norm_pca_bfgs_2d_5 <- local_normalize_logpost(opt_bfgs_2d, basegrid = pca_grid_bfgs_2d_5, adapt = FALSE)
norm_pca_bfgs_2d_7 <- local_normalize_logpost(opt_bfgs_2d, basegrid = pca_grid_bfgs_2d_7, adapt = FALSE)

results_2d <- data.frame(
  "method" = c(
    "Truth",
    paste0("aghq, k = ", c(1, 3, 5, 7)),
    paste0("aghq-pca, k = ", c(1, 3, 5, 7))
  ),
  "lognormconst" = c(
    truelognormconst2d,
    norm_bfgs_2d_1$lognormconst, norm_bfgs_2d_3$lognormconst, norm_bfgs_2d_5$lognormconst, norm_bfgs_2d_7$lognormconst,
    norm_pca_bfgs_2d_1$lognormconst, norm_pca_bfgs_2d_3$lognormconst, norm_pca_bfgs_2d_5$lognormconst, norm_pca_bfgs_2d_7$lognormconst
  )
)

ggplot(results_2d, aes(x = method, y = lognormconst)) +
  geom_point(size = 3) +
  labs(x = "Method", y = "Log normalising constant") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, hjust = 0.5))
```

### Application to Naomi

```{r}
gg3 <- pca_rescale(m = m, C = C, s = 3, k = 2)
nrow(mvQuad::getNodes(gg3))

gg5 <- pca_rescale(m = m, C = C, s = 5, k = 2)
nrow(mvQuad::getNodes(gg5))

gg7 <- pca_rescale(m = m, C = C, s = 7, k = 2)
nrow(mvQuad::getNodes(gg7))

gg9 <- pca_rescale(m = m, C = C, s = 9, k = 2)
nrow(mvQuad::getNodes(gg9))
```

```{r}
start <- Sys.time()
quad <- fit_aghq(tmb_inputs, basegrid = base_grid)
end <- Sys.time()

end - start
```

