---
title: "Scaling up the hyperparameter grid for Naomi"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** We have previously fit empirical Bayes style approaches to the Naomi model, fixing hyperparameter values to a single value.
  
  **Task** We would like to begin to integrate over the hyperparameters using multiple nodes. The problem is that there are >20 hyperparameters, and so using any type of dense grid is not practical. In this notebook we investigate approaches to using a smaller number of grid points to cover the space, including 1) allocating points in proporiton to posterior standard deviation 2) using different varieties of principal components analysis.
editor_options: 
  markdown: 
    wrap: sentence
---

# `TMB` fit

Start by fitting the model with `TMB`.
This is quick to do and allows us to get a model we can play with to figure out which hyperparmeters are important.
In particular we obtain the mode and Hessian at the mode of the Laplace approximation to the hyperparameter marginal posterior.

```{r, class.source = 'fold-hide'}
TMB::compile("naomi_simple.cpp")
dyn.load(TMB::dynlib("naomi_simple"))

# tmb <- readRDS("depends/tmb.rds")
tmb <- readRDS("tmb.rds")
fit <- tmb$fit
```

# Point concentration proportional to standard deviation

```{r}
hypers <- names(fit$par)
```

There are `r length(hypers)` hyperparameters, comprised ofa `r data.frame(hypers) %>% filter(str_detect(hypers, "^logit_")) %>% nrow()` on the logit scale, and `r data.frame(hypers) %>% filter(str_detect(hypers, "^log_")) %>% nrow()` on the log scale.

We can calculate their posterior standard deviations using the delta method via `TMB::sdreport` as follows:

```{r}
testrun <- parallel::mcparallel({TMB::sdreport(obj = fit$obj, par.fixed = fit$par, getJointPrecision = TRUE)})
sd_out <- parallel::mccollect(testrun, wait = TRUE, timeout = 0, intermediate = FALSE)
sd_out <- sd_out[[1]]
```

The marginal standard deviations for each of the hyperparameters are:

```{r sd, class.source = 'fold-hide', fig.cap="Marginal standard deviations."}
cbpalette <- c("#56B4E9","#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

sd <- sqrt(diag(sd_out$cov.fixed))

data.frame(par = names(sd), sd = unname(sd)) %>%
  mutate(
    start = str_extract(par, "^[^_]+"),
    scale = fct_recode(start, "Log" = "log", "Logit" = "logit", "Other" = "OmegaT")
  ) %>%
  ggplot(aes(x = reorder(par, sd), y = sd, col = scale)) +
    geom_point(size = 2) +
    coord_flip() +
    scale_color_manual(values = c("#56B4E9","#009E73", "#E69F00")) +
    lims(y = c(0, 2)) +
    theme_minimal() +
    labs(x = "", y = "Marginal standard deviation", col = "Scale")

ggsave("marginal-sd.png", height = 3.5, w = 6.25)
```

Or, more generally, the covariance matrix is:

```{r cov, class.source = 'fold-hide', fig.cap="Covariance matrix."}
C <- sd_out$cov.fixed
(C_plot <- plot_matrix(C))
```

One way to place grid points would be proportional to the standard deviations shown in the plot above.
For example, we could a product grid with the minimum number of points (2) could be obtained by choosing `k = 2` points in the dimension with the highest SD and `k = 1` point in every other dimension.
This would look as follows:

```{r}
base_grid <- sd_levels_ghe_grid(
  dim = length(hypers),
  level = c(1, 2),
  cut_off = c(0, 1.9),
  sd = sqrt(diag(sd_out$cov.fixed))
)

base_grid
```

The total number of points for this grid is `r prod(base_grid$level)`.
Let's try fitting this, and see how long it takes.
First prepare the data:

```{r, class.source = 'fold-hide', results="hide", message=FALSE}
area_merged <- read_sf(system.file("extdata/demo_areas.geojson", package = "naomi"))
pop_agesex <- read_csv(system.file("extdata/demo_population_agesex.csv", package = "naomi"))
survey_hiv_indicators <- read_csv(system.file("extdata/demo_survey_hiv_indicators.csv", package = "naomi"))
art_number <- read_csv(system.file("extdata/demo_art_number.csv", package = "naomi"))
anc_testing <- read_csv(system.file("extdata/demo_anc_testing.csv", package = "naomi"))
pjnz <- system.file("extdata/demo_mwi2019.PJNZ", package = "naomi")
spec <- naomi::extract_pjnz_naomi(pjnz)

scope <- "MWI"
level <- 4
calendar_quarter_t1 <- "CY2016Q1"
calendar_quarter_t2 <- "CY2018Q3"
calendar_quarter_t3 <- "CY2019Q4"
prev_survey_ids  <- c("DEMO2016PHIA", "DEMO2015DHS")
artcov_survey_ids  <- "DEMO2016PHIA"
vls_survey_ids <- NULL
recent_survey_ids <- "DEMO2016PHIA"
artnum_calendar_quarter_t1 <- "CY2016Q1"
artnum_calendar_quarter_t2 <- "CY2018Q3"
anc_clients_year2 <- 2018
anc_clients_year2_num_months <- 9
anc_prevalence_year1 <- 2016
anc_prevalence_year2 <- 2018
anc_art_coverage_year1 <- 2016
anc_art_coverage_year2 <- 2018

naomi_mf <- naomi_model_frame(
  area_merged,
  pop_agesex,
  spec,
  scope = scope,
  level = level,
  calendar_quarter_t1,
  calendar_quarter_t2,
  calendar_quarter_t3
)

naomi_data <- select_naomi_data(
  naomi_mf,
  survey_hiv_indicators,
  anc_testing,
  art_number,
  prev_survey_ids,
  artcov_survey_ids,
  recent_survey_ids,
  vls_survey_ids,
  artnum_calendar_quarter_t1,
  artnum_calendar_quarter_t2,
  anc_prevalence_year1,
  anc_prevalence_year2,
  anc_art_coverage_year1,
  anc_art_coverage_year2
)

tmb_inputs <- prepare_tmb_inputs(naomi_data)
tmb_inputs_simple <- local_exclude_inputs(tmb_inputs)
```

Now let's fit the model with `aghq`, and time how long it takes:

```{r}
start <- Sys.time()
quad <- fit_aghq(tmb_inputs, k = 1, basegrid = base_grid)
end <- Sys.time()

end - start
```

Verifying the number of grid points used:

```{r}
stopifnot(nrow(quad$modesandhessians) == 2)
```

How are these grid points spaced across the 24 dimensions?^[Note that this section was added in later after in order to build intuitions, and doesn't flow very naturally!]

```{r}
quad$modesandhessians[, 1:24] %>%
  pivot_longer(cols = everything()) %>%
  mutate(id = rep(1:24, times = 2)) %>%
  ggplot(aes(x = reorder(name, id), y = value)) +
    geom_jitter(shape = 1, height = 0, width = 0.2) +
    coord_flip() +
    labs(x = "", y = "Value") + 
    theme_minimal()
```

Interesting to note that for the Cholesky adapted grid with `levels = c(1, 1, ..., 2, 1, ..., 1)` there is variation in all of the dimensions greater than or equal to the 16th.
This is related to the Cholesky adaption being based on a lower triangular matrix.
To verify this, we can look at the differences between the two grid points.
For many of the dimensions the difference is too small to see on the plot above.

```{r}
t(unname(diff(as.matrix(quad$modesandhessians[, 1:24]))))
```

Let's also verify that adaption is happening as we expect it to be:

```{r}
base_grid_copy <- rlang::duplicate(base_grid)

#' Rescale it according to the mode and Hessian with Cholesky factor
mvQuad::rescale(base_grid, m = quad$optresults$mode, C = Matrix::forceSymmetric(solve(quad$optresults$hessian)), dec.type = 2)

#' Verify that the adapted grid has the same nodes and are used by aghq::aghq
stopifnot(all(mvQuad::getNodes(base_grid) == unname(as.matrix(quad$modesandhessians[, 1:24]))))
```

How would a spectral grid behave with the same choice of `levels`?
Notice that there is a difference for all of the 24 dimenions when `dec.type = 1` (spectral) is used:

```{r}
mvQuad::rescale(base_grid_copy, m = quad$optresults$mode, C = Matrix::forceSymmetric(solve(quad$optresults$hessian)), dec.type = 1)
t(diff(mvQuad::getNodes(base_grid_copy)))
```

This model fit in a very reasonable amount of time.
More generally, it would be of interest to know more about the relationship between the number of points in the grid, and the length of time it takes run `fit_aghq`.
Are there any theoretical considerations statements that can be made e.g. scaling $\mathcal{O}(n)$ where $n$ is the number of grid points?

## Limitations

Some hyperparameters have higher standard deviation than others in part due to the different scales that they are on.
For example if the hyperparameter is on the logit scale then it looks as though it will have a higher standard deviation than another those on the log scale (Figure \@ref(fig:sd)).
The average standard deviation for hyperparameters starting with `"log_"` is `r round(data.frame(par = names(sd), sd = unname(sd)) %>% filter(str_detect(par, "^log_")) %>% summarise(mean_sd = mean(sd)) %>% pull(mean_sd), 3)` as compared with `r round(data.frame(par = names(sd), sd = unname(sd)) %>% filter(str_detect(par, "^logit_")) %>% summarise(mean_sd = mean(sd)) %>% pull(mean_sd), 3)` for those starting with `"logit_"`.

It is not clear that this reflects our intuitions regarding which hyperparameters it is important to take greater account of.
Perhaps ideally we would like to know how much variation in each hyperparameter effects variation in model outputs, and then take that into account in the importance we place on each dimension.

# Eigendecomposition

Another option to using the standard deviations is to use principle components analysis (PCA) to create a grid on a subspace of $\mathbb{R}^{24}$ which retains most of the variation.

```{r}
m <- sd_out$par.fixed
eigenC <- eigen(C)
lambda <- eigenC$values
Lambda <- diag(lambda)
E <- eigenC$vectors
```

Such that `C` can be obtained by $E \Lambda E^\top$, or in code `E %*% diag(lambda) %*% t(E)`:

```{r}
max(C - (E %*% Lambda %*% t(E))) < 10E-12
```

The relative contributions of each principle component are given by $\lambda_i / \sum_i \lambda_i$:

```{r scree, class.source = 'fold-hide', fig.cap="Scree plot."}
plot_total_variation(eigenC, label_x = 20)

ggsave("total-variation.png", h = 3, w = 6.25)

tv <- cumsum(eigenC$values) / sum(eigenC$values)
```

Based on Figure \@ref(fig:scree) with 5 dimensions included, we can explain `r signif(100 * tv[5], 3)`% of the total variation.
Or, with 10 dimensions included, that percentage increases to `r signif(100 * tv[10], 3)`%.

What do the PC loadings (columns of the matrix `E`) look like?

```{r pc-loadings, class.source = 'fold-hide', fig.cap="PC loadings."}
rownames(E) <- hypers
plot_pc_loadings(eigenC)

ggsave("pc-loadings.png", h = 3.5, w = 6.25)
```

## Keeping a smaller number of dimensions

Let's start by keeping `s = 8` dimensions.
We can create a dense GHQ grid with `k = 3` on 8 dimensions.

```{r}
s <- 8
d <- dim(E)[1]
gg_s <- mvQuad::createNIGrid(dim = s, type = "GHe", level = 3) 
```

This grid has $3^8 = 6561$ nodes:

```{r}
(n_nodes <- nrow(mvQuad::getNodes(gg_s)))
```

How does the reconstruction of the covariance matrix with `r s` components look?

```{r cov-reconstruction, class.source = 'fold-hide', fig.cap="Hessian matrix recons."}
E_s <- E[, 1:s]
Lambda_s <- Lambda[1:s, 1:s]
C_s <- E_s %*% Lambda_s %*% t(E_s)

C_s_plot <- plot_matrix(C_s)
{C_plot + theme(legend.position = "none") + labs(subtitle = "Full rank")} + {C_s_plot + theme(legend.position = "none") + labs(subtitle = "Reduced rank")}

ggsave("reduced-rank.png", h = 3.5, w = 6.25)
```

## Scoping the application to Naomi

Let's see how large the PCA approach would make the grids for Naomi:

```{r}
m <- sd_out$par.fixed
C <- sd_out$cov.fixed

gg3 <- pca_rescale(m = m, C = C, s = 3, k = 3)
nrow(mvQuad::getNodes(gg3))

gg5 <- pca_rescale(m = m, C = C, s = 5, k = 3)
nrow(mvQuad::getNodes(gg5))

gg7 <- pca_rescale(m = m, C = C, s = 7, k = 3)
nrow(mvQuad::getNodes(gg7))

gg9 <- pca_rescale(m = m, C = C, s = 9, k = 3)
nrow(mvQuad::getNodes(gg9))
```

# Dealing with the scales issue

Eventually we want to put parameters on the log and logit scales in parity.

## Using the correlation matrix

```{r}
C <- sd_out$cov.fixed
d <- 1 / (sqrt(diag(C)))
R <- diag(d) %*% C %*% diag(d)
plot_matrix(R)
```

```{r}
eigenR <- eigen(R)
plot_total_variation(eigenR, label_x = 5)
```

```{r}
rownames(eigenR$vectors) <- hypers
plot_pc_loadings(eigenR)
```

## Scale standardised system

```{r}
C <- sd_out$cov.fixed
var <- diag(C)

df_std <- data.frame(var = var) %>%
  tibble::rownames_to_column("par") %>%
  mutate(
    scale = fct_recode(str_extract(par, "^[^_]+"), "Log" = "log", "Logit" = "logit", "Other" = "OmegaT")
  ) %>%
  group_by(scale) %>%
  mutate(mean_var = mean(var)) %>%
  ungroup() %>%
  mutate(var_std = var / mean_var)

df_std %>%  
  pivot_longer(
    cols = c("var", "var_std"),
    names_to = "method",
    values_to = "value"
  ) %>%
  mutate(
    method = fct_recode(method, "Default" = "var", "Scale standardised" = "var_std"),
    start = str_extract(par, "^[^_]+"),
    scale = fct_recode(start, "Log" = "log", "Logit" = "logit", "Other" = "OmegaT") 
  ) %>%
  ggplot(aes(x = reorder(par, value), y = value, col = scale)) +
    geom_point() +
    facet_grid(~ method) +
    coord_flip() +
    scale_color_manual(values = c("#56B4E9","#009E73", "#E69F00")) +
    labs(x = "Hyperparameter", y = "Marginal variance", col = "Scale") +
    theme_minimal()

Rs <- diag(1 / sqrt(df_std$mean_var)) %*% C %*% diag(1 / sqrt(df_std$mean_var))
plot_matrix(Rs)

eigenRs <- eigen(Rs)
plot_total_variation(eigenRs, label_x = 5)

rownames(eigenRs$vectors) <- hypers
plot_pc_loadings(eigenRs)
```

# Testing pruning

Remove any nodes which have small enough weight.

```{r}
#' TODO!
```
