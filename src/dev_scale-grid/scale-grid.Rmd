---
title: "Scaling up the hyperparameter grid for Naomi"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** We have previously fit empirical Bayes style approaches to the Naomi model, fixing hyperparameter values to a single value.
  
  **Task** We would like to begin to integrate over the hyperparameters using multiple nodes. The problem is that there are >20 hyperparameters, and so using any type of dense grid is not practical. In this notebook we investigate approaches to using a smaller number of grid points to cover the space, including 1) allocating points in proporiton to posterior standard deviation 2) using principle components analysis.
editor_options: 
  markdown: 
    wrap: sentence
---

# `TMB` fit

Start by fitting the model with `TMB`.
This is quick to do and allows us to get a model we can play with to figure out which hyperparmeters are important.
In particular we obtain the mode and Hessian at the mode of the Laplace approximation to the hyperparameter marginal posterior.

```{r, class.source = 'fold-hide'}
compile("naomi_simple.cpp")
dyn.load(dynlib("naomi_simple"))

tmb <- readRDS("depends/tmb.rds")
fit <- tmb$fit
```

# Point concentration proportional to standard deviation

The names of the hyperparameters are:

```{r}
(hypers <- names(fit$par))
```

There are `r length(hypers)` of them, if you'd like to count.
Including `r data.frame(hypers) %>% filter(str_detect(hypers, "^logit_")) %>% nrow()` on the logit scale, and `r data.frame(hypers) %>% filter(str_detect(hypers, "^log_")) %>% nrow()` on the log scale.

We can calculate their posterior standard deviations using the delta method via `TMB::sdreport` as follows:

```{r}
sd_out <- sdreport(obj = fit$obj, par.fixed = fit$par, getJointPrecision = TRUE)
```

The marginal standard deviations for each of the hyperparameters are:

```{r sd, class.source = 'fold-hide', fig.cap="Marginal standard deviations."}
sd <- sqrt(diag(sd_out$cov.fixed))

data.frame(par = names(sd), sd = unname(sd)) %>%
  ggplot(aes(x = reorder(par, sd), y = sd)) +
    geom_point(alpha = 0.6) +
    coord_flip() +
    lims(y = c(0, 2)) +
    theme_minimal() +
    labs(x = "", y = "SD from TMB")
```

Or, more generally, the covariance matrix is:

```{r cov, class.source = 'fold-hide', fig.cap="Covariance matrix."}
C <- sd_out$cov.fixed
C_df <- reshape2::melt(as.matrix(C))

C_plot <- C_df %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  labs(x = "", y = "", fill = "C[i, j]") +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(x = "i", y = "j") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

C_plot
```

One way to place grid points would be proportional to the standard deviations shown in the plot above.
For example, we could a product grid with the minimum number of points (2) could be obtained by choosing `k = 2` points in the dimension with the highest SD and `k = 1` point in every other dimension.
This would look as follows:

```{r}
base_grid <- sd_levels_ghe_grid(
  dim = length(hypers),
  level = c(1, 2),
  cut_off = c(0, 1.9),
  sd = sqrt(diag(sd_out$cov.fixed))
)

base_grid
```

The total number of points for this grid is 2:

```{r}
prod(base_grid$level)
```

Let's try fitting this, and see how long it takes.
First prepare the data:

```{r, class.source = 'fold-hide', results="hide", message=FALSE}
area_merged <- read_sf(system.file("extdata/demo_areas.geojson", package = "naomi"))
pop_agesex <- read_csv(system.file("extdata/demo_population_agesex.csv", package = "naomi"))
survey_hiv_indicators <- read_csv(system.file("extdata/demo_survey_hiv_indicators.csv", package = "naomi"))
art_number <- read_csv(system.file("extdata/demo_art_number.csv", package = "naomi"))
anc_testing <- read_csv(system.file("extdata/demo_anc_testing.csv", package = "naomi"))
pjnz <- system.file("extdata/demo_mwi2019.PJNZ", package = "naomi")
spec <- naomi::extract_pjnz_naomi(pjnz)

scope <- "MWI"
level <- 4
calendar_quarter_t1 <- "CY2016Q1"
calendar_quarter_t2 <- "CY2018Q3"
calendar_quarter_t3 <- "CY2019Q4"
prev_survey_ids  <- c("DEMO2016PHIA", "DEMO2015DHS")
artcov_survey_ids  <- "DEMO2016PHIA"
vls_survey_ids <- NULL
recent_survey_ids <- "DEMO2016PHIA"
artnum_calendar_quarter_t1 <- "CY2016Q1"
artnum_calendar_quarter_t2 <- "CY2018Q3"
anc_clients_year2 <- 2018
anc_clients_year2_num_months <- 9
anc_prevalence_year1 <- 2016
anc_prevalence_year2 <- 2018
anc_art_coverage_year1 <- 2016
anc_art_coverage_year2 <- 2018

naomi_mf <- naomi_model_frame(
  area_merged,
  pop_agesex,
  spec,
  scope = scope,
  level = level,
  calendar_quarter_t1,
  calendar_quarter_t2,
  calendar_quarter_t3
)

naomi_data <- select_naomi_data(
  naomi_mf,
  survey_hiv_indicators,
  anc_testing,
  art_number,
  prev_survey_ids,
  artcov_survey_ids,
  recent_survey_ids,
  vls_survey_ids,
  artnum_calendar_quarter_t1,
  artnum_calendar_quarter_t2,
  anc_prevalence_year1,
  anc_prevalence_year2,
  anc_art_coverage_year1,
  anc_art_coverage_year2
)

tmb_inputs <- prepare_tmb_inputs(naomi_data)
tmb_inputs_simple <- local_exclude_inputs(tmb_inputs)
```

Now let's fit the model with `aghq`, and time how long it takes:

```{r}
start <- Sys.time()
quad <- fit_aghq(tmb_inputs, basegrid = base_grid)
end <- Sys.time()

end - start
```

This model fit in a very reasonable amount of time.
More generally, it would be of interest to know more about the relationship between the number of points in the grid, and the length of time it takes run `fit_aghq`.
Are there any theoretical considerations statements that can be made e.g. scaling $\mathcal{O}(n)$ where $n$ is the number of grid points?

## Limitations

Some hyperparameters have higher standard deviation than others in part due to the different scales that they are on.
For example if the hyperparameter is on the logit scale then it looks as though it will have a higher standard deviation than another those on the log scale (Figure \@ref(fig:sd)).
The average standard deviation for hyperparameters starting with `"log_"` is `r round(data.frame(par = names(sd), sd = unname(sd)) %>% filter(str_detect(par, "^log_")) %>% summarise(mean_sd = mean(sd)) %>% pull(mean_sd), 3)` as compared with `r round(data.frame(par = names(sd), sd = unname(sd)) %>% filter(str_detect(par, "^logit_")) %>% summarise(mean_sd = mean(sd)) %>% pull(mean_sd), 3)` for those starting with `"logit_"`.

It is not clear that this reflects our intuitions regarding which hyperparameters it is important to take greater account of.
Perhaps ideally we would like to know how much variation in each hyperparameter effects variation in model outputs, and then take that into account in the importance we place on each dimension.

## Points of confusion

How does this approach interact with the rescaling which occurs in adaptive quadrature?
That is to say $z \mapsto \hat \theta + Lz$ with $LL^\top = H$.
Do each of the dimensions of $z$ reflect dimensions of $Lz$?

# Eigendecomposition

Another option to using the standard deviations is to use principle components analysis (PCA) to create a grid on a subspace of $\mathbb{R}^{24}$ which retains most of the variation.

```{r}
m <- sd_out$par.fixed
eigenC <- eigen(C)
lambda <- eigenC$values
Lambda <- diag(lambda)
E <- eigenC$vectors
```

Such that `C` can be obtained by $E \Lambda E^\top$, or in code `E %*% diag(lambda) %*% t(E)`:

```{r}
max(C - (E %*% Lambda %*% t(E))) < 10E-12
```

The relative contributions of each principle component are given by $\lambda_i / \sum_i \lambda_i$:

```{r scree, class.source = 'fold-hide', fig.cap="Scree plot."}
tv_df <- data.frame(
  n = 1:length(lambda),
  tv = cumsum(lambda / sum(lambda))
)

ggplot(tv_df, aes(x = n, y = tv)) +
  geom_point() +
  geom_hline(yintercept = 0.9, col = "grey", linetype = "dashed") +
  annotate("text", x = 20, y = 0.875, label = "90% of total variation explained", col = "grey") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "PCA dimensions included", y = "Total variation explained") +
  theme_minimal()
```

Based on Figure \@ref(fig:scree) with 5 dimensions included, we can explain `r round(100 * filter(tv_df, n == 5) %>% pull(tv), 1)`% of the total variation.
Or, with 10 dimensions included, that percentage increases to `r round(100 * filter(tv_df, n == 10) %>% pull(tv), 1)`%.

What do the PC loadings (columns of the matrix `E`) look like?

```{r pc-loadings, class.source = 'fold-hide', fig.cap="PC loadings."}
reshape2::melt(as.matrix(E)) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  labs(x = "", y = "", fill = "E[i, j]") +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(x = "PC loading", y = "Hyper") +
  theme_minimal()
```

## Keeping a smaller number of dimensions

Let's start by keeping `s = 5` dimensions.
We can create a dense GHQ grid with `k = 2` on 5 dimensions.

```{r}
s <- 5
d <- dim(E)[1]
gg_s <- mvQuad::createNIGrid(dim = s, type = "GHe", level = 2) 
```

This grid has $2^5 = 32$ nodes:

```{r}
(n_nodes <- nrow(mvQuad::getNodes(gg_s)))
```

How does the reconstruction of the covariance matrix with `r s` components look?

```{r cov-reconstruction, class.source = 'fold-hide', fig.cap="PC loadings."}
E_s <- E[, 1:s]
Lambda_s <- Lambda[1:s, 1:s]
C_s <- E_s %*% Lambda_s %*% t(E_s)

C_s_plot <- reshape2::melt(as.matrix(C_s)) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  labs(x = "", y = "", fill = "C_s[i, j]") +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(x = "i", y = "j") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    legend.position = "bottom"
  )

{C_plot + theme(legend.position = "bottom")} + C_s_plot
```

Create a function to do the PCA rescaling, which also adapts according to the mean and reweights the nodes:

```{r}
#' @param m Mean vector
#' @param C Covariance matrix
#' @param s Small grid dimension
#' @param k Number of points per small grid dimension
pca_rescale <- function(m, C, s, k) {
  d <- nrow(C)
  stopifnot(d == length(m))
  eigenC <- eigen(C)
  lambda <- eigenC$values
  Lambda <- diag(lambda)
  E <- eigenC$vectors
  E_s <- E[, 1:s] 
  gg_s <- mvQuad::createNIGrid(dim = s, type = "GHe", level = k) 
  nodes_out <- t(E_s %*% diag(lambda[1:s]^{0.5}, ncol = s) %*% t(mvQuad::getNodes(gg_s)))
  for(j in 1:d) nodes_out[, j] <- nodes_out[, j] + m[j]
  weights_out <- mvQuad::getWeights(gg_s) * as.numeric(mvQuad::getWeights(mvQuad::createNIGrid(dim = d - s, type = "GHe", level = 1)))
  
  # Putting things into a mvQuad format manually
  gg <- mvQuad::createNIGrid(dim = d, type = "GHe", level = 1)
  gg$level <- rep(NA, times = d)
  gg$ndConstruction <- "PCA"
  gg$nodes <- nodes_out
  gg$weights <- weights_out
  return(gg)
}
```

Adaptive Gauss-Hermite quadrature involves

## Testing with simple examples

Here we run some unit tests to make sure, visually and numerically, that the methods are working as expected.

### Gaussian 2D function

A simple 2D Gaussian example:

$$
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix}
\sim
\mathcal{N}
\left(
\begin{pmatrix}
1 \\
3/2
\end{pmatrix},
\begin{pmatrix}
2 & 1 \\
1 & 1
\end{pmatrix}
\right)
$$

```{r gaussian-2d, class.source = 'fold-hide', fig.cap="Gaussian 2D objective function."}
m <- c(1, 1.5)
C <- matrix(c(2, 1, 1, 1), ncol = 2)

obj <- function(theta) {
  mvtnorm::dmvnorm(theta, mean = m, sigma = C, log = TRUE)
}

ff <- list(
  fn = obj,
  gr = function(theta) numDeriv::grad(obj, theta),
  he = function(theta) numDeriv::hessian(obj, theta)
)

grid <- expand.grid(
  theta1 = seq(-2, 5, length.out = 700),
  theta2 = seq(-2, 5, length.out = 700)
)

ground_truth <- cbind(grid, pdf = exp(obj(grid)))

plot <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = multi.utils::cbpalette()[1]) +
  coord_fixed(xlim = c(-2, 4.5), ylim = c(-2, 4.5), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal()

plot
```

Define four AGHQ-PCA grids, with `k` running from 1 to 7:

```{r}
pca_grid_1 <- pca_rescale(m = m, C = C, s = 1, k = 1)
pca_grid_3 <- pca_rescale(m = m, C = C, s = 1, k = 3)
pca_grid_5 <- pca_rescale(m = m, C = C, s = 1, k = 5)
pca_grid_7 <- pca_rescale(m = m, C = C, s = 1, k = 7)
```

For example, the version with 3 points looks as follows:

```{r pca-grid-3, class.source = 'fold-hide', fig.cap="AGHQ-PCA grid for Gaussian with $k = 3$."}
plot_points <- function(gg) {
  plot +
    geom_point(
      data = mvQuad::getNodes(gg) %>%
        as.data.frame() %>%
        mutate(weights = mvQuad::getWeights(gg)),
      aes(x = V1, y = V2, size = weights),
      alpha = 0.8,
      col = multi.utils::cbpalette()[2],
      inherit.aes = FALSE
    ) +
    scale_size_continuous(range = c(1, 2)) +
    labs(x = "", y = "", size = "Weight") +
    theme_minimal()
}

plot_points(pca_grid_3)
```

We can optimise the objective function using the BFGS algorithm to recover the mode and Hessian at the mode.
Of course, for this example, these are known in advance as we are dealing with a Gaussian, but in general this is how they will be computed.

```{r}
opt_bfgs <- aghq::optimize_theta(ff, m, control = default_control(method = "BFGS"))
opt_bfgs$mode
opt_bfgs$hessian
Matrix::forceSymmetric(solve(opt_bfgs$hessian))
```

Now we can perform regular AGHQ with a product grid (again with a range of values for `k`):

```{r}
norm_bfgs_1 <- aghq::normalize_logpost(opt_bfgs, k = 1)
norm_bfgs_3 <- aghq::normalize_logpost(opt_bfgs, k = 3)
norm_bfgs_5 <- aghq::normalize_logpost(opt_bfgs, k = 5)
norm_bfgs_7 <- aghq::normalize_logpost(opt_bfgs, k = 7)
norm_bfgs <- list(norm_bfgs_1, norm_bfgs_3, norm_bfgs_5, norm_bfgs_7)
```

As well as the the AGHQ-PCA grids:

```{r}
norm_pca_bfgs_1 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_1, adapt = FALSE)
norm_pca_bfgs_3 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_3, adapt = FALSE)
norm_pca_bfgs_5 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_5, adapt = FALSE)
norm_pca_bfgs_7 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_7, adapt = FALSE)
norm_pca_bfgs <- list(norm_pca_bfgs_1, norm_pca_bfgs_3, norm_pca_bfgs_5, norm_pca_bfgs_7)
```

And a second way of producing the AGHQ-PCA grids directly using `mvQuad`:

```{r, fig.cap="Alternative approach AGHQ-PCA grid for Gaussian with $k = 3$."}
pca2_grid_1 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(1, 1))
mvQuad::rescale(pca2_grid_1, m = m, C = C, dec.type = 1)

pca2_grid_3 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(3, 1))
mvQuad::rescale(pca2_grid_3, m = m, C = C, dec.type = 1)

pca2_grid_5 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(5, 1))
mvQuad::rescale(pca2_grid_5, m = m, C = C, dec.type = 1)

pca2_grid_7 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(7, 1))
mvQuad::rescale(pca2_grid_7, m = m, C = C, dec.type = 1)

plot_points(pca2_grid_3)

norm_pca2_bfgs_1 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_1, adapt = FALSE)
norm_pca2_bfgs_3 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_3, adapt = FALSE)
norm_pca2_bfgs_5 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_5, adapt = FALSE)
norm_pca2_bfgs_7 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_7, adapt = FALSE)
norm_pca2_bfgs <- list(norm_pca2_bfgs_1, norm_pca2_bfgs_3, norm_pca2_bfgs_5, norm_pca2_bfgs_7)
```

The density is already normalised, so it has a normalising constant of 1, and a log normalising constant of $\log(1) = 0$:

```{r, class.source = 'fold-hide', fig.cap="Unit test for the Gaussian example"}
truelognormconst <- 0

results <- data.frame(
  "method" = c("Truth", paste0("aghq, k = ", c(1, 3, 5, 7)), paste0("aghq-pca, k = ", c(1, 3, 5, 7)), paste0("aghq-pca2, k = ", c(1, 3, 5, 7))),
  "lognormconst" = c(truelognormconst, sapply(c(norm_bfgs, norm_pca_bfgs, norm_pca2_bfgs), function(x) x$lognormconst)),
  "type" = c("Truth", rep("AGHQ", 4), rep("AGHQ-PCA", 4), rep("AGHQ-PCA2", 4))
)

ggplot(results, aes(x = method, y = lognormconst, col = as.factor(type))) +
  geom_point(size = 1) +
  labs(x = "Method", y = "Log normalising constant", col = "") +
  scale_color_manual(values = multi.utils::cbpalette()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, hjust = 0.5))
```

### Non-Gaussian 2D function

A more complicated two dimensional function from Alex's [unit tests](https://github.com/awstringer1/aghq/blob/master/tests/testthat/setup-01-optimization.R) for the `aghq` package.

```{r non-gaussian, class.source = 'fold-hide', fig.cap="Non-Gaussian 2D objective function."}
set.seed(84343124)

logfteta <- function(eta, y) {
  n <- length(y)
  n1 <- ceiling(n / 2)
  n2 <- floor(n / 2)
  y1 <- y[1:n1]
  y2 <- y[(n1 + 1):(n1 + n2)]
  eta1 <- eta[1]
  eta2 <- eta[2]
  sum(y1) * eta1 - (length(y1) + 1) * exp(eta1) - sum(lgamma(y1+1)) + eta1 +
    sum(y2) * eta2 - (length(y2) + 1) * exp(eta2) - sum(lgamma(y2+1)) + eta2
}

n1 <- 5
n2 <- 5
n <- n1 + n2
y1 <- rpois(n1, 5)
y2 <- rpois(n2, 5)

truemode <- c(log((sum(y1) + 1) / (length(y1) + 1)), log((sum(y2) + 1) / (length(y2) + 1)))
truelogint <- function(y) lgamma(1 + sum(y)) - (1 + sum(y)) * log(length(y) + 1) - sum(lgamma(y + 1))
truelognormconst <- truelogint(y1) + truelogint(y2)

obj <- function(x) logfteta(x, c(y1, y2))

ff <- list(
  fn = obj,
  gr = function(x) numDeriv::grad(obj, x),
  he = function(x) numDeriv::hessian(obj, x)
)

grid <- expand.grid(
  theta1 = seq(0, 4, length.out = 400),
  theta2 = seq(0, 4, length.out = 400)
)

ground_truth <- data.frame(grid) %>%
  rowwise() %>%
  mutate(pdf = exp(obj(x = c(theta1, theta2))))

plot <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = multi.utils::cbpalette()[1]) +
  coord_fixed(xlim = c(0.8, 2), ylim = c(0.8, 2), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal()

plot
```

Optimise using BFSG:

```{r}
opt_bfgs <- aghq::optimize_theta(ff, c(1.5, 1.5), control = default_control(method = "BFGS"))
(m <- opt_bfgs$mode)
(C <- Matrix::forceSymmetric(solve(opt_bfgs$hessian)))
eigen(C)
```

```{r, fig.cap="AGHQ grid for non-Gaussian with $k = 3$."}
grid_bfgs_1 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = 1)
mvQuad::rescale(grid_bfgs_1, m = m, C = C)

grid_bfgs_3 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = 3)
mvQuad::rescale(grid_bfgs_3, m = m, C = C)

grid_bfgs_5 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = 5)
mvQuad::rescale(grid_bfgs_5, m = m, C = C)

grid_bfgs_7 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = 7)
mvQuad::rescale(grid_bfgs_7, m = m, C = C)

norm_bfgs_1 <- aghq::normalize_logpost(opt_bfgs, 1, 1)
norm_bfgs_3 <- aghq::normalize_logpost(opt_bfgs, 3, 1)
norm_bfgs_5 <- aghq::normalize_logpost(opt_bfgs, 5, 1)
norm_bfgs_7 <- aghq::normalize_logpost(opt_bfgs, 7, 1)
norm_bfgs <- list(norm_bfgs_1, norm_bfgs_3, norm_bfgs_5, norm_bfgs_7)

plot_points(grid_bfgs_3)
```

```{r, fig.cap="AGHQ-PCA grid for non-Gaussian with $k = 3$."}
pca_grid_bfgs_1 <- pca_rescale(m = m, C = C, s = 1, k = 1)
pca_grid_bfgs_3 <- pca_rescale(m = m, C = C, s = 1, k = 3)
pca_grid_bfgs_5 <- pca_rescale(m = m, C = C, s = 1, k = 5)
pca_grid_bfgs_7 <- pca_rescale(m = m, C = C, s = 1, k = 7)

norm_pca_bfgs_1 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_bfgs_1, adapt = FALSE)
norm_pca_bfgs_3 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_bfgs_3, adapt = FALSE)
norm_pca_bfgs_5 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_bfgs_5, adapt = FALSE)
norm_pca_bfgs_7 <- local_normalize_logpost(opt_bfgs, basegrid = pca_grid_bfgs_7, adapt = FALSE)
norm_pca_bfgs <- list(norm_pca_bfgs_1, norm_pca_bfgs_3, norm_pca_bfgs_5, norm_pca_bfgs_7)

plot_points(pca_grid_bfgs_3)
```

```{r, fig.cap="Alternative approach AGHQ-PCA grid for non-Gaussian with $k = 3$."}
pca2_grid_1 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(1, 1))
mvQuad::rescale(pca2_grid_1, m = m, C = C, dec.type = 1)

pca2_grid_3 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(3, 1))
mvQuad::rescale(pca2_grid_3, m = m, C = C, dec.type = 1)

pca2_grid_5 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(5, 1))
mvQuad::rescale(pca2_grid_5, m = m, C = C, dec.type = 1)

pca2_grid_7 <- mvQuad::createNIGrid(dim = 2, type = "GHe", level = c(7, 1))
mvQuad::rescale(pca2_grid_7, m = m, C = C, dec.type = 1)

norm_pca2_bfgs_1 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_1, adapt = FALSE)
norm_pca2_bfgs_3 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_3, adapt = FALSE)
norm_pca2_bfgs_5 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_5, adapt = FALSE)
norm_pca2_bfgs_7 <- local_normalize_logpost(opt_bfgs, basegrid = pca2_grid_7, adapt = FALSE)
norm_pca2_bfgs <- list(norm_pca2_bfgs_1, norm_pca2_bfgs_3, norm_pca2_bfgs_5, norm_pca2_bfgs_7)

plot_points(pca2_grid_3)
```

```{r, class.source = 'fold-hide', fig.cap="Unit test for the non-Gaussian example"}
results <- data.frame(
  "method" = c("Truth", paste0("aghq, k = ", c(1, 3, 5, 7)), paste0("aghq-pca, k = ", c(1, 3, 5, 7)), paste0("aghq-pca2, k = ", c(1, 3, 5, 7))),
  "lognormconst" = c(truelognormconst, sapply(c(norm_bfgs, norm_pca_bfgs, norm_pca2_bfgs), function(x) x$lognormconst)),
  "type" = as.factor(c("Truth", rep("AGHQ", 4), rep("AGHQ-PCA", 4), rep("AGHQ-PCA2", 4)))
)

ggplot(results, aes(x = method, y = lognormconst, col = type)) +
  geom_point(size = 1) +
  labs(x = "Method", y = "Log normalising constant", col = "") +
  scale_color_manual(values = multi.utils::cbpalette()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, hjust = 0.5))
```

## Application to Naomi

Let's apply this PCA approach to the grids for the Naomi model:

```{r}
m <- sd_out$par.fixed
C <- sd_out$cov.fixed

gg3 <- pca_rescale(m = m, C = C, s = 3, k = 2)
nrow(mvQuad::getNodes(gg3))

gg5 <- pca_rescale(m = m, C = C, s = 5, k = 2)
nrow(mvQuad::getNodes(gg5))

gg7 <- pca_rescale(m = m, C = C, s = 7, k = 2)
nrow(mvQuad::getNodes(gg7))

gg9 <- pca_rescale(m = m, C = C, s = 9, k = 2)
nrow(mvQuad::getNodes(gg9))
```
