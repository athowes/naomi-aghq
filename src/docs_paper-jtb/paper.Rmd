---
title: Fast approximate Bayesian inference of HIV indicators using PCA adaptive Gauss-Hermite quadrature
author:
  - name: Adam Howes
    email: ath19@ic.ac.uk
    affiliation: A
    correspondingauthor: true
  - name: Alex Stringer
    email: alex.stringer@uwaterloo.ca
    affiliation: B
  - name: Seth R. Flaxman
    email: seth.flaxman@cs.ox.ac.uk
    affiliation: C
  - name: Jeffrey W. Imai-Eaton
    email: jeaton@hsph.harvard.edu
    affiliation: D
address:
  - code: A
    organization: Department of Mathematics
    country: Imperial College London
  - code: B
    organization: Department of Statistics and Actuarial Science
    country: University of Waterloo
  - code: C
    organization: Department of Computer Science
    country: University of Oxford
  - code: D
    organization: Harvard T.H. Chan School of Public Health
    country: Harvard University
abstract: |
  | Naomi is a spatial evidence synthesis model used to produce district-level HIV epidemic indicators in sub-Saharan Africa. Multiple outcomes of policy interest, including HIV prevalence, HIV incidence, and antiretroviral therapy treatment coverage are jointly modelled using both household survey data and routinely reported health system data. The model is provided as a tool for countries to input their data to and generate estimates with during a yearly process supported by UNAIDS. Previously, inference has been conducted using empirical Bayes and a Gaussian approximation, implemented via the \texttt{TMB} \textsc{R} package. We propose a new inference method based on an extension of adaptive Gauss-Hermite quadrature to deal with more than 20 hyperparameters. Using data from Malawi, our method improves the accuracy of inferences for model parameters, while being substantially faster to run than Hamiltonian Monte Carlo with the No-U-Turn sampler. Our implementation leverages the existing \texttt{TMB} \textsf{C++} template for the model's log-posterior, and is compatible with any model with such a template.
keywords: 
  - Bayesian statistics
  - spatial statistics
  - evidence synthesis
  - small-area estimation
  - approximate inference
  - INLA
  - AGHQ
  - HIV epidemiology
journal: "Journal of Theoretical Biology"
date: "`r Sys.Date()`"
linenumbers: false
numbersections: true
bibliography: citations.bib
biblio-style: elsarticle-harv # author year style for natbib - use 'elsarticle-num' or 'elsarticle-num-names' for numbered scheme
classoption: preprint, 3p, authoryear # remove authoryear is not using `elsarticle-harv`
# Use a CSL with `citation_package = "default"`
# csl: https://www.zotero.org/styles/elsevier-harvard
output:
  bookdown::pdf_book:
    base_format: rticles::elsevier_article
    number_sections: yes
    keep_tex: true
    citation_package: natbib
    includes:
      in_header: preamble.tex
---

```{r echo = FALSE, message = FALSE}
library(dplyr)

options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

# Introduction

Accurate estimates of HIV indicators are crucial for mounting an effective public health response to the epidemic.
Producing timely estimates at the district level, where health systems are planned and delivered, is challenging.
Nationally-representative household surveys, while providing the most statistically reliable data, are costly to run and typically conducted only every five years.
Furthermore, sample sizes at the district level are limited.
Other data sources, such as routine health surveillance of antenatal care (ANC) clinics, are available nearer to real-time, but lack population representativeness.

The Naomi small-area estimation model [@eaton2021naomi; @esra2024improved] addresses these challenges by synthesising data from multiple sources to estimate HIV indicators at a district-level, by age and sex.
Modelling multiple data sources jointly mitigates the limitations of any single source and increases statistical power.
Software developed for Naomi (\url{https://naomi.unaids.org}) facilitates over 35 countries inputting their data and interactively generating estimates during workshops as a part of a yearly estimates process supported by UNAIDS.
Generation of estimates by country teams is an important and distinctive feature of the HIV response.
Drawing on expert knowledge about the data improves the accuracy of the process and strengthens trust in the resulting estimates, creating a virtuous cycle of data quality, use, and ownership [@noor2022country].

Naomi comprises multiple linked generalized linear mixed models (GLMMs) and presents a challenging Bayesian inference problem.
The model contains hundreds of fixed and random effect parameters and over 20 hyperparameters.
This is substantially more than the small number that can typically be handled by approaches like integrated nested Laplace approximations [INLA; @rue2009approximate].
Additionally, observations depend on multiple structured additive predictors, such that Naomi falls into the class of extended latent Gaussian models [ELGMs; @stringer2022fast].
Inference must be fast and have low memory usage, to allow interactive review and iteration of model results by workshop participants.
The scale of the model and features of its posterior geometry [@neal2003slice] make Markov chain Monte Carlo (MCMC) approaches are prohibitively slow.
Inference should also be reliable, automatic across various country settings, and require minimal manual monitoring (e.g. of MCMC convergence).

Inference for Naomi is currently conducted using an empirical Bayes (EB) approach.
A Gaussian approximation to the latent field is used via the Template Model Builder (`TMB`) [@kristensen2016tmb] \textsf{R} package.
`TMB` is gaining popularity due to its speed and flexibility, especially in spatial statistics [@osgood2023statistical.] and via the user-friendly `glmmTMB` \textsc{R} package [@brooks2017glmmTMB].
Inference in `TMB` occurs via optimisation of a \textsf{C++} template function, with the option available to use a Laplace approximation to integrate out any subset of the parameters.
`TMB` uses automatic differentiation [@fournier2012ad; @baydin2017automatic] to calculate the derivatives required for gradient-based numerical optimisation routines and the Laplace approximation.

Although the EB approach is fast, it does not take into account hyperparameter uncertainty in the latent field posterior, potentially resulting in underestimated posterior variances.
This concern could have important practical implications for use of the estimates from the Naomi model, and motivated us to look for an approach closer full Bayesian inference.
We developed a method based on adaptive Gauss-Hermite quadrature (AGHQ) extended to handle integration over many hyperparameters.
AGHQ is a quadrature method based on the theory of polynomial interpolation, and is well suited to statistical estimation problems in which the integrand is well approximated by a Gaussian multiplied by a polynomial.
For example, @bilodeau2024stochastic prove stochastic convergence rates for Bayesian posterior quantities when the normalising constant is estimated using AGHQ.
However, it is not computationally feasible to use AGHQ in greater than 20 dimensions directly, as exponentially many nodes are required.
Instead, we used principal components analysis (PCA) of the inverse curvature at the mode to find a subspace which explained most of the hyperparameter variance.
In an application to Malawi, this resulted in a grid which was tractable as it had millions of times fewer nodes than the corresponding dense grid.
Our implementation of the method makes use of the existing Naomi `TMB` template, and is immediately compatible with any model with such a template.

<!-- See https://inlabru-org.github.io/inlabru/articles/method.html -->
<!-- Want to say in particular what the class of models that `inlabru` allows you to fit are -->
<!-- Don't know the answer to this yet, but asking Janine Illian I'm told you don't have to call R-INLA that many times -->
<!-- Other work aiming to extend the scope of INLA-like methods includes the `inlabru` \textsc{R} package [@bachl2019inlabru], INLA within MCMC [@gomez2018markov], and importance sampling with INLA [@berild2022importance], all of which leverage the `R-INLA` \textsc{R} package [@martins2013bayesian]. -->
<!-- The approach of `inlabru` is to approximate non-linear predictors using linearisation, by making iterative calls to `R-INLA`. -->
<!-- INLA within MCMC and importance sampling with INLA are suitable for models which are LGMs conditional on some subset of the parameters being fixed. -->

The remainder of this paper is organised as follows.
In Section \@ref(sec:background) we give background on latent Gaussian and extended latent Gaussian models.
Section \@ref(sec:naomi) outlines the version of the Naomi model that we consider in this paper.
In Section \@ref(sec:inferencenaomi) we review the deterministic inference method for ELGMs used by @stringer2022fast based on nested application of AGHQ and the Laplace approximation, before introducing the PCA-based modification we use to enable application to Naomi.
In Section \@ref(sec:results) we evaluate the accuracy of a PCA-AGHQ grid for the simplified Naomi model fit to data from Malawi, as compared with EB and gold-standard MCMC.
Finally, we discuss our conclusions, and directions for future research in Section \@ref(sec:conclusions).

# Background {#sec:background}

## Latent Gaussian model

Latent Gaussian models [LGMs; @rue2009approximate] are three-stage hierarchical models with
\begin{align}
y_i &\sim p(y_i \, | \, \eta_i, \btheta_1), \quad i \in [n]\\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}),
\end{align}
where $[n] = \{1, \ldots, n\}$.
The response variable is $\y = (y)_{i \in [n]}$ with likelihood $p(\y \, | \, \bmeta, \btheta_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \btheta_1)$, where $\bmeta = (\eta)_{i \in [n]}$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\btheta_1 \in \mathbb{R}^{s_1}$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, linear effects $\beta_j$ of the covariates $z_{ji}$, and unknown functions $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian priors.
It is convenient to collect these parameters into a vector $\x \in \mathbb{R}^N$ called the latent field such that $\x \sim \mathcal{N}(0, \bm{Q}(\btheta_2)^{-})$ where $\btheta_2 \in \mathbb{R}^{s_2}$ are further parameters, again with $s_2$ assumed small.
Let $\btheta = (\btheta_1, \btheta_2) \in \mathbb{R}^s$ with $m = s_1 + s_2$ be all hyperparameters, with prior $p(\btheta)$.

## Extended latent Gaussian model

Extended latent Gaussian models [ELGMs; @stringer2022fast] relax the restriction that there is a one-to-one mapping between the mean response $\bmu$ and structured additive predictor $\bmeta$.
Instead, the structured additive predictor is redefined as $\bmeta = (\eta)_{i \in [N_n]}$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\bmeta$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$.
The inverse link function $g$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\bmeta_{\mathcal{J}_i})$.
Importantly, this mapping allows for the presence of non-linearity in the model.
Put together, ELGMs are then of the form
\begin{align}
y_i &\sim p(y_i \, | \, \bmeta_{\mathcal{J}_i}, \btheta_1), \quad i \in [n] \\
\mu_i &= \mathbb{E}(y_i \, | \, \bmeta_{\mathcal{J}_i}) = g_i(\bmeta_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j \in [N_n],
\end{align}
with latent field and hyperparameter priors as in the LGM case.

## Template Model Builder

Template Model Builder [`TMB`; @kristensen2016tmb] is an R package which implements the Laplace approximation.
In `TMB`, derivatives are obtained using automatic differentiation [AD; @baydin2017automatic].
The approach of AD is to decompose any function into a sequence of elementary operations with known derivatives.
The known derivatives of the elementary operations may then be composed by repeat use of the chain rule to obtain the function's derivative.
A review of AD and how it can be efficiently implemented is provided by @margossian2019review.
`TMB` uses the C++ package `CppAD` [@cppaddocumentation] for AD [Section 3; @kristensen2016tmb].
The development of `TMB` was strongly inspired by the Automatic Differentiation Model Builder [ADMB; @fournier2012ad; @bolker2013strategies] project.
An algorithm is used in `TMB` to automatically determine matrix sparsity structure [Section 4.2; @kristensen2016tmb].
The R package `Matrix` and C++ package `Eigen` are then used for sparse and dense matrix calculations.
@kristensen2016tmb highlight the modular design philosophy of `TMB`.
A review of the use of `TMB` for spatial modelling, including comparison to `R-INLA`, is provided by @osgood2023statistical.

Models are specified in `TMB` using a C++ template file which evaluates $\log p(\mathbf{y}, \mathbf{x}, \boldsymbol{\mathbf{\theta}})$.
Other software packages have been developed which also use `TMB` C++ templates.
The `tmbstan` R package [@monnahan2018no] allows running the Hamiltonian Monte Carlo (HMC) algorithm via `Stan`.
The `aghq` R package [@stringer2021implementing] allows use of AGHQ, and AGHQ over the marginal Laplace approximation, via the `mvQuad` R package [@weiser2016mvquad].
The `glmmTMB` R package [@brooks2017glmmTMB] allows specification of common GLMM models via a formula interface.
It is also possible to extract the `TMB` objective function used by `glmmTMB`, which may then be passed into `aghq` or `tmbstan`.
This discussion highlights the utility of `TMB` templates, emphasising that our approach, built on `TMB`, is both general and broadly applicable.

# Simplified Naomi model {#sec:naomi}

(ref:naomi-results) District-level HIV prevalence (A), ART coverage (B), and new HIV cases and HIV incidence (C) for adults 15-49 in Malawi. Inference here was conducted using and Gaussian approximation and EB via `TMB`.

```{r naomi-results, fig.cap="(ref:naomi-results)"}
knitr::include_graphics("figB.png")
```

@eaton2021naomi introduce a joint ELGM linking three small-area estimation models.
We consider a simplified version defined only at the time of the most recent household survey with HIV testing.
While this version omits nowcasting and temporal projection, as these time points involve limited inference we expect conclusions for the simplified model to be applicable to the complete model.
An overview of the simplified model is given below, and a full specification is provided in Appendix 1.

## Notation and overview

Consider a country in sub-Saharan Africa where a household survey with complex survey design has taken place.
Let $x \in \mathcal{X}$ index district, $a \in \mathcal{A}$ index five-year age group, and $s \in \mathcal{S}$ index sex.
For ease of notation, let $i$ index the finest district-age-sex division included in the model.
Let $I \subseteq \mathcal{X} \times \mathcal{A} \times \mathcal{S}$ be a set of indices $i$ for which an aggregate observation is reported, and $\mathcal{I}$ be the set of all $I$ such that $I \in \mathcal{I}$.

Let $N_i \in \mathbb{N}$ be the known, fixed population size, $\rho_i \in [0, 1]$ be the HIV prevalence, $\alpha_i \in [0, 1]$ be the antiretroviral therapy (ART) coverage, $\kappa_i \in [0, 1]$ be the proportion who are recently infected among HIV positive persons, and $\lambda_i > 0$ be the annual HIV incidence rate.

Some observations are made at an aggregate level over a collection of strata $i$ rather than for a single $i$.
Let $I \subseteq \mathcal{X} \times \mathcal{A} \times \mathcal{S}$ be a set of indices $i$ for which an aggregate observation is reported.
The set of all $I$ is denoted $\mathcal{I}$ such that $I \in \mathcal{I}$.

Naomi is a joint model on the observations $\mathbf{y} = (y^\theta_I)$ for $\theta \in \{\rho, \alpha, \kappa, \rho^\text{ANC}, \alpha^\text{ANC}, N^\text{ART}\}$ and $I \in \mathcal{I}$.
A superscript is used here to refer to components of the model (and does not refer to a power, unless made clear).
The structured additive predictors contain intercept effects, age random effects, and spatial random effects which we collectively describe as the latent field $\x$.
The latent field is controlled by hyperparamters $\btheta$ which include standard deviations, first-order autoregressive model correlation parameters, and reparameterised Besag-York-Mollie model [BYM2; @simpson2017penalising] proportion parameters.
The number of hyperparameters is a key feature making inference challenging.

## Household survey component {#sec:household}

Independent logistic regression models are specified for HIV prevalence and ART coverage in the general population such that $\text{logit}(\rho_i) = \eta^\rho_i$ and $\text{logit}(\alpha_i) = \eta^\alpha_i$.
HIV incidence rate is modelled by $\log(\lambda_i) = \eta^\lambda_i$ and depends on adult HIV prevalence and adult ART coverage.
The structured additive predictors $\eta^\theta_i$ for $\theta \in \{\rho, \alpha, \lambda\}$ are given in Appendix 1.
The proportion recently infected $\kappa_i$ is linked to HIV incidence via
\begin{equation}
\kappa_i = 1- \exp \left( - \lambda_i \cdot \frac{1 - \rho_i}{\rho_i} \cdot (\Omega_T - \beta_T) - \beta_T \right), \label{eq:kappa}
\end{equation}
where the mean duration of recent infection $\Omega_T$ and the proportion of long-term HIV infections misclassified as recent $\beta_T$ are strongly informed by priors for the particular survey.

These processes are each informed by household survey data.
Weighted aggregate survey observations are calculated based on individual responses $\theta_j \in \{0, 1\}$ as
\begin{equation}
\hat \theta_I = \frac{\sum_j w_j \cdot\theta_j}{\sum_j w_j}.
\end{equation}
Design weights $w_j$ for each of $\theta \in \{\rho, \alpha, \kappa\}$ are supplied by the survey provider and aim to reduce bias by decreasing possible correlation between response and recording mechanism [@meng2018statistical].
The index $j$ runs across all individuals in strata $i \in I$ within the relevant denominator i.e. for ART coverage, only those individuals who are HIV positive.
The weighted observed number of outcomes is taken to be $y^{\theta}_{I} = m^{\theta}_{I} \cdot \hat \theta_{I}$ where
\begin{equation}
m^{\theta}_I = \frac{\left(\sum_j w_j\right)^2}{\sum_j w_j^2},
\end{equation}
is the Kish effective sample size [ESS; @kish1965survey].
The weighted aggregated number of outcomes are modelled using a binomial working likelihood [@chen2014use] defined to operate on the reals
\begin{equation}
y^{\theta}_{I} \sim \text{xBin}(m^{\theta}_{I}, \theta_{I})
\end{equation}
The terms $\theta_{I}$ are the following weighted aggregates
\begin{equation}
\rho_{I} = \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}, \quad
\alpha_{I} = \frac{\sum_{i \in I} N_i \rho_i \alpha_i}{\sum_{i \in I} N_i \rho_i}, \quad
\kappa_{I} = \frac{\sum_{i \in I} N_i \rho_i \kappa_i}{\sum_{i \in I} N_i \rho_i}.
\end{equation}

## ANC testing component {#sec:anc}

Women attending ANC clinics are routinely tested for HIV, to help prevent mother-to-child transmission.
HIV prevalence $\rho^\text{ANC}_i$ and ART coverage $\alpha^\text{ANC}_i$ among pregnant women are modelled as offset from the general population indicators as follows
\begin{align}
\text{logit}(\rho^\text{ANC}_i) &= \text{logit}(\rho_i) + \eta^{\rho^\text{ANC}}_i, \\
\text{logit}(\alpha^\text{ANC}_i) &= \text{logit}(\alpha_i) + \eta^{\alpha^\text{ANC}}_i.
\end{align}
These processes are informed by likelihoods specified for aggregate ANC data from the year of the most recent survey.
The number of ANC clients with ascertained status to be fixed as $m^{\rho^\text{ANC}}_I$. 
The number of those with positive status $y^{\rho^\text{ANC}}_I$, and the number of those already on ART prior to their first ANC visit $y^{\alpha^\text{ANC}}_I$ are modelled using nested binomial likelihoods
\begin{align}
y^{\rho^\text{ANC}}_I &\sim \text{Bin}(m^{\rho^\text{ANC}}_I, \rho^\text{ANC}_{I}), \\
y^{\alpha^\text{ANC}}_I &\sim \text{Bin}(y^{\rho^\text{ANC}}_I, \alpha^\text{ANC}_{I}).
\end{align}
As in the household survey component, weighted aggregates
\begin{equation}
\rho^\text{ANC}_{I} = \frac{\sum_{i \in I} \Psi_i \rho_i^\text{ANC}}{\sum_{i \in I} \Psi_i}, \quad
\alpha^\text{ANC}_{I} = \frac{\sum_{i \in I} \Psi_i \rho_i^\text{ANC} \alpha^\text{ANC}_i}{\sum_{i \in I} \Psi_i \rho_i^\text{ANC}},
\end{equation}
are used, with $\Psi_i$ the number of pregnant women assumed here to be fixed.

## ART attendance component {#sec:art}

Data on attendance of ART clinics are routinely collected. These data provide helpful information about HIV prevalence and coverage of ART, but are challenging to use because people living with HIV sometimes choose to access ART services outside of the district that they reside in.
These probabilities of accessing services outside the home district are modelled using multinomial logistic regressions.

Briefly, let $\gamma_{x, x'}$ be the probability that a person on ART residing in district $x$ receives ART in district $x'$, and assume $\gamma_{x, x'} = 0$ unless $x = x'$ or the two districts are neighbouring such that $x \sim x'$.
The log-odds $\tilde \gamma_{x, x'} = \text{logit}(\gamma_{x, x'})$ are modelled using a structured additive predictor $\eta_x^{\tilde \gamma}$ which only depends on the home district $x$.
As such, travel to each neighbouring district, for all age-sex strata, is assumed to be equally likely.
The aggregate ART attendance data $y^{N^\text{ART}}_I$ are modelled using a Gaussian approximation to a sum of binomials.
This sum is over both strata $i \in I$ and the number of ART clients travelling from district $x'$ to $x$.

# Inference methods for Naomi {#sec:inferencenaomi}

Section \@ref(sec:inferenceelgm) gives the inference method for ELGMs of @stringer2022fast based on nested applications of the Laplace approximation and AGHQ.
In Section \@ref(sec:pca) we propose an extension of the method which uses PCA to facilitate inference for Naomi, which otherwise would be intractable.

## Inference for ELGMs {#sec:inferenceelgm}

The joint posterior of the parameters $(\x, \btheta)$ given data $\y$ and hyperparameter prior distribution $p(\btheta)$ in an ELGM is given by
\begin{equation}
  p(\x, \btheta \, | \, \y)
  \propto p(\btheta) |\mathbf{Q}(\btheta)|^{N/2} \exp \left( - \frac{1}{2} \x^\top \mathbf{Q}(\btheta) \x + \sum_{i = 1}^n \log p(y_i \, | \, \x_{\mathcal{J}_i}, \btheta) \right).
\end{equation}
We consider approximations to the posterior marginals of each latent random variable $x_i$ and hyperparameter $\theta_j$ given by
\begin{align}
  p(x_i \, | \, \y) &\approx \tilde p(x_i \, | \, \y) = \int \tilde p(x_i \, | \, \btheta, \y) \tilde p(\btheta \, | \, \y) \text{d}\btheta, \quad i \in [N], \label{eq:inla1} \\
  p(\theta_j \, | \, \y) &\approx \tilde p(\theta_j \, | \, \y) = \int \tilde  p(\btheta \, | \, \y) \text{d}\btheta_{-j} \quad j \in [m], \label{eq:inla2}
\end{align}
where the approximations $\tilde p(x_i \, | \, \btheta, \y)$ and $\tilde  p(\btheta \, | \, \y)$ remain to be defined.

### Laplace approximation\label{sec:la}

Let $\tilde p_\texttt{G}(\x \, | \, \btheta, \y) = \mathcal{N}(\x \, | \, \hat \x(\btheta), \hat{\Hb}(\btheta)^{-1})$ be a Gaussian approximation to $p(\x \, | \, \btheta, \y)$ defined by mode and precision matrix
\begin{align}
\hat \x(\btheta) &= \argmax_\x \log p(\y, \x, \btheta), \label{eq:mode} \\
\hat {\Hb}(\btheta) &= - \frac{\partial^2}{\partial \x \partial \x^\top} \log p(\y, \x, \btheta) \rvert_{\x = \hat \x(\btheta)}. \label{eq:precision}
\end{align}
Then the Laplace approximation [@naylor1982applications] to $p(\btheta, \y)$ is given by
\begin{equation}
\tilde p_\texttt{LA}(\btheta, \y)
= \frac{p(\y, \x, \btheta)}{\tilde p_\texttt{G}(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \hat \x(\btheta)}
= \sqrt{\frac{ \lvert \hat {\Hb}(\btheta) \rvert}{(2 \pi)^{N}}} p(\y, \hat \x(\btheta), \btheta). \label{eq:la}
\end{equation}
Inference proceeds by optimising Equation \@ref(eq:la) using a gradient-based routine to obtain $\hat{\btheta}_\texttt{LA} = \argmax_{\btheta} \tilde p_\texttt{LA}(\btheta, \y)$.
Each evaluation in the optimisation requires an inner optimisation to obtain $\hat \x(\btheta)$ via Equation \@ref(eq:mode).
Supposing the hyperparameters are to be considered fixed, as with the `TMB` approach used currently for Naomi, then latent field joint and marginal inferences then follow directly from the Gaussian approximation $\tilde p_\texttt{G}(\x \, | \, \hat{\btheta}_\texttt{LA}, \y)$.

### Adaptive Gauss-Hermite quadrature

Let $\z \in \mathcal{Q}(m, k)$ be an $m$-dimensional Gauss-Hermite quadrature [GHQ; @davis1975methods] rule with $k$ nodes per dimension constructed using the product rule.
In particular, $\mathcal{Q}(m, k) = \mathcal{Q}(1, k)^m$ where
\begin{equation}
\mathcal{Q}(1, k) = \{z: H_k(z) = 0\} \\
\end{equation}
are the zeroes of the $k$th Hermite polynomial $H_k(z) = (-1)^k \exp(z^2 / 2) \frac{\text{d}}{\text{d}z^k} \exp(-z^2 / 2)$.
The corresponding weighting function $\omega: \mathcal{Q}(m, k) \to \mathbb{R}$ is given by $\omega(\z) = \prod_{j = 1}^m \omega(z_j)$ where $\omega(z) = k! / [H_{k + 1}(z)]^2 \phi(z)$, and $\phi(\cdot)$ is a standard Gaussian density.
GHQ is exact for functions which are a Gaussian density multiplied by a polynomial of total order no more than $2k - 1$.

Let $\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA}) = - \partial^2 \log p_\texttt{LA}(\hat{\btheta}_\texttt{LA}, \y)$ be the curvature at the mode $\hat{\btheta}_\texttt{LA}$ and $[\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA})]^{-1} = \hat{\mathbf{P}}_\texttt{LA} {\hat{\mathbf{P}}_\texttt{LA}}^\top$ be a matrix decomposition of the inverse curvature.
An adaptive Gauss-Hermite quadrature [AHGQ; @naylor1982applications; @tierney1986accurate] estimate of the normalising constant $p(\y)$ based on the Laplace approximation is given by
\begin{equation}
p(\y) \approx \int_{\btheta} \tilde p_\texttt{LA}(\btheta, \y) \approx \tilde p_\texttt{AGHQ}(\y) = |\hat{\mathbf{P}}_\texttt{LA}|\sum_{\z \in \mathcal{Q}(m, k)} \tilde p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y) \omega(\z). \label{eq:aghq}
\end{equation}
When $k = 1$ Equation \@ref(eq:aghq) corresponds to a Laplace approximation.
The unadapted nodes are shifted by the mode and rotated by a matrix decomposition of the inverse curvature such that $\z \mapsto \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}$.
Repositioning the nodes is crucial for statistical quadrature problems like ours, where the integral depends on data $\y$ and regions of high density are not known in advance.
Two alternatives for the matrix decomposition [@jackel2005note] are the Cholesky decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{L}}_\texttt{LA}$, where $\hat{\mathbf{L}}_\texttt{LA}$ is lower triangular, and the spectral decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{E}}_\texttt{LA} \hat{\mathbf{\Lambda}}_\texttt{LA}^{1/2}$, where $\hat{\mathbf{E}}_\texttt{LA} = (\hat{\mathbf{e}}_{\texttt{LA}, 1}, \ldots \hat{\mathbf{e}}_{\texttt{LA}, m})$ contains the eigenvectors of $[\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA})]^{-1}$ and $\hat{\mathbf{\Lambda}}_\texttt{LA}$ is a diagonal matrix containing its eigenvalues $(\hat \lambda_{\texttt{LA}, 1}, \ldots, \hat \lambda_{\texttt{LA}, m})$.
This estimate may be used to normalise the Laplace approximation
\begin{equation}
\tilde p_\texttt{LA}(\btheta \, | \, \y) = \frac{\tilde p_\texttt{LA}(\btheta, \y)}{\tilde p_\texttt{AGHQ}(\y)}.
\end{equation}
To obtain inferences for the latent field (Equation \@ref(eq:inla1)) we reuse the adapted nodes and weights [@rue2009approximate; @stringer2022fast]
\begin{equation}
\tilde p(\x \, | \, \y) = |\hat{\mathbf{P}}_\texttt{LA}| \sum_{\z \in \mathcal{Q}(m, k)} \tilde p_\texttt{G}(\x \, | \, \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y) \tilde p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA} \, | \, \y) \omega(\z). \label{eq:nest}
\end{equation}
Samples from this mixture of Gaussians may be obtained by drawing a node $\z$ with multinomial probabilities $\lambda(\z) = |\hat{\mathbf{P}}_\texttt{LA}| p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA} \, | \, \y) \omega(\z)$, then drawing from the corresponding Gaussian $\tilde p_\texttt{G}(\x \, | \, \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y)$.

## Principal components analysis\label{sec:pca}

(ref:aghq) The Gauss-Hermite quadrature nodes $\z \in \mathcal{Q}(2, 3)$ for a two dimensional integral with three nodes per dimension (A). Adaption occurs based on the mode and covariance matrix of the target via the Cholesky decomposition of the inverse curvature at the mode (B). In PCA-AGHQ (C) only nodes along the first $s$ principal components are kept. Here, 95\% of variation is explained by the first principal component. The integrand is $f(\btheta) = \text{sn}(0.5 \theta_1, \alpha = 2) \cdot \text{sn}(0.8 \theta_1 - 0.5 \theta_2, \alpha = -2)$, where $\text{sn}(\cdot)$ is the standard skewnormal probability density function with shape parameter $\alpha \in \mathbb{R}$.

```{r aghq, fig.cap="(ref:aghq)"}
knitr::include_graphics("figA.png")
```

Use of the product rule grid described above requires $|\mathcal{Q}(m, k)| = k^m$ quadrature points which quickly becomes intractable as $m$ increases for $k > 1$.
We propose to let $\bk = (k_1, \ldots, k_m)$ be a vector of levels for each dimension of $\btheta$.
Then define $\mathcal{Q}(m, \bk) = \mathcal{Q}(1, k_1) \times \cdots \times \mathcal{Q}(1, k_m)$ as a GHQ grid with possible variable levels of size $|\mathcal{Q}(m, \bk)| = \prod_{j = 1}^m k_j$.
Let $\mathcal{Q}(m, s, k)$ correspond to $\mathcal{Q}(m, \bk)$ with choice of levels $k_j = k, j \leq s$ and $k_j = 1, j > s$ for some $s \leq m$.
For example, for $m = 2$ and $s = 1$ then $\bk = (k, 1)$.
In combination with use of the spectral decomposition, this choice of levels is analogous to a principal components analysis (PCA) approach to AGHQ.
We describe this approach as PCA-AGHQ, with corresponding estimate of the normalising constant given by
\begin{equation}
\tilde p_\texttt{PCA}(\y) = |\hat{\mathbf{E}}_{\texttt{LA}} \hat{\mathbf{\Lambda}}_{\texttt{LA}}^{1/2}|\sum_{\z \in \mathcal{Q}(m, s, k)} \tilde p_\texttt{LA}(\hat{\mathbf{E}}_{\texttt{LA}, s} \hat{\mathbf{\Lambda}}_{\texttt{LA}, s}^{1/2} \z + \hat{\btheta}_\texttt{LA}, \y) \omega(\z),
\end{equation}
where $\hat{\mathbf{E}}_{\texttt{LA}, s}$ is an $m \times s$ matrix containing the first $s$ eigenvectors, $\hat{\mathbf{\Lambda}}_{\texttt{LA}, s}$ is the $s \times s$ diagonal matrix containing the first $s$ eigenvalues, and $\omega(\z) = \prod_{j = 1}^s \omega_s(z_j) \times \prod_{j = s + 1}^d \omega_1(z_j)$.
Panel C of Figure \@ref(fig:aghq) illustrates PCA-AGHQ for a case when $m = 2$ and $s = 1$.
As AGHQ with $k = 1$ corresponds to the Laplace approximation, PCA-AGHQ can be interpreted as performing AGHQ on the first $s$ principal components of the inverse curvature, and a Laplace approximation on the remaining $m - s$ principal components.
Inference for the latent field follows analogously to Equation \@ref(eq:nest).

# Application to data from Malawi {#sec:results}

We fit the simplified Naomi model (Section \@ref(sec:naomi)) to data from Malawi.
Malawi, which has $n = 30$ districts, has previously been used to demonstrate the Naomi model, including as a part of the `naomi` R package vignette.
Three Bayesian inference approaches were considered:

1. Gaussian marginals and EB with `TMB`. This approach was previously used in production for Naomi. As short-hand, this approach is referred to as GEB.
2. Gaussian marginals and PCA-AGHQ with `TMB`. This is a novel approach. As short-hand, this approach is referred to as GPCA-AGHQ.
3. The Hamiltonian Monte Carlo algorithm No-U-Turn Sampling (NUTS) with `tmbstan` [@monnahan2018no]. Conditional on assessing chain convergence and suitability, inferences from NUTS represent a gold-standard.

\begin{table}[]
\small
\begin{tabularx}{\textwidth}{p{0.15\linewidth}p{0.15\linewidth}p{0.6\linewidth}}
\toprule
Method & Software & Details \\
\midrule
GEB & \texttt{TMB} & $1000$ samples \\
GPCA-AGHQ & \texttt{aghq} & $k = 3, s = 8$, 1000 samples \\
NUTS & \texttt{tmbstan} & 4 chains of 100000 iterations, with the first half of each chain discarded as warmup, thinned by a factor of 20, to give a total of 10000 samples. Default NUTS tuning parameters \citep{hoffman2014no}. \\
\bottomrule
\end{tabularx}
\caption{A summary of settings used for each inferential method.}
\label{tab:inference-methods}
\small
\end{table}

Our goal was to determine the accuracy of the approximate methods (GEB and GPCA-AGHQ) as compared with the gold-standard (NUTS).
Settings used for each inferential method are provided in Table \@ref(tab:inference-methods), and, where relevant, discussed further below.
The `TMB` C++ user-template used to specify the log-posterior was the same for each approach.
The dimension of the latent field was $N = 467$ and the dimension of the hyperparameters was $m = 24$.
For GEB and GPCA-AGHQ, hyperparameter and latent field samples were simulated following deterministic inference.
For all methods, age-sex-district specific HIV prevalence, ART coverage and HIV incidence were simulated from the latent field and hyperparameter posteriors.

The \textsc{R} [@r] code used to produce all results we describe below is available at `github.com/athowes/naomi-aghq`.
We used `orderly` [@orderly] for reproducible research, `ggplot2` for data visualisation [@wickham2016ggplot2] and `rticles` [@allaire2022rticles] to write this paper via `rmarkdown` [@allaire2022rmarkdown].

## NUTS convergence and suitability

```{r}
naomi_nuts <- readRDS("depends/mcmc-out.rds")
```

The Naomi model was difficult to efficiently sample from using NUTS via `tmbstan`.
The lowest effective sample size (ESS) was `r round(naomi_nuts$ess_min, 0)` (2.5% quantile `r round(naomi_nuts$ess_lower, 0)`, 50% quantile `r round(naomi_nuts$ess_median, 0)`, and 97.5% quantile `r round(naomi_nuts$ess_upper, 0)`; Supplementary Figure 1A).
The largest potential scale reduction factor ($\hat R$) was `r round(naomi_nuts$rhat_max, 3)` (2.5% quantile `r round(naomi_nuts$rhat_lower, 3)`, 50% quantile `r round(naomi_nuts$rhat_median, 3)`, and 97.5% quantile `r round(naomi_nuts$rhat_upper, 3)`; Supplementary Figure 2).
Supplementary Figure 3 shows traceplots for the parameters with the lowest ESS and $\hat R$ respectively.
Though inaccuracies remain possible, these diagnostics are sufficient to treat inferences obtained from NUTS as a gold-standard.

## Use of PCA-AGHQ

```{r}
tv_df <- readr::read_csv("depends/total-variation.csv")
```

(ref:total-variation) Under PCA, the proportion of total variation explained is given by the sum of the first $s$ eigenvalues over the sum of all eigenvalues. A typical rule-of-thumb is to include dimensions sufficient to explain 90% of total variation. In this case, for computational reasons `r round(100 * tv_df$tv[8])`\% was considered sufficient.

```{r total-variation, fig.cap="(ref:total-variation)"}
knitr::include_graphics("depends/total-variation.png")
```

(ref:reduced-rank) The full rank original covariance matrix (Panel A) was closely reproduced by its reduced rank ($s = 8$) matrix approximation (Panel B).

```{r reduced-rank, fig.cap="(ref:reduced-rank)"}
knitr::include_graphics("depends/reduced-rank.png")
```

(ref:nodes-samples-comparison) The grey histograms show the 24 hyperparameter marginal distributions obtained with NUTS. The green lines indicate the position of the 6561 PCA-AGHQ nodes projected onto each hyperparameter marginal. For some hyperparameters, the PCA-AGHQ nodes vary over the domain of the posterior marginal distribution, while for others they concentrate at the mode.

```{r nodes-samples-comparison, fig.cap="(ref:nodes-samples-comparison)"}
knitr::include_graphics("depends/nodes-samples-comparison.png")
```

For the PCA-AGHQ quadrature grid, a Scree plot based on the spectral decomposition of $\hat {\mathbf{H}}_\texttt{LA}^{-1}$ was used to select the number of principal components to keep (Figure \@ref(fig:total-variation)).
Keeping $s = 8$ principal components was sufficient to explain `r round(100 * tv_df$tv[8])`% of total variation.
The reduced rank approximation to the inverse curvature with this choice of $s$ was visually similar to the full rank matrix (Figure \@ref(fig:reduced-rank)).

The principal component (PC) loadings (Supplementary Figure 8) provide interpretable information about which directions had the greatest variation.
Several of the first $s = 8$ PC loadings are sums of two hyperparameters, indicating some redundancy in the hyperparameter parameterisation.
This finding is supported by analysis of correlation structure in the NUTS hyperparameter posterior (Appendix 2.1).

Projecting the $3^8 = 6561$ PCA-AGHQ quadrature nodes onto each univariate hyperparameter dimension, there was substantial variation in coverage by hyperparameter (Figure \@ref(fig:nodes-samples-comparison)).
Approximately 12 hyperparameters had well covered marginals: greater than the 8 naively obtained with a dense grid, but nonetheless far fewer than the full 24.
Coverage was higher among hyperparameters on the logistic scale, and lower among hyperparameters on the logarithmic scale (Supplementary Figure 9).
This discrepancy occurred due to logistic hyperparameters naturally having higher posterior marginal standard deviation than logarithmic hyperparameters (Supplementary Figure 10).

## Time taken comparison

```{r naomi-time}
time_taken <- readr::read_csv("depends/time-taken.csv")

time_taken$aghq <- time_taken$aghq * 60 * 60 
time_taken$tmbstan <- time_taken$tmbstan * 60 * 60 * 24

time_taken_df <- as.data.frame(t(time_taken)) %>%
  tibble::rownames_to_column("method") %>%
  mutate(
    mins = V1 / 60,
    hours = mins / 60,
    days = hours / 24,
    method = case_when(
      method == "TMB" ~ "GEB",
      method == "aghq" ~ "GPCA-AGHQ",
      method == "tmbstan" ~ "NUTS"
    ),
    software = case_when(
      method == "NUTS" ~ "tmbstan",
      TRUE ~ "TMB"
    )
  )

naomi_time_tmbstan_hours <- signif(filter(time_taken_df, method == "NUTS")$hours, 2)
naomi_time_aghq_hours <- signif(filter(time_taken_df, method == "GPCA-AGHQ")$hours, 2)
naomi_time_tmb_hours <- signif(filter(time_taken_df, method == "GEB")$hours, 2)
naomi_time_tmb_minutes <- signif(filter(time_taken_df, method == "GEB")$mins, 2)
```

Inference with NUTS took `r naomi_time_tmbstan_hours` hours, while inference with GPCA-AGHQ took `r naomi_time_aghq_hours` hours and GEB just `r naomi_time_tmb_hours` hours (equivalent to `r naomi_time_tmb_minutes` minutes).
Both the NUTS and GPCA-AGHQ algorithms can be run under a range of settings, trading off accuracy and runtime.

## Inference comparison

Posterior inferences from GEB, GPCA-AGHQ and NUTS were compared using point estimates (Section \@ref(sec:point-estimates)), distributional quantities (Section \@ref(sec:distributional-quantities)), and exceedance probabilities (Section \@ref(sec:exceedance)).

### Point estimates {#sec:point-estimates}

```{r}
df_point <- readr::read_csv("depends/mean-sd.csv")

df_point_pct <- df_point %>%
  ungroup() %>%
  group_by(indicator, type) %>%
  summarise(
    rmse_diff = 100 * diff(rmse) / max(rmse),
    mae_diff = 100 * diff(mae) / max(mae)
  )

rmse_aghq_mean <- filter(df_point, method == "GPCA-AGHQ", indicator == "Posterior mean", type == "latent") %>% pull(rmse) %>% signif(2)
rmse_tmb_mean <- filter(df_point, method == "GEB", indicator == "Posterior mean", type == "latent") %>% pull(rmse) %>% signif(2)
rmse_diff_mean <- filter(df_point_pct, indicator == "Posterior mean", type == "latent") %>% pull(rmse_diff) %>% signif(2)

rmse_aghq_sd <- filter(df_point, method == "GPCA-AGHQ", indicator == "Posterior SD", type == "latent") %>% pull(rmse) %>% signif(2)
rmse_tmb_sd <- filter(df_point, method == "GEB", indicator == "Posterior SD", type == "latent") %>% pull(rmse) %>% signif(2)
rmse_diff_sd <- filter(df_point_pct, indicator == "Posterior SD", type == "latent") %>% pull(rmse_diff) %>% signif(0)
```

(ref:mean-sd-alt-latent) The latent field posterior mean and posterior standard deviation point estimates from each inference method as compared with those from NUTS. The root mean square error (RMSE) and mean absolute error (MAE) are displayed in the top left. For the posterior mean and posterior standard deviation, GPCA-AGHQ reduced RMSE and MAE as compared with GEB.

```{r mean-sd-alt-latent, fig.cap="(ref:mean-sd-alt-latent)"}
knitr::include_graphics("depends/mean-sd-alt-latent.png")
```

Latent field mean and standard deviation point estimates obtained from GPCA-AGHQ were more accurate than those from GEB (Figure \@ref(fig:mean-sd-alt-latent)).
The root mean square error (RMSE) between posterior mean estimates from GPCA-AGHQ and NUTS (`r rmse_aghq_mean`) was `r abs(rmse_diff_mean)`% lower than that between GEB and NUTS (`r rmse_tmb_mean`).
For the posterior standard deviation estimates, there was a substantial `r abs(rmse_diff_sd)`% reduction in RMSE: from `r abs(rmse_tmb_sd)` (GEB) to `r abs(rmse_aghq_sd)` (GPCA-AGHQ).
However, improvements in latent field estimate accuracy transferred to model outputs to a more limited and mixed extent (Supplementary Figures 12 and 13).

### Distributional quantities {#sec:distributional-quantities}

#### Kolmogorov-Smirnov

```{r}
ks_summary <- readRDS("depends/ks-summary.rds")

ks_summary_summarise <- ks_summary %>%
  ungroup() %>%
  filter(type == "Latent") %>%
  summarise(
    tmb = sum(size * TMB) / sum(size),
    aghq = sum(size * aghq) / sum(size)
  )

ks_pct <- (ks_summary_summarise$tmb - ks_summary_summarise$aghq) / ks_summary_summarise$tmb
```

(ref:ks-summary) The average Kolmogorov-Smirnov (KS) test statistic for each latent field parameter of the Naomi model. Vectors of parameters were grouped together. For points above the dashed line at zero, performance of GEB was better. For points below the dashed line, performance of GPCA-AGHQ was better. Most notably, for the latent field parameters $\texttt{ui\_lambda\_x}$ the average test statistic for GEB was substantially higher than for GPCA-AGHQ. This length 32 parameter $\mathbf{u}_x^{\lambda}$ plays a key role in the ART attendance component of the Naomi (Section \@ref(sec:art)).

```{r ks-summary, fig.cap="(ref:ks-summary)"}
knitr::include_graphics("depends/ks-summary.png")
```

```{r}
ui_lambda_x <- readRDS("depends/ui-lambda-x.rds")
```

(ref:ui-lambda-x) The parameter ``r paste0(ui_lambda_x$par)`` had the greatest difference in KS test statistics between GEB and GPCA-AGHQ to NUTS. For this parameter, the potential scale reduction factor was `r ui_lambda_x$rhat` and effective sample size was `r ui_lambda_x$ess`, indicating that NUTS results are likely to be accurate.

```{r ui-lambda-x, fig.cap="(ref:ui-lambda-x)"}
knitr::include_graphics("depends/ui-lambda-x.png")
```

The two-sample Kolmogorov-Smirnov (KS) test statistic [@smirnov1948table] is the maximum absolute difference between two ECDFs $F(\omega) = \frac{1}{n} \sum_{i = 1}^n \mathbb{I}_{\phi_i \leq \omega}$.
It is a relatively stringent worst-case measure of distance between empirical distributions.
The average KS test statistic for GPCA-AGHQ (`r signif(ks_summary_summarise$aghq, 2)`) was `r signif(100 * ks_pct, 2)`% less than the average KS test statistic for GEB (`r signif(ks_summary_summarise$tmb, 2)`).
Figure \@ref(fig:ks-summary) shows the differences in average KS test statistic for each latent field parameter.
GCPA-AGHQ showed the greatest average improvement in KS test statistic for a vector of parameters in the ART attendance component of Naomi (Figure \@ref(fig:ui-lambda-x)).
For both GEB and GPCA-AGHQ the KS test statistic for a parameter was correlated with low NUTS ESS (Supplementary Figure 14).
This correlation may be due to by difficulties estimating particular parameters for all inference methods, or high KS values caused by NUTS inaccuracies.

#### Maximum mean discrepancy

```{r}
mmd <- readRDS("depends/mmd.rds")

first_order_pct <- signif(100 * (mmd$mmd_tmb@mmdstats[1] - mmd$mmd_aghq@mmdstats[1]) / mmd$mmd_tmb@mmdstats[1], 1)
third_order_pct <- signif(100 * (mmd$mmd_tmb@mmdstats[2] - mmd$mmd_aghq@mmdstats[2]) / mmd$mmd_tmb@mmdstats[2], 1)
```

The maximum mean discrepancy [MMD; @gretton2006kernel] is a measure of distance between joint distributions and can be estimated empirically using samples.
Let $\Phi^{1} = \{\boldsymbol{\mathbf{\phi}}^1_s\}_{s = 1}^S$ and $\Phi^2 = \{\boldsymbol{\mathbf{\phi}}^2_s\}_{s = 1}^S$ be two sets of joint posterior samples and $k$ be a kernel, then
\begin{equation}
\text{MMD}(\Phi^1, \Phi^2) = \sqrt{\frac{1}{S^2} \sum_{s, l = 1}^S k(\boldsymbol{\mathbf{\phi}}^1_s, \boldsymbol{\mathbf{\phi}}^1_l) - \frac{2}{S^2} \sum_{s, l = 1}^S k(\boldsymbol{\mathbf{\phi}}_s^1, \boldsymbol{\mathbf{\phi}}_l^2) + \frac{1}{S^2} \sum_{s, l = 1}^S k(\boldsymbol{\mathbf{\phi}}^2_i, \boldsymbol{\mathbf{\phi}}^2_l)}.
\end{equation}
The kernel was set to $k(\boldsymbol{\mathbf{\phi}}^1, \boldsymbol{\mathbf{\phi}}^2) = \exp(-\sigma \lVert \boldsymbol{\mathbf{\phi}}^1 - \boldsymbol{\mathbf{\phi}}^2 \rVert^2)$ with $\sigma$ estimated from data using the `kernlab` \textsc{R} package [@karatzoglou2019package].
Then, the first and third order MMD statistics for GEB were `r signif(mmd$mmd_tmb@mmdstats[1], 2)` and `r signif(mmd$mmd_tmb@mmdstats[2], 2)` while those of GPCA-AGHQ (`r signif(mmd$mmd_aghq@mmdstats[1], 2)` and `r signif(mmd$mmd_aghq@mmdstats[2], 2)`) were `r first_order_pct`% and `r third_order_pct`% lower.

### Exceedance probabilities {#sec:exceedance}

As realistic use cases for Naomi model outputs, we considered the following two case-studies based on exceedance probabilities.

#### Meeting the second 90

(ref:exceedance-second90) The probability each strata has met the second 90 (ART coverage above 81\%) calculated using each inference method, as compared with NUTS. The root mean square error (RMSE) and mean absolute error (MAE) are displayed in the top left.

```{r exceedance-second90, fig.cap="(ref:exceedance-second90)"}
knitr::include_graphics("depends/exceedance-second90.png")
```

Ambitious targets for scaling up ART treatment have been developed by UNAIDS, with the goal of ending the AIDS epidemic by 2030 [@unaids2014target].
Meeting the 90-90-90 fast-track target requires that 90% of people living with HIV know their status, 90% of those are on ART, and 90% of those have a suppressed viral load.
Inferences from Naomi can be used to identify treatment gaps by calculating the probability that the second 90 target has been met, that is $\mathbb{P}(\alpha_i > 0.9^2 = 0.81)$ for each strata $i$.
Strata probabilities of having met the second 90 target were more accurately estimated by GPCA-AGHQ than GEB (Figure \@ref(fig:exceedance-second90)).
However, both GPCA-AGHQ and GEB had substantial error as compared to results from NUTS, particularly for women and girls.
The discrepancy in accuracy by sex may be due to a more challenging posterior geometry caused by interactions between the household survey and ANC components of the model.

#### Finding strata with high incidence

(ref:exceedance-1inc) The probability each strata has high HIV incidence (above 1\% per year) calculated using each inference method, as compared with NUTS. The root mean square error (RMSE) and mean absolute error (MAE) are displayed in the top left.

```{r exceedance-1inc, fig.cap="(ref:exceedance-1inc)"}
knitr::include_graphics("depends/exceedance-1inc.png")
```

Some HIV interventions are cost-effective only within high HIV incidence settings, typically defined as higher than 1% incidence per year.
Inferences from Naomi can be used to calculate the probability of a strata having high incidence by evaluating $\mathbb{P}(\lambda_i > 0.01)$.
GPCA-AGHQ gave more accurate estimates of the probability that a strata has high HIV incidence than GEB (Figure \@ref(fig:exceedance-1inc)).
Again, both methods had significant error as compared with NUTS.

# Conclusion {#sec:conclusions}

In the Malawi case-study, GPCA-AGHQ more accurately inferred latent field posterior marginal distributions than GEB.
Benefits were particularly large for latent field posterior marginal standard deviations (Figure \@ref(fig:mean-sd-alt-latent)).
However, the benefit of using GPCA-AGHQ for model outputs, obtained by a link function applied to a linear transformation of latent field parameters, was more limited.
Although posterior exceedance probabilities based on model outputs from GPCA-AGHQ were more accurate (Figures \@ref(fig:exceedance-second90) and \@ref(fig:exceedance-1inc)), both GEB and GPCA-AGHQ showed systematic inaccuracies as compared with NUTS.

Inaccuracies in model outputs from GEB and GPCA-AGHQ do have potential to meaningfully mislead policy.
As such, where it is possible to do so, gold-standard NUTS results should be computed.
Although NUTS is too slow to run during a workshop, as the UNAIDS HIV estimates process occurs annually, it would be viable to compute more accurate estimates afterwards.
That said, Malawi is one of the countries with the fewest number of districts.
As NUTS took over two days to reach convergence in Malawi, for larger countries, with hundreds of districts, it may be impossible to run NUTS to convergence, and faster approximate methods such as GEB and GPCA-AGHQ may be necessary.

To empower users, GPCA-AGHQ and NUTS could be added to the Naomi web interface as alternatives to GEB.
Analysts would be able to quickly iterate over model options using GEB, before switching to a more accurate approach once they are happy with the results.
One benefit of PCA-AGHQ is that it can be adjusted to suit the computational budget available by choice of the number of dimensions kept in the PCA $s$ and the number of points per dimension $k$.
The scree plot is a well established heuristic for choosing $s$, while heuristics for choosing $k$ are less well established.
Whether it is preferable for a given computational budget to increase $s$ or increase $k$ is an open question.
Further strategies, such as gradually lowering $k$ over the principal components, could also be considered.

## Suggestions for future work

### Further comparisons {#sec:further-comparisons}

Comparison to further Bayesian inference methods could prove beneficial.
Four possibilities stand out as being particularly valuable.
First, there exist other quadrature rules for moderate dimension, such as the Box-Wilson central composite design [@box1992experimental].
It would be of interest to compare INLA with a PCA-AGHQ rule to INLA with other such quadrature rules.
Second, rather than use quadrature to integrate the marginal Laplace approximation, an alternative approach is to run HMC [@monnahan2018no; @margossian2020hamiltonian].
This method can be run by using `tmbstan` and setting `laplace = TRUE`.
When run to convergence, inferential error of this method would solely be due to the Laplace approximation, helping to clarify the extent to which the inferential error of INLA is attributable to the quadrature grid.
Third, NUTS is not especially well suited to sampling from Gaussian latent field models like Naomi.
Other MCMC algorithms, such as blocked Gibbs sampling [@geman1984stochastic] or slice sampling [@neal2003slice], could be considered.
It may be difficult to implement such algorithms using `TMB`.
Many MCMC algorithms are implemented and customisable (including, for example, the choice of block structure) within the `NIMBLE` probabilistic programming language [@de2017programming], however requiring rewriting the Naomi model log-posterior outside of `TMB` would be a downside.
Finally, it would be of substantial interest to implement the Naomi model using the iterative INLA method via `inlabru`.
However, as `inlabru`, like `R-INLA`, is based on a formula interface, it may not be possible to do so directly.

### Better quadrature grids

PCA-AGHQ is a sensible approach to allocating more computational effort to dimensions which contribute more to the integral in question.
Its application to Naomi surfaced several ideas for improvement and challenges to be overcome.
First, the amount of variation explained in the Hessian matrix may not be of direct interest.
For the Naomi model, interest is in the effect of including each dimension on the relevant model outputs.
As such, using alternative measures of importance from sensitivity analysis, such as Shapley values [@shapley1953value] or Sobol indices, could be preferable.
Second, use of PCA is challenging when the dimensions have different scales.
For the Naomi model, logit-scale hyperparameters were systematically favoured over those on the log-scale.
Third, when the quadrature rule is used within an INLA algorithm, it is more important to allocate quadrature nodes to those hyperparameter marginals which are non-Gaussian.
This is because the Laplace approximation is exact when the integrand is Gaussian, so a single quadrature node is sufficient.
The difficulty is knowing in advance which marginals will be non-Gaussian.
This could be done if there were a cheap way to obtain posterior means, which could then be compared to posterior modes obtained using optimisation.
Another approach would be to measure the fit of marginal samples from a cheap approximation, like EB.
The measures of fit would have to be for marginals, ruling out approaches like PSIS [@yao2018yes] which operate on joint distributions.
Finally, it is likely possible to achieve better performance by pruning and prerotation of the quadrature grid, as discussed by @jackel2005note.

### Statistical theory

The class of functions which are integrated exactly by PCA-AGHQ remains to be shown.
Theorem 1 of @stringer2022fast bounds the total variation error of AGHQ, establishing convergence in probability of coverage probabilities under the approximate posterior distribution to those under the true posterior distribution.
Similar theory could be established for PCA-AGHQ, or more generally AGHQ with varying levels.
The challenge of connecting this theory to nested use of any quadrature rule, like GPCA-AGHQ, remains an important open question.

### Testing quadrature assumptions

It may be possible to test the assumptions underlying use of AGHQ grids, enabling assessment of their suitability for specific integrals.
In particular, AGHQ assumes that the integrand is well approximated by a polynomial multiplied by a Gaussian density.
This assumption could be tested by fitting a model using a polynomial times Gaussian kernel, given NUTS hyperparameter samples (or even better, hyperparameter samples obtained from the Laplace NUTS approach mentioned in Section \@ref(sec:further-comparisons)).
This approach could be generalised to also test the suitability of PCA-AGHQ grids.

# Acknowledgements {-}

AH was supported by the EPSRC Centre for Doctoral Training in Modern Statistics and Statistical Machine Learning (EP/S023151/1).
AH conducted part of this research while an International Visiting Graduate Student at the University of Waterloo.
AH and JWE were supported by the Bill and Melinda Gates Foundation (OPP1190661, OPP1164897).
AS was supported by NSERC Discovery Grant (RGPIN-2023-03331).
SRF was supported by the EPSRC (EP/V002910/2).
JWE was supported by UNAIDS and National Institute of Allergy and Infectious Disease of the National Institutes of Health (R01AI136664).
This research was supported by the MRC Centre for Global Infectious Disease Analysis (MR/R015600/1), jointly funded by the UK Medical Research Council (MRC) and the UK Foreign, Commonwealth \& Development Office (FCDO), under the MRC/FCDO Concordat program and is also part of the EDCTP2 programme supported by the European Union.

# References {-}
