---
title: "Deterministic Bayesian inference methods for the Naomi model"
author: "Adam Howes"
date: "April 2023"
output:
  beamer_presentation:
    latex_engine: pdflatex
    highlight: haddock
    fig_width: 7
    fig_height: 3
    includes:
      in_header: preamble.tex
subtitle: HIV Inference Lab Group Meeting
bibliography: citations.bib
institute: Imperial College London
---

# Bayesian modelling and inference

* As a statistical modeller, the bulk of our job is in constructing a generative model for data $y$ using parameters $\vartheta$
* This is the joint distribution $p(y, \vartheta) = p(y \, | \, \vartheta) p(\vartheta)$
* Then, we want to compute (ideally without having to think much about it) the posterior $p(\vartheta \, | \, y)$ which is \textcolor{hilit}{just}^[I've highlighted this with sarcasm in mind: it's a difficult problem]
$$
p(\vartheta \, | \, y) = \frac{p(y, \vartheta)}{p(y)} = \frac{p(y \, | \, \vartheta) p(\vartheta)}{p(y)}
$$
* The central problem of Bayesian inference is doing the following integral
$$
p(y) = \int p(y, \vartheta) \text{d} \vartheta
$$

# Numerical integration

* If you want to integrate something deterministically, you could use numerical integration, otherwise called \textcolor{hilit}{quadrature}
* Choose nodes $\vartheta \in \mathcal{Q} \subset \varTheta$ and weights $\omega: \varTheta \to \mathbb{R}$ and compute the weighted sum
$$
\tilde p(y) = \sum_{\vartheta \in \mathcal{Q}} p(y, \vartheta) \omega(\vartheta)
$$
* By "deterministic" I mean: if you follow the same procedure you will get the same answer

# Naive quadrature example

* Remember how integration is taught? (Rienmann sums)
* Try computing $\int_0^\pi \sin(x) \text{d}x = 2$ using trapezoid rule
* Nodes evenly spaced through $[0, \pi]$

```{r}
trapezoid_rule <- function(x, spacing) {
  w <- rep(spacing, length(x)) # Weights given by space between nodes
  w[1] <- w[1] / 2 # Apart from the first which is halved
  w[length(x)] <- w[length(x)] / 2 # And the last, also halved
  sum(w * x) # Compute the weighted sum
}
```

#

```{r message=FALSE, echo=FALSE,  fig.cap="With 10 nodes it's 0.02 off."}
library(tidyverse)
cbpalette <- c("#56B4E9","#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

finegrid <- seq(0, pi, length.out = 1000)

plot <- data.frame(x = finegrid, y = sin(finegrid)) %>%
  ggplot(aes(x = x, y = y)) +
   geom_line(col = cbpalette[1]) +
   theme_minimal() +
   labs(x = "x", y = "sin(x)")

trapezoid_plot <- function(N) {
  grid <- seq(0, pi, length.out = N)
  int <- trapezoid_rule(x = sin(grid), spacing = grid[2] - grid[1])

  plot + 
    geom_bar(
      data = data.frame(x = grid, y = sin(grid)),
      aes(x = x, y = y), alpha = 0.7, stat = "identity",
      inherit.aes = FALSE, fill = cbpalette[2]) +
    theme_minimal() +
    labs(
      subtitle = paste0("Number of nodes: ", N, "\nTrapezoid rule estimate: ", round(int, 3), "\nTruth: 2"),
      x = "x", y = "sin(x)"
    )
}

trapezoid_plot(N = 10)
```

#

```{r message=FALSE, echo=FALSE, fig.cap="With 30 nodes it's 0.002 off."}
trapezoid_plot(N = 30)
```

#

```{r message=FALSE, echo=FALSE, fig.cap="With 100 nodes it's pretty much correct."}
trapezoid_plot(N = 100)
```

# Monte Carlo as it relates to numerical integration

* Suppose we can sample $\vartheta_i \sim p(y, \vartheta)$ for $i = 1, \ldots, N$
* The Monte Carlo (MC) estimate is
$$
\tilde p(y) = \frac{1}{N} \sum_i p(y, \vartheta_i)
$$
which is quadrature with the samples as nodes and $\omega(\vartheta_i) = 1/N$ for all $i$
* For complicated models^[Or not even that complicated] it's not possible to sample directly from $p(y, \vartheta)$, but we can usually sample from a Markov chain (MCMC)

# Monte Carlo is fundamentally unsound

* "Monte Carlo ignores information" according to @o1987monte
* Suppose $N = 3$ and we sample $\vartheta_1, \vartheta_2, \vartheta_3$ with $\vartheta_2 = \vartheta_3$ then our MC estimate is
$$
\tilde p(y) = \frac{1}{3} \left( p(y, \vartheta_1) + p(y, \vartheta_2) + p(y, \vartheta_3) \right)
$$
* This is despite the fact that nothing new about the function has been learned by adding $\{\vartheta_3, p(y, \vartheta_3)\}$

# Application to HIV survey sampling

* This is a digression but...
* Say we're running a household survey, and sample the same individual twice
* We didn't learn anything new about HIV by surveying them again!
* This doesn't just bite for nodes or individuals which are exactly the same: an analogous argument can be made if they are close together and we expect their function evaluations to be similar

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
$\implies$ Bayesian quadrature, Bayesian survey design \\
For some half-baked thoughts, see \texttt{athowes.github.io/fourth-gen/paper.pdf}
\end{tcolorbox}
\end{center}

# Curse of dimensionality

* Quadrature doesn't work very well when $\text{dim}(\vartheta)$ gets even moderately sized, because there is an exponential increase in the volume you need to cover
* If you're using $k$ points per dimension it's $k^{\text{dim}(\vartheta)}$ e.g. $k = 5$ then
  
```{r}
5^{1:8}
```

# Latent variables and hyperparameters

* Previously all of the parameters were under the symbol $\vartheta$ -- what if we split them up as being $\vartheta = (x, \theta)$
* The key part about this is that $\dim(x) = N$ is big and $\dim(\theta) = m$ is small

| Names for $x$ | Names for $\theta$ |
|---------------|--------------------|
| Latent variables, random effects, latent field | Hyperparameters, fixed effects 

# Spatio-temporal statistics

* There is nothing inherently special about spatio-temporal statistics, but we do often end up tackling problems with similar structures
* We have observations indexed by space $s \in \mathcal{S}$ and time $t \in \mathcal{T}$
* Usually we associate parameters to spatio-temporal locations, giving us something like $\{x_{s, t}\}_{s \in \mathcal{S}, t \in \mathcal{T}}$

# What's important about this?

1. There might be \textcolor{hilit}{a lot} of spatio-temporal locations, so $N$ might be pretty big! If you have 100 districts and 10 years, that's already $100 \times 10 = 1000$ parameters
2. Perhaps we're willing to make quite strong assumptions about how things vary over space-time^[Are there any slides about spatial statistics that don't describe Tobler's first law of geography?]: not IID anymore!

<!-- As you get more data, the posterior approaches a Gaussian -->

# Latent Gaussian models

* A \textcolor{hilit}{latent Gaussian model} (LGM) [@rue2009approximate] looks along these lines:
\begin{alignat*}{2}
  &\text{(Observations)}     &   y &\sim p(y \, | \, x, \theta), \\
  &\text{(Latent field)}     &   x &\sim \mathcal{N}(x  \, | \, \mu(\theta), Q(\theta)^{-1}), \\
  &\text{(Hyperparameters)}  &   \qquad \theta &\sim p(\theta).
\end{alignat*}
* Many models are LGMs, especially in spatio-temporal statistics
* $\dim(x) = n$, $\dim(x) = N$, $\dim(\theta) = m$

# Laplace approximation

* Remember that we wanted to compute
$$
p(y) = \int p(y, \vartheta) \text{d} \vartheta
$$

* One trick for doing this is to pretend $p(\vartheta \, | \, y)$ is Gaussian

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
\begin{itemize}
\item Mode $\hat \vartheta = \argmax_\vartheta \log p(y, \vartheta)$
\item Hessian $H(\hat \vartheta) = - \partial_\vartheta^2 \log p(y, \vartheta) \rvert_{\vartheta = \hat \vartheta}$
\item Gaussian approximation $\implies \tilde p_{\texttt{G}}(\vartheta \, | \, y) = \mathcal{N}(\vartheta \, | \, \hat \vartheta, H(\hat \vartheta)^{-1})$
\end{itemize}
\end{tcolorbox}
\end{center}

# Example of computing the Laplace approximation

* Consider the following model for $i = 1, \ldots, n$ with fixed $a$ and $b$
$$
y_i \sim \text{Poisson}(\lambda), \quad
\lambda \sim \text{Gamma}(a, b).
$$
* It's conjugate so we directly know that
$$
\lambda \, | \, y \sim \text{Gamma}(a + \sum_i y_i, b + n)
$$

#

```{r message=FALSE, echo=FALSE,  fig.cap="A Gamma prior with $a = 3$ and $b = 1$."}
a <- 3
b <- 1

prior <- ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dgamma, n = 500, args = list(shape = a, rate = b), col = cbpalette[1]) +
  labs(x = "", y = "") +
  theme_minimal()
  
prior
```

#

```{r message=FALSE, echo=FALSE,  fig.cap="Draw 3 points from $\\text{Poisson}(3)$, then compute the posterior."}
set.seed(2)
y <- rpois(3, lambda = 2)

posterior <- prior +
  geom_point(data = data.frame(x = y, y = 0), aes(x = x, y = y), inherit.aes = FALSE, alpha = 0.7, size = 2) +
  stat_function(data = data.frame(x = c(0, 10)), aes(x), fun = dgamma, n = 500, args = list(shape = a + sum(y), rate = b + length(y)), col = cbpalette[2])

posterior
```

#

```{r message=FALSE}
fn <- function(x) dgamma(x, a + sum(y), b + length(y), log = TRUE)

# Here we are using numerical derivatives rather than automatic
ff <- list(
  fn = fn,
  gr = function(x) numDeriv::grad(fn, x),
  he = function(x) numDeriv::hessian(fn, x)
)

opt_bfgs <- aghq::optimize_theta(
  ff, 1, control = aghq::default_control(method = "BFGS")
)
```

# Laplace approximation

```{r}
laplace <- posterior +
  stat_function(
    data = data.frame(x = c(0, 10)),
    aes(x),
    fun = dnorm,
    n = 500,
    args = list(mean = opt_bfgs$mode, sd = sqrt(1 / opt_bfgs$hessian)),
    col = cbpalette[3]
  )
```

# Laplace approximation

```{r message=FALSE, echo=FALSE,  fig.cap="The Laplace approximation matches the true posterior near the mode but it's not great in the tails."}
laplace
```

# Laplace approximation

* Now
$$
p(y) = \frac{p(\vartheta, y)}{p(\vartheta \, | \, y)} \approx \frac{p(\vartheta, y)}{p_{\texttt{G}}(\vartheta \, | \, y)}
$$
and we can evaluate RHS where we would like, so let's pick the point at which the Gaussian is most accurate, which is $\hat \vartheta$
$$
p_{\texttt{LA}}(y) = \frac{p(\vartheta, y)}{p_{\texttt{G}}(\vartheta \, | \, y)} \rvert_{\vartheta = \hat \vartheta} = (2 \pi)^{\dim(\vartheta) / 2} \det(H(\hat \vartheta))^{- 1 / 2} p(\hat \vartheta, y)
$$

# Marginal Laplace approximation

* Hey wait a second, is it reasonable to just assume $p(\vartheta \, | \, y)$ is Gaussian?
* There is the Bernstein–von Mises theorem, but we can't rely on that in general. However...

1. We just described a class of models (LGMs) where some subset of the parameters ($x$) have a Gaussian prior $\implies$ it's a lot more reasonable to think that they would have a marginal posterior which is close to Gaussian
2. We just talked about how big $x$ is in comparison to $\theta$! $\implies$ most of the work in our integral can be done using a \textcolor{hilit}{marginal Laplace} approximation to get rid of $x$

# Generalist versus specialist methods

Dichotomy in statistical inference methods:

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]

\begin{enumerate}
  \item Generalist: works in all situations
  \item Specialist: "exploits" properties of the problem at hand
\end{enumerate}

\end{tcolorbox}
\end{center}

We are taking approach 2!

# Marginal Laplace approximation

* What does this look like? Instead of assuming $p(\vartheta \, | \, y) = p(x, \theta \, | \, y)$ is Gaussian we assume $p(x \, | \, \theta, y)$ is
$$
\tilde p_{\texttt{G}}(x \, | \, \theta, y) = \mathcal{N}(x \, | \, \hat x, H(\hat x))^{-1}
$$
where $\hat x = \hat x(\theta)$
* Now the marginal Laplace approximation is
$$
p_{\texttt{LA}}(\theta, y) = \frac{p(x, \theta, y)}{\tilde p_{\texttt{G}}(x \, | \, \theta, y)} \rvert_{x = \hat x} = (2 \pi)^{N / 2} \det(H(\hat x))^{- 1 / 2} p(\hat x, \theta, y)
$$

# Integrated nested Laplace approximation

* Now we can compute $p_{\texttt{LA}}(\theta, y)$ but what we really want is still $p(y)$
* But hopefully^[Really: hopefully] $m$ is small enough that we can now tackle this with quadrature
* So pick some nodes $\mathcal{Q}$ and a weighting function $\omega$ and away we go
$$
p(y) \approx \sum_{\theta \in \mathcal{Q}} p_{\texttt{LA}}(\theta, y) \omega(\theta)
$$
* This is the famous integrated nested Laplace approximation (INLA)

# Taking stock

1. Bayesian inference is integration
2. Spatial statistics has parameters $(x, \theta)$
3. Integrate $x$ cheaply using a Gaussian assumption
4. Try a bit harder with $\theta$ performing quadrature

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]

Now let's apply this to a difficult problem in HIV inference!

\end{tcolorbox}
\end{center}

# The Naomi model

:::::::::::::: {.columns}

::: {.column width=.65}

* Naomi is a spatial evidence synthesis model
* Used by countries to produce HIV estimates in a yearly process supported by UNAIDS
* Inference for Naomi is currently conducted using `TMB` by optimising $p_{\texttt{LA}}(\theta, y)$, and has to be pretty quick to allow for interactive review and development of estimates

:::

::: {.column width=.35}

```{r, echo=FALSE, fig.cap="A supermodel", out.width = '65%'}
knitr::include_graphics("naomi_hex.png")
```

:::

::::::::::::::

# 

```{r, echo=FALSE, fig.cap="Example of the user interface from \\texttt{https://naomi.unaids.org/}", out.width = '70%'}
knitr::include_graphics("naomi_user.png")
```

# Template Model Builder refresher

* To use `TMB` [@kristensen2015tmb], you write your objective function $- \log p(y, x, \theta)$ in the C++ syntax
* For example, for the model $\y \sim \mathcal{N}(\mu, 1)$ with $p(\mu) \propto 1$ then the `TMB` user template looks like (next slide)
* `TMB` implements the Laplace approximation! Set `random = "x"`

# 

```{cpp eval=FALSE}
#include <TMB.hpp>

template <class Type>
Type objective_function<Type>::operator()() {
  // Define data e.g.
  DATA_VECTOR(y);
  // Define parameters e.g.
  PARAMETER(mu);
  // Calculate negative log-likelihood e.g.
  nll = Type(0.0);
  nll -= dnorm(y, mu, 1, true).sum()
  return(nll);
}
```

# Why do we use `TMB`

* It runs quickly, and is flexible enough to write the model
* Another answer is that we don't have any better options

| Option | Would it work? |
|--------|--------------------|
| MCMC | No! It is accurate (eventually) but takes too long |
| INLA via `R-INLA`        | No! Though Naomi is a spatial model with a large Gaussian latent field, it isn't technically a LGM, and isn't compatible with `R-INLA` |

# Idea

* Implement an algorithm inspired by INLA which
1. is compatible with Naomi and its intricacies
2. uses `TMB` -- and thereby automatic differentiation -- to perform the Laplace approximation

# Not a new idea!

> My main comment is that several aspects of the computational machineery that is presented by Rue and his colleagues **could benefit from the use of a numerical technique known as automatic differentiation (AD)** ... By the use of AD one could obtain a system that is automatic from a user's perspective... the benefit would be a fast, flexible and easy-to-use system for doing Bayesian analysis in models with Gaussian latent variables 

-- Hans J. Skaug (coauthor of `TMB`), RSS discussion of @rue2009approximate

# Meanwhile in Canada...

* Alex Stringer in Toronto $\to$ Waterloo had independently been thinking along similar lines, and made a lot of progress

1. Implementing an algorithm similar to INLA, using a specific quadrature rule $\mathcal{Q}$ called adaptive Gauss-Hermite quadrature (AGHQ) which arguably should be the default for this problem
2. Defining a class of models called extended latent Gaussian models (ELGMs) which Naomi fits into

* I will explain what both of these acronyms (AGHQ^[Amusingly similar to AGYW: adolescent girls and young women] and ELGM) mean!

# Gauss-Hermite quadrature

* Recall
$$
\int f(z) \text{d} z \approx \sum_{z \in \mathcal{Q}} f(z) \omega(z)
$$
* Replace $f(z)$ by $\phi(z) f(z)$ and say that $f$ is a polynomial and $\phi$ is unknown
* Suppose that $\phi(z) = \exp(-z^2)$

# Adaptation

* The nodes and weights we use should probably depend on the integrand
* Especially when the integrand is also a function of $y$, which we don't know in advance, as in $p(y, \vartheta)$
  * i.e. how could any fixed quadrature rule be appropriate for all possible $y$?
* Let $z \in \mathcal{Q}(m, k)$ then
$$
\theta(z) = \hat \theta + Lz
$$
where $L$ is the lower Cholesky of $H = LL^\top$
* Other matrix decompositions can also be used e.g. spectral $H = E \Lambda E^\top = (E\Lambda^{1/2})(E\Lambda^{1/2})$
  * Arguably this is preferable: symmetric with respect to the principle axis [@jackel2005note]

# 

```{r, echo=FALSE, message=FALSE, fig.cap="Unadapted points in two dimensions with $k = 3$."}
mu <- c(1, 1.5)
cov <- matrix(c(2, 1, 1, 1), ncol = 2)

obj <- function(theta) {
  mvtnorm::dmvnorm(theta, mean = mu, sigma = cov)
}

grid <- expand.grid(
  theta1 = seq(-2, 5, length.out = 700),
  theta2 = seq(-2, 5, length.out = 700)
)

ground_truth <- cbind(grid, pdf = obj(grid))

plot0 <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = cbpalette[1]) +
  coord_fixed(xlim = c(-2, 4.5), ylim = c(-2, 4.5), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal()

gg <- mvQuad::createNIGrid(2, "GHe", 3)

add_points <- function(plot0, gg) {
  plot0 +
    geom_point(
      data = mvQuad::getNodes(gg) %>%
              as.data.frame() %>%
              mutate(weights = mvQuad::getWeights(gg)), 
      aes(x = V1, y = V2, size = weights),
      alpha = 0.8,
      col = cbpalette[2],
      inherit.aes = FALSE
    ) +
    scale_size_continuous(range = c(1, 2))
}

add_points(plot0, gg) +
  labs(size = "Weight", caption = "")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Add the mean."}
gg2 <- gg
mvQuad::rescale(gg2, m = mu, C = diag(c(1, 1)), dec.type = 2)

add_points(plot0, gg2) +
  labs(size = "Weight", caption = "")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="First option: rotate by the lower Cholesky $L$."}
gg3 <- gg
mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 2)

add_points(plot0, gg3) +
  labs(size = "Weight", caption = "")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Second option: rotate using the eigendecomposition $E \\Lambda^{1/2}$."}
gg3 <- gg
mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 1)

add_points(plot0, gg3) +
  labs(size = "Weight", caption = "")
```

# Extended latent Gaussian models

* To explain what an extended latent Gaussian model (ELGM) is, we need a little more detail on LGMs
* In an LGM the conditional mean depends on exactly one structured additive predictor, specifically
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \theta_1), \quad i \in [n]\\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}),
\end{align*}

# Extended latent Gaussian models

* ELGM remove this requirement such that
$$
\mu_i = g(\eta_{\mathcal{J}_i})
$$
where $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$ and $\mathcal{J_i}$ is some set of indices
* This allow for a higher degree of non-linearity in the model

# Why is Naomi an ELGM?

| | Things I thought made Naomi an ELGM | Does it actually? |
|-|---------------|--------------------|
|1| ANC offset from household survey | ? |
|2| Incidence depends on adult prevalence and coverage | ? |
|3| ART coverage and recent infection are products | ? |
|4| ART attendance uses a multinomial | ? |
|5| Aggregation of finer processes | ? |

* I will explain each of these, and whether or not they make Naomi an ELGM, in more detail in slides to follow
  * The notation may not have been introduced properly, but hopefully the gist will still make sense

# ANC offset from household survey

* Linear predictors for ANC indicators contain nested in them the linear predictors for household survey indicators
\begin{align*}
\text{logit}(\rho_{x, a}^{\text{ANC}}) &= \text{logit}(\rho_{x, F, a}) + \beta^{\rho^{\text{ANC}}} + u_x^{\rho^{\text{ANC}}} + \eta_{R_x, a}^{\rho^{\text{ANC}}}, \\
\text{logit}(\alpha_{x, a}^{\text{ANC}}) &= \text{logit}(\alpha_{x, F, a}) + \beta^{\alpha^{\text{ANC}}} + u_x^{\alpha^{\text{ANC}}} + \eta_{R_x, a}^{\alpha^{\text{ANC}}}. 
\end{align*}
* Here $\text{logit}(\rho_{x, F, a})$ and $\text{logit}(\alpha_{x, F, a})$ *are* Gaussian
* `R-INLA` does have the `copy` feature $\eta^\star = A \eta$ but $A$ must be $n \times n$
* Conclusion: \textcolor{hilit}{does} make Naomi an ELGM

# Incidence depends on adult prevalence and coverage

* Linear predictor for incidence contains aggregated prevalence and coverage
$$
\log(\lambda_{x, s, a}) = \beta_0^\lambda + \beta_S^{\lambda, s = \text{M}} + \log(\rho_{x}^{\text{15-49}}) + \log(1 - \omega \cdot \alpha_{x}^{\text{15-49}}) + u_x^\lambda + \eta_{R_x, s, a}^\lambda.
$$
* Here $\log(\rho_{x}^{\text{15-49}})$ and $\log(1 - \omega \cdot \alpha_{x}^{\text{15-49}})$ are not going to be Gaussian
* Conclusion: \textcolor{hilit}{does} make Naomi an ELGM

# ART coverage and recent infection are products

* In the household survey, say, individuals who are taking ART or have been recently infected must be HIV positive
\begin{align*}
y^{\hat \alpha}_{x, s, a} &\sim \text{xBin}(m_{x, s, a}, \rho_{x, s, a} \cdot \alpha_{x, s, a}), \\
y^{\hat \kappa}_{x, s, a} &\sim \text{xBin}(m_{x, s, a}, \rho_{x, s, a} \cdot \kappa_{x, s, a}).
\end{align*}
* $\text{logit}(\rho_{x, s, a})$ and $\text{logit}(\alpha_{x, s, a})$ are Gaussian, but we're taking a product here
* $\kappa_{x, s, a}$ is more complicated: a function of incidence, prevalence, mean duration of recent infection and false recent ratio
* Conclusion: \textcolor{hilit}{does} make Naomi an ELGM

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
Warning! These equations as writen do not appear in Naomi: instead there are aggregated versions, more about this soon.
\end{tcolorbox}
\end{center}

# ART attendance uses the multinomial

# Aggregation of finer processes

* There are many instances of models being placed on aggregate quantities
\begin{align*}
y^{\hat \theta}_{\mathcal{I}} &\sim \text{xBin}(m^{\hat \theta}_{\mathcal{I}}, \theta_{\mathcal{I}}), \\
\rho_{\mathcal{I}} &= \frac{\sum_{i \in \mathcal{I}} N_i \rho_i}{\sum_{i \in \mathcal{I}} N_i}.
\end{align*}
* Here we have $|\mathcal{I}|$ linear predictors being informed by one observation
* Conclusion: \textcolor{hilit}{does} make Naomi an ELGM

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
I've previously worked on this "disaggregation regression" situation with respect to space, see \texttt{athowes/areal-comparison}
\end{tcolorbox}
\end{center}

# The algorithm

#

```{tikz, echo=FALSE, fig.align='center', fig.cap = "Inference for the hyperparameters. Shaped like a snake for no real reason.", fig.ext = 'png', cache=TRUE}
\usetikzlibrary{arrows}
\begin{tikzpicture}[node distance=3.5cm, auto,>=latex', thick, scale = 1]
\node (1) {$p(\theta, x, y)$};
\node (2) [below of=1] {$\tilde p_\texttt{LA}(\theta, y)$};
\node (3) [right of=2] {$\tilde p_\texttt{AQ}(y)$};
\node (4) [above of=3] {$\tilde p_\texttt{AQ}(\theta \, | \, y)$};
\node (5) [right of=4] {$\tilde p_\texttt{AQ}(\theta_j \, | \, y)$};
\node (6) [below of=5] {Output};

\draw[->] (1) to node {Laplace} (2);
\draw[->] (2) to node {AGHQ} (3);
\draw[->] (3) to node {Normalise} (4);
\draw[->] (4) to node {Marginalise} (5);
\draw[->] (5) to node {Spline} (6);
\end{tikzpicture}
```

#

```{tikz, echo=FALSE, fig.align='center', fig.cap = "Inference for the latent field: naive Laplace version.", fig.ext = 'png', cache=TRUE}
\usetikzlibrary{arrows}
\begin{tikzpicture}[node distance=3.5cm, auto,>=latex', thick, scale = 1]
\node (1) {$p(\theta, x_i, x_{-i}, y)$};
\node (2) [below of=1] {$\tilde p_\texttt{LA}(x_i, \theta, y)$};
\node (3) [right of=2] {$\tilde p_\texttt{AQ}(x_i, y)$};
\node (4) [above of=3] {$\tilde p_\texttt{AQ}(x_i \, | \, y)$};
\node (5) [right of=4] {Output};

\draw[->] (1) to node {Laplace} (2);
\draw[->] (2) to node {AGHQ} (3);
\draw[->] (3) to node {Normalise} (4);
\draw[->] (4) to node {Spline} (5);
\end{tikzpicture}
```

# Comparing posterior distributions

* Let $\{\theta_i\}_{i = 1}^n$ be posterior marginal samples from some quantity with empirical cumulative distribution (ECDF) function
$$
F(\vartheta) = \frac{1}{n} \sum_{i = 1}^n \mathbb{I}_{\theta_i \leq \vartheta}
$$
* Could be a hyperparameter, latent field parameter, model outputs

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
Our primary interest is in the model outputs: do any of the inference methods do a better job at computing the posterior for quantities countries use?
\end{tcolorbox}
\end{center}

<!-- # Moments -->

<!-- * Are the means and standard deviations the same? More generally the $t$th moment -->
<!-- $$ -->
<!-- \hat \mathbb{E}(\theta^t) = \frac{1}{n} \sum_{i = 1}^n \theta_i^t -->
<!-- $$ -->
<!-- * Won't do a great job if you're only looking at the first few moments $t = 1, 2$ -->

<!-- # Kolmogorov-Smirov test -->

<!-- * Compare $F_\bullet(\vartheta)$ to $F_\texttt{NUTS}(\vartheta)$ -->
<!-- $$ -->
<!-- D_\bullet = \sup_\vartheta | F_\texttt{NUTS}(\vartheta) - F_\bullet(\vartheta)|. -->
<!-- $$ -->
<!-- * A value $D_\bullet$ means there is at most a $100 \cdot D_\bullet$% difference between posterior densities -->

<!-- # -->

<!-- ```{r, echo=FALSE, message=FALSE, fig.cap="KS test example."} -->
<!-- plot(1:10) -->
<!-- ``` -->

# Maximum mean discrepancy

# Pareto smoothed importance sampling

# Thanks for listening!

* Working on a paper "Fast approximate Bayesian inference for small-area estimation of HIV indicators using the Naomi model" based on this work, joint with Alex Stringer (Waterloo) and my PhD supervisors Seth Flaxman (Oxford) and Jeff Eaton (Imperial)
* Let me know if you'd be up for being an early reader!
* Code for this project is at `athowes.github.io/elgm-inf`

# References {.allowframebreaks}
