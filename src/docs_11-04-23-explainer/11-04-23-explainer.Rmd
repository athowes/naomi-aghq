---
title: Deterministic Bayesian inference methods for the Naomi model
subtitle: HIV Inference Lab Group Meeting
author: Adam Howes
institute: Imperial College London
date: April 2023
bibliography: citations.bib
output:
  beamer_presentation:
    latex_engine: pdflatex
    highlight: haddock
    fig_width: 7 
    fig_height: 3
    includes:
        in_header: preamble.tex
---

# Bayesian modelling and inference

* As a statistical modeller, the bulk of our job is in constructing a generative model for data $y$ using parameters $\vartheta$
* This is the joint distribution $p(y, \vartheta) = p(y \, | \, \vartheta) p(\vartheta)$
* Then, we want to compute (ideally without having to think much about it) the posterior $p(\vartheta \, | \, y)$ which is **just**^[I've bolded this with sarcasm in mind: it's a difficult problem]
$$
p(\vartheta \, | \, y) = \frac{p(y, \vartheta)}{p(y)} = \frac{p(y \, | \, \vartheta) p(\vartheta)}{p(y)}
$$
* The central problem of Bayesian inference is doing the following integral
$$
p(y) = \int p(y, \vartheta) \text{d} \vartheta
$$

# Numerical integration

* If you want to integrate something deterministically, you could use numerical integration, otherwise called \textcolor{hilit}{quadrature}
* Select nodes $\vartheta \in \mathcal{Q} \subset \varTheta$ and weights $\omega: \varTheta \to \mathbb{R}$ then compute the sum
$$
\tilde p(y) = \sum_{\vartheta \in \mathcal{Q}} p(y, \vartheta) \omega(\vartheta)
$$

# Naive quadrature example

* Remember how integration is taught? (Rienmann sums)
* Try computing $\int_0^\pi \sin(x) \text{d}x = 2$ using trapezoid rule

```{r}
trapezoid_rule <- function(x, spacing) {
  w <- rep(spacing, length(x))
  w[1] <- w[1] / 2
  w[length(x)] <- w[length(x)] / 2
  sum(w * x)
}
```

#

```{r message=FALSE, echo=FALSE}
library(tidyverse)
cbpalette <- c("#56B4E9","#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

finegrid <- seq(0, pi, length.out = 1000)

plot <- data.frame(x = finegrid, y = sin(finegrid)) %>%
  ggplot(aes(x = x, y = y)) +
   geom_line(col = cbpalette[1]) +
   theme_minimal() +
   labs(x = "x", y = "sin(x)")

trapezoid_plot <- function(N) {
  grid <- seq(0, pi, length.out = N)
  int <- trapezoid_rule(x = sin(grid), spacing = grid[2] - grid[1])

  plot + 
    geom_bar(
      data = data.frame(x = grid, y = sin(grid)),
      aes(x = x, y = y), alpha = 0.7, stat = "identity",
      inherit.aes = FALSE, fill = cbpalette[2]) +
    theme_minimal() +
    labs(
      subtitle = paste0("Number of quadrature points: ", N, "\nTrapezoid rule estimate: ", round(int, 3), "\nTruth: 2"),
      x = "x", y = "sin(x)"
    )
}

trapezoid_plot(N = 10)
```

#

```{r message=FALSE, echo=FALSE}
trapezoid_plot(N = 30)
```

#

```{r message=FALSE, echo=FALSE}
trapezoid_plot(N = 100)
```

# Monte Carlo as an example of numerical integration

* Suppose we can sample $\vartheta_i \sim p(y, \vartheta)$ for $i = 1, \ldots, N$
* If we set $\omega(\vartheta_i) = 1/N$ for all $i$ then we get a **Monte Carlo** (MC) estimate
$$
\tilde p(y) = \frac{1}{N} \sum_i p(y, \vartheta_i)
$$
* For complicated models^[Or not even that complicated] it's not possible to sample directly from $p(y, \vartheta)$, but we can usually sample from a Markov chain which if you squint a bit is good enough (MCMC)

# Monte Carlo is fundamentally unsound

* "Monte Carlo ignores information" according to @o1987monte
* Suppose $N = 3$ and we sample $\vartheta_1, \vartheta_2, \vartheta_3$ with $\vartheta_2 = \vartheta_3$ then our MC estimate is
$$
\tilde p(y) = \frac{1}{3} \left( p(y, \vartheta_1) + p(y, \vartheta_2) + p(y, \vartheta_3) \right)
$$
* This is despite the fact that nothing new about the function has been learned by adding $\{\vartheta_3, p(y, \vartheta_3)\}$

# Application to HIV survey sampling

* This is a digression but...
* Say we're running a household survey, and sample the same individual twice
* We didn't learn anything new about HIV by surveying them again!
* This doesn't just bite for nodes or individuals which are exactly the same: an analogous argument can be made if they are close together and we expect their function evaluations to be similar

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
$\implies$ Bayesian quadrature, Bayesian survey design \\
For some half-baked thoughts, see \texttt{athowes.github.io/fourth-gen/paper.pdf}
\end{tcolorbox}
\end{center}

# Latent variables and hyperparameters

* Quadrature doesn't work very well when $\text{dim}(\vartheta)$ gets even moderately sized
  * If you're using $k$ points per dimension it's $k^{\text{dim}(\vartheta)}$
* Previously all of the parameters were under the symbol $\vartheta$ -- what if we split them up as being $\vartheta = (x, \theta)$
* The key part about this is that $\dim(x)$ is big and $\dim(\theta)$ is small

| Names for $x$ | Names for $\theta$ |
|---------------|--------------------|
| Latent variables, random effects, latent field | Hyperparameters, fixed effects 

# Spatio-temporal statistics

* There is nothing inherently special about spatio-temporal statistics, but we do often end up in similar situations because of the structures of the problems we tackle
* We have observations indexed by space $s \in \mathcal{S}$ and time $t \in \mathcal{T}$
* Usually we associate parameters to spatio-temporal locations as well as observations
* This ends up with us having something like $\{x_{s, t}\}$

# What's important about this?

1. There might be **a lot** of spatio-temporal locations, so $\text{dim}(x)$ might be pretty big! If you have 100 districts and 10 years, that's already $100 \times 10 = 1000$ parameters
2. Perhaps we're willing to make quite strong assumptions about how things vary over space-time^[Are there any slides about spatial statistics that don't describe Tobler's first law of geography?]
* Not IID anymore!

<!-- As you get more data, the posterior approaches a Gaussian -->

# Latent Gaussian models

* A \textcolor{hilit}{latent Gaussian model} (LGM) [@rue2009approximate] looks along these lines:
\begin{alignat*}{2}
  &\text{(Observations)}     &   y &\sim p(y \, | \, x, \theta), \\
  &\text{(Latent field)}     &   x &\sim \mathcal{N}(x  \, | \, \mu(\theta), Q(\theta)^{-1}), \\
  &\text{(Hyperparameters)}  &   \qquad \theta &\sim p(\theta).
\end{alignat*}
* Many models are LGMs, especially in spatio-temporal statistics

# Laplace approximation

* Remember that we wanted to compute
$$
p(y) = \int p(y, \vartheta) \text{d} \vartheta
$$

* One trick for doing this is to pretend $p(\vartheta \, | \, y)$ is Gaussian

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
\begin{itemize}
\item Mode $\hat \vartheta = \argmax_\vartheta \log p(y, \vartheta)$
\item Hessian $H(\hat \vartheta) = - \partial_\vartheta^2 \log p(y, \vartheta) \rvert_{\vartheta = \hat \vartheta}$
\item Gaussian approximation $\implies \tilde p_{\texttt{G}}(\vartheta \, | \, y) = \mathcal{N}(\vartheta \, | \, \hat \vartheta, H(\hat \vartheta)^{-1})$
\end{itemize}
\end{tcolorbox}
\end{center}

# Example of computing the Laplace approximation

* Consider the following model for $i = 1, \ldots, n$ with fixed $a$ and $b$
$$
y_i \sim \text{Poisson}(\lambda), \quad
\lambda \sim \text{Gamma}(a, b).
$$
* It's conjugate so we directly know that
$$
\lambda \, | \, y \sim \text{Gamma}(a + \sum_i y_i, b + n)
$$

# Prior

```{r message=FALSE, echo=FALSE}
a <- 3
b <- 1

prior <- ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dgamma, n = 500, args = list(shape = a, rate = b), col = cbpalette[1]) +
  labs(x = "", y = "") +
  theme_minimal()
  
prior
```

# Posterior

```{r message=FALSE, echo=FALSE}
set.seed(2)
y <- rpois(3, lambda = 2)

posterior <- prior +
  geom_point(data = data.frame(x = y, y = 0), aes(x = x, y = y), inherit.aes = FALSE, alpha = 0.7, size = 2) +
  stat_function(data = data.frame(x = c(0, 10)), aes(x), fun = dgamma, n = 500, args = list(shape = a + sum(y), rate = b + length(y)), col = cbpalette[2])

posterior
```

# Laplace approximation

```{r message=FALSE}
fn <- function(x) dgamma(x, a + sum(y), b + length(y), log = TRUE)

# Here we are using numerical derivatives rather than automatic
ff <- list(
  fn = fn,
  gr = function(x) numDeriv::grad(fn, x),
  he = function(x) numDeriv::hessian(fn, x)
)

opt_bfgs <- aghq::optimize_theta(
  ff, 1, control = aghq::default_control(method = "BFGS")
)
```

# Laplace approximation

```{r}
laplace <- posterior +
  stat_function(
    data = data.frame(x = c(0, 10)),
    aes(x),
    fun = dnorm,
    n = 500,
    args = list(mean = opt_bfgs$mode, sd = sqrt(1 / opt_bfgs$hessian)),
    col = cbpalette[3]
  )
```

# Laplace approximation

```{r message=FALSE, echo=FALSE}
laplace
```

# Laplace approximation

* Now
$$
p(y) = \frac{p(\vartheta, y)}{p(\vartheta \, | \, y)} \approx \frac{p(\vartheta, y)}{p_{\texttt{G}}(\vartheta \, | \, y)}
$$
and we can evaluate RHS where we would like, so let's pick the point at which the Gaussian is most accurate, which is $\hat \vartheta$
$$
p_{\texttt{LA}}(y) = \frac{p(\vartheta, y)}{p_{\texttt{G}}(\vartheta \, | \, y)} \rvert_{\vartheta = \hat \vartheta} = (2 \pi)^{\dim(\vartheta) / 2} \det(H(\hat \vartheta))^{- 1 / 2} p(\hat \vartheta, y)
$$

# Marginal Laplace approximation

* Hey wait a second, is it reasonable to just assume $p(\vartheta \, | \, y)$ is Gaussian?
* No, not in general. But...

1. We just described a class of models (LGMs) where some subset of the parameters ($x$) have a Gaussian prior $\implies$ it's a lot more reasonable to think that they would have a marginal posterior which is close to Gaussian
2. We just talked about how big $x$ is in comparison to $\theta$! $\implies$ most of the work in our integral can be done using a **marginal Laplace** approximation to get rid of $x$

# Generalist versus specialist methods

Dichotomy in statistical inference methods:

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]

\begin{enumerate}
  \item \textbf{Generalist} works in all situations
  \item \textbf{Specialist} "exploits" properties of the problem at hand
\end{enumerate}

\end{tcolorbox}
\end{center}

We are taking approach 2!

# Marginal Laplace approximation

* What does this look like? Instead of assuming $p(\vartheta \, | \, y) = p(x, \theta \, | \, y)$ is Gaussian we assume $p(x \, | \, \theta, y)$ is
$$
\tilde p_{\texttt{G}}(x \, | \, \theta, y) = \mathcal{N}(x \, | \, \hat x, H(\hat x))^{-1}
$$
where $\hat x = \hat x(\theta)$
* Now the marginal Laplace approximation is
$$
p_{\texttt{LA}}(\theta, y) = \frac{p(x, \theta, y)}{\tilde p_{\texttt{G}}(x \, | \, \theta, y)} \rvert_{x = \hat x} = (2 \pi)^{\dim(x) / 2} \det(H(\hat x))^{- 1 / 2} p(\hat x, \theta, y)
$$

# Integrated nested Laplace approximation

* Now we can compute $p_{\texttt{LA}}(\theta, y)$ but what we really want is still $p(y)$
* But hopefully^[Really: hopefully] the dimension of $\theta$ is small enough that we can now tackle this with quadrature
* So pick some nodes $\mathcal{Q}$ and a weighting function $\omega$ and away we go
$$
p(y) \approx \sum_{\theta \in \mathcal{Q}} p_{\texttt{LA}}(\theta, y) \omega(\theta)
$$
* This is the famous integrated nested Laplace approximation (INLA)

# Taking stock

1. Bayesian inference is integration
2. Spatial statistics has parameters $(x, \theta)$
3. Integrate $x$ cheaply using a Gaussian assumption
4. Try a bit harder with $\theta$ performing quadrature

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]

Now let's apply this to a difficult problem in HIV inference!

\end{tcolorbox}
\end{center}

# The Naomi model

:::::::::::::: {.columns}

::: {.column width=.65}

* Naomi is a spatial evidence synthesis model
* Used by countries to produce HIV estimates in a yearly process supported by UNAIDS
* Inference for Naomi is currently conducted using `TMB` by optimising $p_{\texttt{LA}}(\theta, y)$, and has to be pretty quick to allow for interactive review and development of estimates

:::

::: {.column width=.35}

```{r, echo=FALSE, fig.cap="A supermodel", out.width = '65%'}
knitr::include_graphics("naomi_hex.png")
```

:::

::::::::::::::

# 

```{r, echo=FALSE, fig.cap="Example of the user interface from \\texttt{https://naomi.unaids.org/}", out.width = '70%'}
knitr::include_graphics("naomi_user.png")
```

# Template Model Builder refresher

* `TMB` [@kristensen2015tmb] is an R package which implements the Laplace approximation for latent variable models
* To use `TMB`, you write your objective function $- \log p(\y \, | \, \x, \btheta) p(\x \, | \, \btheta) p(\btheta)$ in `TMB`'s C++ syntax
* For example, for the model $\y \sim \mathcal{N}(\mu, 1)$ with $p(\mu) \propto 1$ then the `TMB` user template looks as follows

# 

```{cpp eval=FALSE}
#include <TMB.hpp>

template <class Type>
Type objective_function<Type>::operator()() {
  // Define data e.g.
  DATA_VECTOR(y);
  // Define parameters e.g.
  PARAMETER(mu);
  // Calculate negative log-likelihood e.g.
  nll = Type(0.0);
  nll -= dnorm(y, mu, 1, true).sum()
  return(nll);
}
```

# Why do we use `TMB`

* It runs quickly, and is flexible enough to write the model
* Another answer: we don't have any better options
  * Markov chain Monte Carlo (MCMC) is accurate (eventually) but takes too long
  * Though Naomi is a spatial model with a large Gaussian latent field, it isn't technically a latent Gaussian model, and isn't compatible with the `R-INLA` implementation of INLA

# Idea

* Implement an algorithm similar to INLA which
1. is compatible with Naomi
2. uses `TMB` to perform Laplace approximation 
* Does this improves the quality of the inferences we get?

# Not a new idea!

> My main comment is that several aspects of the computational machineery that is presented by Rue and his colleagues **could benefit from the use of a numerical technique known as automatic differentiation (AD)** ... By the use of AD one could obtain a system that is automatic from a user's perspective... the benefit would be a fast, flexible and easy-to-use system for doing Bayesian analysis in models with Gaussian latent variables 

-- Hans J. Skaug (coauthor of `TMB`), RSS discussion of @rue2009approximate

# Meanwhile in Canada...

* Alex Stringer in Toronto $\to$ Waterloo had been thinking along similar lines, and made a lot of progress

1. Implementing an algorithm similar to INLA, using a specific quadrature rule $\mathcal{Q}$ called adaptive Gauss-Hermite quadrature (AGHQ) which he and others argue should be the default for this problem
2. Defining a class of models called extended latent Gaussian models (ELGMs) which Naomi fits into

* I will explain what both of these acronyms (AGHQ^[Amusingly similar to AGYW: adolescent girls and young women] and ELGM) mean!

# Gauss-Hermite quadrature

# Adaptation

* The nodes and weights we use should probably depend on the integrand
  * Especially when the integrand is also a funciton of $y$ as in $p(y, \vartheta)$

# 

```{r, echo=FALSE, message=FALSE, fig.cap="Unadapted points."}
mu <- c(1, 1.5)
cov <- matrix(c(2, 1, 1, 1), ncol = 2)

obj <- function(theta) {
  mvtnorm::dmvnorm(theta, mean = mu, sigma = cov)
}

grid <- expand.grid(
  theta1 = seq(-2, 5, length.out = 700),
  theta2 = seq(-2, 5, length.out = 700)
)

ground_truth <- cbind(grid, pdf = obj(grid))

plot0 <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = cbpalette[1]) +
  coord_fixed(xlim = c(-2, 4.5), ylim = c(-2, 4.5), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal()

gg <- mvQuad::createNIGrid(2, "GHe", 3)

add_points <- function(plot0, gg) {
  plot0 +
    geom_point(
      data = mvQuad::getNodes(gg) %>%
              as.data.frame() %>%
              mutate(weights = mvQuad::getWeights(gg)), 
      aes(x = V1, y = V2, size = weights),
      alpha = 0.8,
      col = cbpalette[2],
      inherit.aes = FALSE
    ) +
    scale_size_continuous(range = c(1, 2))
}

add_points(plot0, gg) +
  labs(size = "Weight", caption = "")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Add the mean."}
gg2 <- gg
mvQuad::rescale(gg2, m = mu, C = diag(c(1, 1)), dec.type = 2)

add_points(plot0, gg2) +
  labs(size = "Weight", caption = "")
```

#

```{r, echo=FALSE, message=FALSE, fig.cap="Rotate by the lower Cholesky."}
gg3 <- gg
mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 2)

add_points(plot0, gg3) +
  labs(size = "Weight", caption = "")
```

# Extended latent Gaussian models

# Why is Naomi an ELGM?

1. ANC offset from household survey
2. Incidence depends on adult prevalence and coverage
3. ART attendance is a product
4. ART attendance uses a multinomial
5. Aggregation of finer processes

# ANC offset from household survey

\begin{align*}
\text{logit}(\rho_{x, a}^{\text{ANC}}) &= \text{logit}(\rho_{x, F, a}) + \beta^{\rho^{\text{ANC}}} + u_x^{\rho^{\text{ANC}}} + \eta_{R_x, a}^{\rho^{\text{ANC}}}, \\
\text{logit}(\alpha_{x, a}^{\text{ANC}}) &= \text{logit}(\alpha_{x, F, a}) + \beta^{\alpha^{\text{ANC}}} + u_x^{\alpha^{\text{ANC}}} + \eta_{R_x, a}^{\alpha^{\text{ANC}}}. 
\end{align*}

# Incidence depends on adult prevalence and coverage

$$
\log(\lambda_{x, s, a}) = \beta_0^\lambda + \beta_S^{\lambda, s = \text{M}} + \log(\rho_{x}^{\text{15-49}}) + \log(1 - \omega \cdot \alpha_{x}^{\text{15-49}}) + u_x^\lambda + \eta_{R_x, s, a}^\lambda.
$$

# ART attendance is a product

$$
\dot A_{x', x, s, a} \sim \text{Bin}(N_{x', s, a}, \pi_{x', x, s, a})
$$
where $\pi_{x', x, s, a} = \rho_{x', s, a} \cdot \alpha_{x', s, a} \cdot \gamma_{x', x, s, a}$.

# ART attendance multinomial

# Aggregation of finer processes

# The algorithm

#

```{tikz, echo=FALSE, fig.align='center', fig.cap = "Inference for the hyperparameters. Shaped like a snake for no real reason.", fig.ext = 'png', cache=TRUE}
\usetikzlibrary{arrows}
\begin{tikzpicture}[node distance=3.5cm, auto,>=latex', thick, scale = 1]
\node (1) {$p(\theta, x, y)$};
\node (2) [below of=1] {$\tilde p_\texttt{LA}(\theta, y)$};
\node (3) [right of=2] {$\tilde p_\texttt{AQ}(y)$};
\node (4) [above of=3] {$\tilde p_\texttt{AQ}(\theta \, | \, y)$};
\node (5) [right of=4] {$\tilde p_\texttt{AQ}(\theta_j \, | \, y)$};
\node (6) [below of=5] {Output};

\draw[->] (1) to node {Laplace} (2);
\draw[->] (2) to node {AGHQ} (3);
\draw[->] (3) to node {Normalise} (4);
\draw[->] (4) to node {Marginalise} (5);
\draw[->] (5) to node {Spline} (6);
\end{tikzpicture}
```

#

```{tikz, echo=FALSE, fig.align='center', fig.cap = "Inference for the latent field.", fig.ext = 'png', cache=TRUE}
\usetikzlibrary{arrows}
\begin{tikzpicture}[node distance=3.5cm, auto,>=latex', thick, scale = 1]
\node (1) {$p(\theta, x, y)$};
\node (2) [below of=1] {$\tilde p_\texttt{LA}(\theta, y)$};
\node (3) [right of=2] {$\tilde p_\texttt{AQ}(y)$};
\node (4) [above of=3] {$\tilde p_\texttt{AQ}(\theta \, | \, y)$};
\node (5) [right of=4] {$\tilde p_\texttt{AQ}(\theta_j \, | \, y)$};
\node (6) [below of=5] {Output};

\draw[->] (1) to node {Laplace} (2);
\draw[->] (2) to node {AGHQ} (3);
\draw[->] (3) to node {Normalise} (4);
\draw[->] (4) to node {Marginalise} (5);
\draw[->] (5) to node {Spline} (6);
\end{tikzpicture}
```

# Thanks for listening!

* Working on a paper "Fast approximate Bayesian inference for small-area estimation of HIV indicators using the Naomi model" based on this work, joint with Alex Stringer (Waterloo) and my PhD supervisors Seth Flaxman (Oxford) and Jeff Eaton (Imperial)
* Let me know if you'd be up for being an early reader!
* Code for this project is at `athowes.github.io/elgm-inf`

# References {.allowframebreaks}
