---
title: "Inference methods comparison for the simplified Naomi model"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background**  We have run the simplified Naomi model using a range of inference methods: `TMB`, `aghq`, `adam` and `tmbstan`.
  
  **Task** In this report, we compare the accuracy of the posterior distributions obtained from these inference methods using histograms, Kolmogorov-Smirnov tests, maximum mean discrepancy and Pareto-smoothed importance sampling.
---

# Background

We compare the inference results from `TMB`, `aghq`, `adam`, and `tmbstan`.
Import these inference results as follows:

```{r}
tmb <- readRDS("depends/tmb.rds")
aghq <- readRDS("depends/aghq.rds")
adam <- readRDS("depends/adam.rds")
tmbstan <- readRDS("depends/tmbstan.rds")

depends <- yaml::read_yaml("orderly.yml")$depends
```

Check that the parameters (latent field, hyperparameters, model outputs) sampled from each of the four methods are the same:

```{r}
stopifnot(names(tmb$fit$sample) == names(aghq$quad$sample))
stopifnot(names(tmb$fit$sample) == names(adam$adam$sample))
stopifnot(names(tmb$fit$sample) == names(tmbstan$mcmc$sample))
```

## Run details {.tabset}

For more information about the conditions under which these results were generated, see:

### `TMB`

```{r}
dependency_details <- function(i) {
  report_name <- names(depends[[i]])
  print(paste0("Inference results obtained from ", report_name, " with the query ", depends[[i]][[report_name]]$id))
  report_id <- orderly::orderly_search(query = depends[[i]][[report_name]]$id, report_name)
  print(paste0("Obtained report had ID ", report_id, " and was run with the following parameters:"))
  print(orderly::orderly_info(report_id, report_name)$parameters)
}

dependency_details(1)
```

### `aghq`

```{r}
dependency_details(2)
```

### `adam`

```{r}
dependency_details(3)
```

### `tmbstan`

```{r}
tmbstan_details <- dependency_details(4)
tmbstan_details
```

## Time taken

```{r}
time_taken <- data.frame(
  "TMB" = tmb$time,
  "aghq" = aghq$time,
  "adam" = adam$time,
  "tmbstan" = tmbstan$time
)

write_csv(time_taken, "time_taken.csv")

time_taken

adam <- adam$adam
```

# Histograms and ECDF difference plots {.tabset .tabset-fade}

We create histograms and empirical cumulative distribution function (ECDF) difference plots of the samples from each method.
All of the possible parameter names are as follows:

```{r}
pars <- names(tmb$fit$sample)
pars
```

We will produce plots about the following subset of them.
There is no particular reason to choose this subset rather than other, it's quite arbitrary.

```{r}
pars_eval <- pars %in% c("beta_rho", "beta_alpha", "beta_lambda", "beta_anc_rho", "beta_anc_alpha", "u_rho_x", "us_rho_x", "u_rho_xs")
names(pars_eval) <- pars
```

We will write the `tmbstan` $\hat R$ and ESS for the particular parameter on each plot to help in deciding how seriously to take the MCMC results:

```{r}
rhats <- bayesplot::rhat(tmbstan$mcmc$stanfit)
ess_ratio <- bayesplot::neff_ratio(tmbstan$mcmc$stanfit)
niter <- 4 * tmbstan_details$niter / tmbstan_details$nthin
ess <- ess_ratio * niter
```

The `beta_rho` and `beta_anc_rho` parameters are used in other reports, so create dataframes here to save as artefacts:

```{r}
beta_rho <- histogram_and_ecdf("beta_rho", i = 1, return_df = TRUE)
saveRDS(beta_rho$df, "beta_rho.rds")

beta_anc_rho <- histogram_and_ecdf("beta_anc_rho", return_df = TRUE)
saveRDS(beta_anc_rho$df, "beta_anc_rho.rds")
```

## `beta_rho`

```{r eval = pars_eval[["beta_rho"]]}
histogram_and_ecdf_helper <- function(par) lapply(1:sum(names(tmb$fit$obj$env$par) == par), histogram_and_ecdf, par = par)
histogram_and_ecdf_helper("beta_rho")
```

<!-- ## `beta_alpha` -->

<!-- ```{r eval = pars_eval[["beta_rho"]]} -->
<!-- histogram_and_ecdf_helper("beta_alpha") -->
<!-- ``` -->

<!-- ## `beta_lambda` -->

<!-- ```{r eval = pars_eval[["beta_rho"]]} -->
<!-- histogram_and_ecdf_helper("beta_lambda") -->
<!-- ``` -->

<!-- ## `beta_anc_rho` -->

<!-- ```{r eval = pars_eval[["beta_rho"]]} -->
<!-- histogram_and_ecdf("beta_anc_rho") -->
<!-- ``` -->

<!-- ## `beta_anc_alpha` -->

<!-- ```{r eval = pars_eval[["beta_rho"]]} -->
<!-- histogram_and_ecdf("beta_anc_alpha") -->
<!-- ``` -->

<!-- ## `logit` -->

<!-- ```{r eval = FALSE} -->
<!-- lapply(pars[stringr::str_starts(pars, "logit")], histogram_and_ecdf) -->
<!-- ``` -->

<!-- ## `log_sigma` -->

<!-- ```{r eval = FALSE} -->
<!-- lapply(pars[stringr::str_starts(pars, "log_sigma")], histogram_and_ecdf) -->
<!-- ``` -->

<!-- ## `u_rho_x` -->

<!-- ```{r eval = pars_eval[["u_rho_x"]]} -->
<!-- histogram_and_ecdf_helper("u_rho_x") -->
<!-- ``` -->

<!-- ## `u_rho_x` -->

<!-- ```{r eval = pars_eval[["u_rho_x"]]} -->
<!-- histogram_and_ecdf_helper("u_rho_x") -->
<!-- ``` -->

<!-- ## `us_rho_x` -->

<!-- ```{r eval = pars_eval[["us_rho_x"]]} -->
<!-- histogram_and_ecdf_helper("us_rho_x") -->
<!-- ``` -->

<!-- ## `u_rho_xs` -->

<!-- ```{r eval = pars_eval[["u_rho_xs"]]} -->
<!-- histogram_and_ecdf_helper("u_rho_xs") -->
<!-- ``` -->

<!-- ## `us_rho_xs` -->

<!-- ```{r eval = pars_eval[["us_rho_xs"]]} -->
<!-- histogram_and_ecdf_helper("us_rho_xs") -->
<!-- ``` -->

<!-- ## `u_rho_as` -->

<!-- ```{r eval = pars_eval[["u_rho_as"]]} -->
<!-- histogram_and_ecdf_helper("u_rho_as") -->
<!-- ``` -->

<!-- ## `u_alpha_x` -->

<!-- ```{r eval = pars_eval[["u_alpha_x"]]} -->
<!-- histogram_and_ecdf_helper("u_alpha_x") -->
<!-- ``` -->

<!-- ## `us_alpha_x` -->

<!-- ```{r eval = pars_eval[["us_alpha_x"]]} -->
<!-- histogram_and_ecdf_helper("us_alpha_x") -->
<!-- ``` -->

<!-- ## `u_alpha_xs` -->

<!-- ```{r eval = pars_eval[["u_alpha_xs"]]} -->
<!-- histogram_and_ecdf_helper("u_alpha_xs") -->
<!-- ``` -->

<!-- ## `us_alpha_xs` -->

<!-- ```{r eval = pars_eval[["beta_rho"]]} -->
<!-- histogram_and_ecdf_helper("us_alpha_xs") -->
<!-- ``` -->

<!-- ## `u_alpha_a` -->

<!-- ```{r eval = pars_eval[["u_alpha_a"]]} -->
<!-- histogram_and_ecdf_helper("u_alpha_a") -->
<!-- ``` -->

<!-- ## `u_alpha_as` -->

<!-- ```{r eval = pars_eval[["u_alpha_as"]]} -->
<!-- histogram_and_ecdf_helper("u_alpha_as") -->
<!-- ``` -->

<!-- ## `u_alpha_xa` -->

<!-- ```{r eval = pars_eval[["u_alpha_xa"]]} -->
<!-- histogram_and_ecdf_helper("u_alpha_xa") -->
<!-- ``` -->

<!-- ## `ui_lambda_x` -->

<!-- ```{r eval = pars_eval[["ui_lambda_x"]]} -->
<!-- histogram_and_ecdf_helper("ui_lambda_x") -->
<!-- ``` -->

<!-- ## `ui_anc_rho_x` -->

<!-- ```{r eval = pars_eval[["ui_anc_rho_x"]]} -->
<!-- histogram_and_ecdf_helper("ui_anc_rho_x") -->
<!-- ``` -->

<!-- ## `ui_anc_alpha_x` -->

<!-- ```{r eval = pars_eval[["ui_anc_alpha_x"]]} -->
<!-- histogram_and_ecdf_helper("ui_anc_alpha_x") -->
<!-- ``` -->

<!-- ## `log_or_gamma` -->

<!-- ```{r eval = pars_eval[["log_or_gamma"]]} -->
<!-- histogram_and_ecdf_helper("log_or_gamma") -->
<!-- ``` -->

## Outcome variables

These are all of the outcome variables which we might be interested in to monitor:

```{r}
names(tmb$fit$sample)[!(names(tmb$fit$sample) %in% unique(names(tmb$fit$obj$env$par)))]
```
The most important ones (checked with Jeff) are HIV prevalence (`rho_t1_out`), ART coverage (`alpha_t1_out`), and HIV incidence (`lambda_t1_out`).

* HIV prevalence has `r dim(tmb$fit$sample$rho_t1_out)[1]` variables (rows)
* ART coverage has `r dim(tmb$fit$sample$alpha_t1_out)[1]` variables (rows)
* HIV incidence has `r dim(tmb$fit$sample$lambda_t1_out)[1]` variables (rows)

```{r}
dim(tmb$fit$sample$rho_t1_out)
dim(tmb$fit$sample$alpha_t1_out)
dim(tmb$fit$sample$lambda_t1_out)
```

Within `tmb$naomi_data` there is a dataframe called `mf_out` (modelframe output) which contains the area, age, sex mapping these `r dim(tmb$fit$sample$rho_t1_out)[1]` rows.
As a sanity check, confirm that the number of areas times number of ages times number of sexes indeed equals the number of rows:

```{r}
mf_out <- tmb$naomi_data$mf_out
(n_area <- length(unique(mf_out$area_id)))
(n_age <- length(unique(mf_out$age_group)))
(n_sex <- length(unique(mf_out$sex)))

stopifnot(n_area * n_age * n_sex == dim(tmb$fit$sample$rho_t1_out)[1])
```

It makes sense only to assess inferential accuracy at the finest level so as to avoid double counting.

```{r}
mf_out$area_id %>% unique()
mf_out$sex %>% unique()
mf_out$age_group %>% unique()

mf_out_fine <- mf_out %>%
  tibble::rownames_to_column("id") %>%
  mutate(id = as.numeric(id)) %>%
  filter(
    area_id %in% paste0("MWI_4_", 1:32, "_demo"),
    sex %in% c("male", "female"),
    age_group %in%
      c(
        "Y000_004", "Y005_009", "Y010_014", "Y015_019", "Y020_024", "Y025_029",
        "Y025_034", "Y030_034", "Y035_039", "Y040_044", "Y045_049", "Y050_054",
        "Y055_059", "Y060_064", "Y065_069", "Y070_074", "Y075_079", "Y080_999"
      )
  )
```

An `id` column has been added to `mf_out_fine` to enable merging to the samples.

```{r}
head(mf_out_fine)
nrow(mf_out_fine)
```

```{r}
rho_t1_out_fine <- tmb$fit$sample$rho_t1_out[mf_out_fine$id, ]
dim(rho_t1_out_fine)
rho_t1_out_fine[1:10, 1:10]
```

Something weird happening with `adam`, go back and check this:

```{r}
histogram_and_ecdf(par = "alpha_t1_out", i = 1)
```

# KS plots 

## Individual parameters {.tabset .tabset-fade}

```{r}
ks_helper <- function(par, starts_with = FALSE, ...) {
  to_ks_df(par, starts_with = starts_with) %>% ks_plot(par, ...)
}
```

### `beta`

```{r}
ks_helper("beta", starts_with = TRUE)
ks_helper("beta", starts_with = TRUE, method1 = "TMB", method2 = "adam")
```

<!-- ### `logit` -->

<!-- ```{r} -->
<!-- ks_helper("logit", starts_with = TRUE) -->
<!-- ks_helper("logit", starts_with = TRUE, method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `log_sigma` -->

<!-- ```{r} -->
<!-- ks_helper("log_sigma", starts_with = TRUE) -->
<!-- ks_helper("log_sigma", starts_with = TRUE , method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_rho_x` -->

<!-- ```{r eval = pars_eval[["u_rho_x"]]} -->
<!-- ks_helper("u_rho_x") -->
<!-- ks_helper("u_rho_x", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_rho_xs` -->

<!-- ```{r eval = pars_eval[["u_rho_xs"]]} -->
<!-- ks_helper("u_rho_xs") -->
<!-- ks_helper("u_rho_xs", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `us_rho_x` -->

<!-- ```{r eval = pars_eval[["us_rho_x"]]} -->
<!-- ks_helper("us_rho_x") -->
<!-- ks_helper("us_rho_x", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `us_rho_xs` -->

<!-- ```{r eval = pars_eval[["us_rho_xs"]]} -->
<!-- ks_helper("us_rho_xs") -->
<!-- ks_helper("us_rho_xs", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_rho_a` -->

<!-- ```{r eval = pars_eval[["u_rho_a"]]} -->
<!-- ks_helper("u_rho_a") -->
<!-- ks_helper("u_rho_a", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_rho_as` -->

<!-- ```{r eval = pars_eval[["u_rho_as"]]} -->
<!-- ks_helper("u_rho_as") -->
<!-- ks_helper("u_rho_as", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_alpha_x` -->

<!-- ```{r eval = pars_eval[["u_alpha_x"]]} -->
<!-- ks_helper("u_alpha_x") -->
<!-- ks_helper("u_alpha_x", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_alpha_xs` -->

<!-- ```{r eval = pars_eval[["u_alpha_xs"]]} -->
<!-- ks_helper("u_alpha_xs") -->
<!-- ks_helper("u_alpha_xs", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `us_alpha_x` -->

<!-- ```{r eval = pars_eval[["us_alpha_x"]]} -->
<!-- ks_helper("us_alpha_x") -->
<!-- ks_helper("us_alpha_x", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `us_alpha_xs` -->

<!-- ```{r eval = pars_eval[["us_alpha_xs"]]} -->
<!-- ks_helper("us_alpha_xs") -->
<!-- ks_helper("us_alpha_xs", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_alpha_a` -->

<!-- ```{r eval = pars_eval[["u_alpha_a"]]} -->
<!-- ks_helper("u_alpha_a") -->
<!-- ks_helper("u_alpha_a", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_alpha_as` -->

<!-- ```{r eval = pars_eval[["u_alpha_as"]]} -->
<!-- ks_helper("u_alpha_as") -->
<!-- ks_helper("u_alpha_as", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `u_alpha_xa` -->

<!-- ```{r eval = pars_eval[["u_alpha_xa"]]} -->
<!-- ks_helper("u_alpha_xa") -->
<!-- ks_helper("u_alpha_xa", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `ui_anc_rho_x` -->

<!-- ```{r eval = pars_eval[["ui_anc_rho_x"]]} -->
<!-- ks_helper("ui_anc_rho_x") -->
<!-- ks_helper("ui_anc_rho_x", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `ui_anc_alpha_x` -->

<!-- ```{r eval = pars_eval[["ui_anc_alpha_x"]]} -->
<!-- ks_helper("ui_anc_alpha_x") -->
<!-- ks_helper("ui_anc_alpha_x", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

<!-- ### `log_or_gamma` -->

<!-- ```{r eval = pars_eval[["log_or_gamma"]]} -->
<!-- ks_helper("log_or_gamma") -->
<!-- ks_helper("log_or_gamma", method1 = "TMB", method2 = "adam") -->
<!-- ``` -->

### Outcome variables

```{r}
ks_rho_t1_out <- to_ks_df(par = "rho_t1_out", outputs = TRUE, id = mf_out_fine$id)

merge_to_mf_out_fine <- function(ks_df) {
  ks_df %>%
  select(-par) %>%
  left_join(
    mf_out_fine %>%
      rename("full_id" = "id") %>%
      tibble::rowid_to_column("index") %>%
      mutate(index = as.numeric(index))
  ) %>%
  select(-index)
}
  
ks_rho_t1_out <- merge_to_mf_out_fine(ks_rho_t1_out)

ks_alpha_t1_out <- to_ks_df(par = "alpha_t1_out", outputs = TRUE, id = mf_out_fine$id) %>%
  merge_to_mf_out_fine()

ks_lambda_t1_out <- to_ks_df(par = "lambda_t1_out", outputs = TRUE, id = mf_out_fine$id) %>%
  merge_to_mf_out_fine()

ks_plot(ks_df = ks_rho_t1_out, par = "rho_t1_out")
ks_plot(ks_df = ks_alpha_t1_out, par = "alpha_t1_out")
ks_plot(ks_df = ks_lambda_t1_out, par = "lambda_t1_out")
```

## Summary table

```{r}
options(dplyr.summarise.inform = FALSE)

ks_summary <- lapply(unique(names(tmb$fit$obj$env$par)), function(x) {
  to_ks_df(x) %>%
    group_by(method, index) %>%
    summarise(ks = mean(ks), par = x) %>%
    ungroup()
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = "method", values_from = "ks") %>%
  rename(
    "Parameter" = "par",
    "KS(adam, tmbstan)" = "adam",
    "KS(aghq, tmbstan)" = "aghq",
    "KS(TMB, tmbstan)" = "TMB",
  )

r <- adam$quad$obj$env$random
x_names <- names(adam$quad$obj$env$par[r])
theta_names <- names(adam$quad$obj$env$par[-r])

dict <- data.frame(
  Parameter = c(unique(x_names), unique(theta_names)),
  Type = c(rep("Latent field", length(unique(x_names))), rep("Hyper", length(unique(theta_names))))
)

ks_summary <- ks_summary %>%
  left_join(dict, by = "Parameter")
```

Values in green are the lowest KS difference for that particular parameter:

```{r}
ks_summary %>%
  gt::gt() %>%
  gt::fmt_number(
    columns = starts_with("KS"),
    decimals = 3
  ) %>%
  gt::tab_style(
    style = cell_fill(color = "#009E73"),
    locations = cells_body(
      columns = `KS(adam, tmbstan)`,
      rows = `KS(adam, tmbstan)` < `KS(aghq, tmbstan)` & `KS(adam, tmbstan)` < `KS(TMB, tmbstan)`
    )
  ) %>%
    gt::tab_style(
    style = cell_fill(color = "#009E73"),
    locations = cells_body(
      columns = `KS(aghq, tmbstan)`,
      rows = `KS(aghq, tmbstan)` < `KS(adam, tmbstan)` & `KS(aghq, tmbstan)` < `KS(TMB, tmbstan)`
    )
  ) %>%
    gt::tab_style(
    style = cell_fill(color = "#009E73"),
    locations = cells_body(
      columns = `KS(TMB, tmbstan)`,
      rows = `KS(TMB, tmbstan)` < `KS(aghq, tmbstan)` & `KS(TMB, tmbstan)` < `KS(adam, tmbstan)`
    )
  )
```

This plot could be improved by making the red to green transition smooth, then adding legend for KS difference.
The density plots could be filled according to KS difference.

```{r}
summary_ks_plot <- function(ks_summary, method1, method2) {
  ks_method1 <- paste0("KS(", method1, ", tmbstan)")
  ks_method2 <- paste0("KS(", method2, ", tmbstan)")
  
  xy_length <- min(1, max(ks_summary[[ks_method1]], ks_summary[[ks_method2]]) + 0.05)
  
  x <- y <- seq(0, xy_length, length.out = 20)

  gradient_base <- expand.grid(x, y) %>%
    mutate(diff = Var1 - Var2) %>%
    ggplot(aes(x = Var1, y = Var2)) +
    geom_tile(aes(fill = diff), alpha = 0.7) +
    scale_fill_gradientn(
      colours = c("#56B4E9", "white", "#009E73"),
      rescaler = ~ scales::rescale_mid(.x, mid = 0)
    )
  
  densityplot <- gradient_base +
    stat_density_2d(data = ks_summary, aes(x = .data[[ks_method1]], y = .data[[ks_method2]], linetype = Type), col = "black") +
    xlim(0, xy_length) +
    ylim(0, xy_length) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", alpha = 0.5) +
    scale_linetype_manual(values = c("solid", "dashed")) +
    labs(x = ks_method1, y = ks_method2) +
    theme_minimal() +
    guides(fill = "none") +
    theme(
      legend.position = "bottom"
    )
  
  ks_summary[["KS difference"]] <- ks_summary[[ks_method1]] - ks_summary[[ks_method2]]
  
  ridgeplot <- ggplot(ks_summary, aes(y = Type, x = `KS difference`)) +
    ggridges::geom_density_ridges(alpha = 0.7, fill = NA, aes(linetype = Type)) +
    coord_flip() +
    scale_linetype_manual(values = c("solid", "dashed")) +
    labs(y = "", x = paste0(ks_method1, " - ", ks_method2)) +
    guides(linetype = FALSE) +
    theme_minimal()

  densityplot + ridgeplot
}

summary_ks_plot(ks_summary, "TMB", "aghq")
summary_ks_plot(ks_summary, "TMB", "adam")
summary_ks_plot(ks_summary, "aghq", "adam")

(ks_summary_out <- ks_summary %>%
  group_by(Type) %>%
  summarise(
    TMB = mean(`KS(TMB, tmbstan)`),
    aghq = mean(`KS(aghq, tmbstan)`),
    adam = mean(`KS(adam, tmbstan)`)
  ))
```
Compute $p$-values, then plot expecting $\mathcal{U}(0, 1)$.
If not conclude there is a significant difference.

```{r}
#' To write!
```

Save some things for output:

```{r}
pdf("ks_summary_tmb_adam.pdf", h = 4, w = 6.25)
summary_ks_plot(ks_summary, "TMB", "adam")
dev.off()

saveRDS(ks_summary_out, "ks_summary_out.rds")
```

## Investigation into large KS values

Want to create rank order lists of largest KS differences between methods:

```{r}
ks_comparison <- function(ks_summary, method1, method2, n) {
  ks_method1 <- paste0("KS(", method1, ", tmbstan)")
  ks_method2 <- paste0("KS(", method2, ", tmbstan)")
  ks_summary[["KS difference"]] <- ks_summary[[ks_method1]] - ks_summary[[ks_method2]]
  ks_summary %>%
    filter(Type != "Hyper") %>%
    arrange(desc(`KS difference`)) %>%
    head(n = n)
}
```

Nodes where `TMB` beats `adam`:

```{r}
ks_comparison(ks_summary, method1 = "adam", method2 = "TMB", 10)
```

```{r}
histogram_and_ecdf(par = "u_alpha_xs", i = 18)
histogram_and_ecdf(par = "u_alpha_xs", i = 29)
histogram_and_ecdf(par = "ui_anc_alpha_x", i = 29)
```

Nodes where `adam` beats `TMB`:

```{r}
ks_comparison(ks_summary, method1 = "TMB", method2 = "adam", 10)  
```

```{r}
histogram_and_ecdf(par = "log_or_gamma", i = 18)
histogram_and_ecdf(par = "ui_anc_rho_x", i = 10)
histogram_and_ecdf(par = "log_or_gamma", i = 1)
```

## Correlation between KS values and ESS

Is there any correlation between the value of $\text{KS}(\texttt{method}, \texttt{tmbstan})$ for a particular parameter and the ESS of that parameter from `tmbstan` output?

```{r}
ks_summary %>%
  pivot_longer(
    col = starts_with("KS"),
    names_to = "Method",
    values_to = "KS"
  ) %>%
  mutate(
    Method = fct_recode(Method,
      "adam" = "KS(adam, tmbstan)",
      "aghq" = "KS(aghq, tmbstan)",
      "TMB" = "KS(TMB, tmbstan)"
    )
  ) %>%
  group_by(Parameter) %>%
  mutate(
    par_num = case_when(
      max(index) > 1 ~ paste0(Parameter, "[", index, "]"),
      TRUE ~ Parameter
    )
  ) %>%
  ungroup() %>%
  left_join(data.frame(ess) %>%
    tibble::rownames_to_column("par_num")
  ) %>%
  ggplot(aes(x = ess, y = KS)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "#56B4E9") +
    facet_grid(~ Method) +
    theme_minimal() +
    labs(x = "ESS", y = "KS(method, tmbstan)")
```

# MMD

```{r}
#' To write!
```

# PSIS

Suppose we have two sets of samples from the posterior.
For each sample we are going to want to evaluate the log-likelihood, so that we can calculate the log-likelihood ratios.
We can extract the `TMB` objective function for the log-likelihood as follows:

```{r}
tmb$fit$obj$fn()
#' To write!
```
