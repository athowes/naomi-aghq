---
title: Fast approximate Bayesian inference for small-area estimation of HIV indicators using the Naomi model
author:
  - name: Adam 
    surname: Howes
    email: ath19@ic.ac.uk
    label: e1
    addressLabel: A,E
    sepNext: ","
  - name: Alex
    surname: Stringer
    email: alex.stringer@uwaterloo.ca
    label: e2
    addressLabel: B
  - name: Seth R.
    surname: Flaxman
    email: seth.flaxman@cs.ox.ac.uk
    label: e3
    addressLabel: C
    sepNext: ","
  - name: Jeffrey W.
    surname: Eaton
    email: jeffrey.eaton@imperial.ac.uk
    label: e4
    addressLabel: D,E
affiliation:
  - label: A
    name: Department of Mathematics, Imperial College London
    authorsLabels: e1
  - label: B
    name: Department of Statistics and Actuarial Science, University of Waterloo
    authorsLabels: e2
  - label: C
    name: Department of Computer Science, University of Oxford
    authorsLabels: e3
  - label: E
    name: MRC Centre for Global Infectious Disease Analysis, School of Public Health, Imperial College London
    authorsLabels: e4
  - label: D
    name: Center for Communicable Disease Dynamics, Harvard T.H. Chan School of Public Health, Harvard University
abstract: |
  | Naomi is a spatial evidence synthesis model used to produce district-level HIV epidemic indicators in sub-Saharan Africa. Multiple outcomes of policy interest, including HIV prevalence, HIV incidence, and antiretroviral therapy treatment coverage are jointly modelled using both household survey data and routinely reported health system data. The model is provided as a tool for countries to input their data to and generate estimates, during a yearly process supported by UNAIDS. Currently, inference is conducted using empirical Bayes and a Gaussian approximation via the \texttt{TMB} \textsc{R} package. We propose a new inference method which extends adaptive Gauss-Hermite quadrature to deal with >20 hyperparameters, enabling fast and accurate inference for Naomi and other extended latent Gaussian models. Using data from Malawi, our method provides more accurate inferences than \texttt{TMB}, and is substantially faster to run to Hamiltonian Monte Carlo with the No-U-Turn sampler. By extending the \texttt{aghq} \textsc{R} package we facilitate easy, flexible use of our method when provided a \texttt{TMB} \textsf{C++} template for the model's log-posterior.
keyword-subclass: | 
 \begin{keyword}[class=MSC2020] % It must be define for aap, aop, aos journals. For aoas, sts is not used
 \kwd[Primary ]{00X00}
 \kwd{00X00}
 \kwd[; secondary ]{00X00}
 \end{keyword}
keywords: 
  - spatial statistics
  - small-area estimation
  - INLA
  - AGHQ
  - HIV epidemiology

predefined-theoremstyle: true # use in section Environments for Axiom, Theorem, etc
bibliography: citations.bib
biblio-style: imsart-nameyear # alternative: imsart-number
output:
  rticles::ims_article:
    journal: aoas # aap, aoas, aop, aos, sts. See documentation
    toc: false # Please use for articles with 50 pages and more
    includes:
      in_header: preamble.tex
---

<!-- Remaining things to add, and or desiderata: -->

<!-- * [ ] Tying together Naomi model within LGM / ELGM mathematical notation -->
<!-- * [ ] Create DAG representing Naomi model. This would likely go into the appenxix, so use that notation -->
<!-- * [ ] Rewrite section on why Naomi is an ELGM, drawing from slides -->
<!-- * [ ] Write section on Laplace approximation -->
<!-- * [ ] Write section on AGHQ, tying together connection to Laplace for k = 1 -->
<!-- * [ ] Move algorithm to algorithm environment -->

<!-- Seth notes that Naomi is a spatial evidence synthesis model used to produce district- level HIV epidemic indicators in sub-Saharan Africa is not the right topic sentence for the abstract -->
<!-- Better to lead with something about the inference method? -->

```{r echo = FALSE}
colours <- c("#56B4E9", "#009E73", "#E69F00")

options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

<!-- Most published papers will not exceed 20 pages -->
<!-- We strongly encourage submission of data sets, computer algorithms and supporting material -->

# Introduction

To mount an effective public health response to the HIV epidemic, it is crucial to have accurate, timely estimates of HIV indicators at the geographic level at which health systems are planned and delivered.
However, producing these estimates is challenging, in large part due to limitations of the available data sources.
Nationally-representative household surveys provide the most statistically reliable data, but due to their high cost, in most countries they are conducted only every five years or so, with limited sample size at the district level.
Other data sources, such as routine health surveillance of antenatal care (ANC) clinics, are available closer to real-time but based on limited or non-representative samples of the population.
To address these challenges, the Naomi small-area estimation model [@eaton2021naomi] synthesises data from multiple sources to estimate HIV indicators at a district-level.
Modelling multiple data sources jointly has many benefits, including mitigating the limitations of any single source, increasing statistical power, and prompting investigation into any conflicts of information between sources.
Software (\url{https://naomi.unaids.org}) has been developed for Naomi, allowing countries to input their data and interactively generate estimates in a yearly process supported by UNAIDS.
Creation of estimates by country teams, rather than external agencies, is a noteworthy feature of the HIV response.
Drawing on expertise closest to the data being modelled improves the accuracy of the process, as well as strengthening trust and ownership of the resulting estimates.

The complexity of Naomi, in combination with practical requirements, present a difficult Bayesian inference problem.
First, due to dependence of observations on multiple structured additive predictors, Naomi falls into the class of extended latent Gaussian models [ELGMs; @stringer2022fast].
Further, as well as hundreds of latent field parameters, Naomi has >20 hyperparameters: substantially more than the small number typically required for use of integrated nested Laplace approximations [@rue2009approximate].
Second, any inferential strategy must be fast enough for interactive review and iteration of modelling results, as well as easy to run in production across a range of countries.
In this setting, Markov chain Monte Carlo (MCMC) approaches are prohibitively slow due to the scale of the model and challenging features of its posterior geometry [@neal2003slice].

Inference is currently conducted using an empirical Bayes (EB) approach, with a Gaussian approximation to the latent field, via the Template Model Builder (`TMB`) \textsf{R} package [@kristensen2016tmb].
Owing to its speed and flexibility, `TMB` is gaining popularity more broadly in spatial statistics [@osgood2022statistical].
Inference in `TMB` is based on optimisation of a \textsf{C++} template function, with the option available to use a Laplace approximation to integrate out any subset of the parameters.
In the Naomi model, this subset is the high-dimensional latent field, leaving the smaller number of hyperparameters.
`TMB` uses automatic differentiation [@baydin2017automatic] to calculate the derivatives required for numerical optimisation routines and the Laplace approximation, Taking tnspiration from the AD Model Builder package [@fournier2012ad].
Although the approach of `TMB` is fast, within the empirical Bayes framework hyperparameter uncertainty is not properly accounted for in the latent field posterior.
This consideration motivates us to look for an approach closer to full Bayesian inference, which is also flexible enough to be compatible with the model, as well as fast enough to be run in production by country teams.

To obtain fast, accurate Bayesian inferences for the Naomi model we developed an inference method which extends adaptive Gauss-Hermite quadrature to handle many hyperparameters.
We used principal components analysis to create a quadrature grid which is computationally feasible.
Our method is implemented as an extension of the `aghq` \textsf{R} package [@stringer2021implementing].
As `aghq` is designed to naturally interface with `TMB`, use is simple when provided a \textsf{C++} user template for the log-posterior.

The remainder of this paper is organised as follows.
Section \ref{sec:naomi} outlines the version of the Naomi model that we consider in this paper, and  Section \ref{sec:elgm} describes how it falls within the ELGM framework.
Section \ref{sec:fastinferencemethods} outlines our approach to fast, accurate Bayesian inference for ELGMs using simplified INLA and AGHQ.
As a case-study, we compare the accuracy of our inference method to `TMB` and `tmbstan` for the simplified Naomi model fit to data from Malawi, in Section \ref{sec:naomi}.
We also demonstrate a Bayesian workflow, illustrating the applicability of these tools in a deterministic inference setting.
Finally, in Section \ref{sec:conclusions} we discuss our conclusions, how we anticipate our method might be useful for other models, and directions for future research.

# Simplified Naomi model\label{sec:naomi}

@eaton2021naomi specify a joint model linking three small-area estimation models, defined over three time points.
We consider a simplified version defined only at the time of the most recent household survey with HIV testing, omitting nowcasting and temporal projection, as these time points involve limited inferences.
An overview of the simplified model is given below, and a more complete mathematical description is provided in Appendix S1.

## Household survey component \label{sec:household}

Consider a country in sub-Saharan Africa where a household survey with complex survey design has taken place.
Let $x \in \mathcal{X}$ index district, $a \in \mathcal{A}$ index five-year age group, and $s \in \mathcal{S}$ index sex.
For ease of notation, let $i$ index the finest district-age-sex division included in the model.
Let $I \subseteq \mathcal{X} \times \mathcal{A} \times \mathcal{S}$ be a set of indices $i$ for which an aggregate observation is reported, with $I \in \mathcal{I}$.

Let $N_i \in \mathbb{N}$ be the known, fixed population size.
We infer the following unknown HIV indicators using linked regression equations: HIV prevalence $\rho_i \in [0, 1]$, the proportion of individuals who are HIV positive; antiretroviral therapy (ART) coverage $\alpha_i \in [0, 1]$, the proportion of people living with HIV who receive ART treatment; and annual HIV incidence rate $\lambda_i > 0$, the yearly rate of new HIV infections occurring.
Independent logistic regression models for HIV prevalence and ART coverage in the general population are specified such that $\text{logit}(\rho_i) = \eta^\rho_i$ and $\text{logit}(\alpha_i) = \eta^\alpha_i$, for certain choice of structured additive predictors.
HIV incidence rate is modelled on the log scale as $\log(\lambda_i) = \eta^\lambda_i$, and depends on adult HIV prevalence and adult ART coverage. 
Let $\kappa_i$ be the proportion recently infected among HIV positive persons.
We link this proportion to HIV incidence via
\begin{equation}
\kappa_i = 1- \exp \left( - \lambda_i \cdot \frac{1 - \rho_i}{\rho_i} \cdot (\Omega_T - \beta_T) - \beta_T \right), \label{eq:kappa}
\end{equation}
where the mean duration of recent infection $\Omega_T$ and the proportion of long-term HIV infections misclassified as recent $\beta_T$ are strongly informed by priors for the particular survey.

These processes are informed by household survey data.
For $\theta \in \{\rho, \alpha, \kappa\}$ let
\begin{equation*}
\hat \theta_I = \frac{\sum_j w_j \cdot\theta_j}{\sum_j w_j}
\end{equation*}
be weighted, aggregate survey observations, where $j$ indexes individuals across all strata $i \in I$ and $w_j$ are design weights.
The observed number of outcomes are $y^{\hat \theta}_{I} = m^{\hat \theta}_{I} \cdot \hat \theta_{I}$ where
\begin{equation*}
m^{\hat \theta}_I = \frac{\left(\sum_j w_j\right)^2}{\sum_j w_j^2},
\end{equation*}
is the Kish effective sample size [@kish1965survey].
We use a binomial working likelihood
\begin{equation*}
y^{\hat \theta}_{I} \sim \text{xBin}(m^{\hat \theta}_{I}, \theta_{I})
\end{equation*}
to model these aggregate observations, where $\theta_{I}$ are the following weighted aggregates
\begin{equation*}
\rho_{I} = \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}, \quad
\alpha_{I} = \frac{\sum_{i \in I} N_i \rho_i \alpha_i}{\sum_{i \in I} N_i \rho_i}, \quad
\kappa_{I} = \frac{\sum_{i \in I} N_i \rho_i \kappa_i}{\sum_{i \in I} N_i \rho_i}.
\end{equation*}

## ANC testing component \label{sec:anc}

HIV prevalence $\rho^\text{ANC}_i$ and ART coverage $\alpha^\text{ANC}_i$ among pregnant women are modelled as offset from the general population indicators as follows
\begin{align*}
\text{logit}(\rho^\text{ANC}_i) &= \text{logit}(\rho_i) + \eta^{\rho^\text{ANC}}_i, \\
\text{logit}(\alpha^\text{ANC}_i) &= \text{logit}(\alpha_i) + \eta^{\alpha^\text{ANC}}_i.
\end{align*}
These processes are informed by likelihoods specified for aggregate ANC data from the year of the most recent survey.
In particular

1. $x^\text{ANC}_I$, the number of ANC clients with ascertained status,
2. $y^\text{ANC}_I$, the number of those with positive status, and
3. $z^\text{ANC}_I$, the number of those already on ART prior to their first ANC visit.

We use the binomial likelihoods
\begin{align*}
y^\text{ANC}_I &\sim \text{Bin}(x^\text{ANC}_I, \rho^\text{ANC}_{I}), \\
z^\text{ANC}_I &\sim \text{Bin}(y^\text{ANC}_I, \alpha^\text{ANC}_{I}),
\end{align*}
where, again, we use weighted aggregates
\begin{equation*}
\rho^\text{ANC}_{I} = \frac{\sum_{i \in I} \Psi_i \rho_i^\text{ANC}}{\sum_{i \in I} \Psi_i}, \quad
\alpha^\text{ANC}_{I} = \frac{\sum_{i \in I} \Psi_i \rho_i^\text{ANC} \alpha^\text{ANC}_i}{\sum_{i \in I} \Psi_i \rho_i^\text{ANC}},
\end{equation*}
with $\Psi_i$ the number of pregnant women, which we assume to be fixed.

## ART attendance component \label{sec:art}

People living with HIV sometimes choose to access ART services outside of the district that they reside in.
To account for this, we use multinomial logistic regression to model the probabilities of accessing services outside the home district.
Let $\gamma_{x, x'}$ be the probability that a person on ART residing in district $x$ receives ART in district $x'$, and assume $\gamma_{x, x'} = 0$ unless $x = x'$ or the two districts are neighbouring, denoted by $x \sim x'$.
The log-odds $\tilde \gamma_{x, x'} = \text{logit}(\gamma_{x, x'})$ are modelled using a structured additive predictor $\eta_x^{\tilde \gamma}$ which only depends on the home district $x$, such that travel to each neighbouring district, for all age-sex strata, is equally likely.
We then model aggregate ART attendance data $\dot A_I$ using a Gaussian approximation to a sum of binomials.
This sum is both by over $i \in I$ and by number of ART clients travelling from district $x'$ to $x$.
More details regarding this part of the model are provided in Appendix S1.

## Collected together

Let $\mathbf{y} = (y^{\hat \theta}_I)$ for $\theta \in \{\rho, \alpha, \kappa\}$ and $I \in \mathcal{I}$ be the vector of observations.
Attempt to write fully the Naomi model in a small number of collected together equations here.
The ANC testing component and ART attendance component might be hard to do this for.

# Extended Latent Gaussian models\label{sec:elgm}

We now describe the popular latent Gaussian class of models, and an extension which encapsulates the complexities of Naomi.

## Definitions

Latent Gaussian models [LGMs; @rue2009approximate] are three-stage hierarchical models of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \btheta_1), \quad i \in [n]\\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}),
\end{align*}
where $[n] = \{1, \ldots, n\}$.
The response variable is $\y = (y)_{i \in [n]}$ with likelihood $p(\y \, | \, \bmeta, \btheta_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \btheta_1)$, where $\bmeta = (\eta)_{i \in [n]}$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\btheta_1 \in \mathbb{R}^s$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, linear effects $\beta_j$ of the covariates $z_{ji}$, and unknown functions $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian priors.
It is convenient to collect these parameters into a vector $\x \in \mathbb{R}^N$ called the latent field such that $\x \sim \mathcal{N}(0, \bm{Q}(\btheta_2)^{-1})$ where $\btheta_2 \in \mathbb{R}^{s_2}$ are further parameters, again with $s_2$ assumed small.
Let $\btheta = (\btheta_1, \btheta_2) \in \mathbb{R}^s$ with $m = s_1 + s_2$ be all hyperparameters, with prior $p(\btheta)$.

Extended latent Gaussian models [ELGMs; @stringer2022fast] relax the restriction that there is a one-to-one mapping between the mean response $\bmu$ and structured additive predictor $\bmeta$.
Instead, the structured additive predictor is redefined as $\bmeta = (\eta)_{i \in [N_n]}$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\bmeta$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$.
The inverse link function $g(\cdot)$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\bmeta_{\mathcal{J}_i})$.
Importantly, this mapping allows for the presence of non-linearity in the model.
Put together, ELGMs are then of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \bmeta_{\mathcal{J}_i}, \btheta_1), \quad i \in [n] \\
\mu_i &= \mathbb{E}(y_i \, | \, \bmeta_{\mathcal{J}_i}) = g_i(\bmeta_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j \in [N_n].
\end{align*}

## Naomi framed as an ELGM

Naomi has a lot in common with many LGMS: it is a spatio-temporal model with a large latent field, governed by a smaller number of hyperparameters.
However, Naomi is not an LGM, and instead falls into the ELGM class, for the following reasons:

1. In the household survey component, HIV incidence depends on district-level adult HIV prevalence and ART coverage, such that $\lambda \propto \rho (1 - \omega \cdot \alpha)$, where $\omega = 0.7$ is a fixed constant. This reflects basic HIV epidemiology: HIV incidence is proportional to unsuppresed viral load. As a result, $\log(\lambda_i)$ depends on 28 structured additive predictors (2 sexes $\times$ 7 age groups $\times$ 2 indicators, HIV prevalence and ART coverage).
2. In the household survey component, HIV incidence and HIV prevalence are linked to the proportion recently infected via Equation \ref{eq:kappa}.
3. In the ANC testing component, HIV prevalence and ART coverage depend upon the respective indicators in the household survey component. Although $\text{logit}(\rho_i)$ and $\text{logit}(\alpha_i)$ are Gaussian, nonetheless this introduces dependence of each mean response on two structured additive predictors.
4. Throughout the model components, processes are modelled at the finest distict-age-sex division, but likelihoods are defined for observations aggregated over sets of indices. As such, single observations are related to $|\mathcal{I}|$ structured additive predictors.
5. Individuals taking ART, or who have been recently infected, must be HIV positive. 
6. The ART attendance component uses a multinomial model with softmax link function which takes as input $|\{x': x' \sim x\}| + 1$ structured additive predictors.
7. Multiple link functions are used throughout the model, such that there is no one inverse link function $g$. Instead, 

If Naomi was written out in full mathematical detail, I might be able to make these reasons more concrete.
It would also be good to specify how much of an impact each of these reasons is likely to have on the difficulty of inference.

# Fast approximate inference method\label{sec:fastinferencemethods}

The joint posterior of the parameters $(\x, \btheta)$ given data $\y$ for an ELGM is given by
\begin{equation*}
  p(\x, \btheta \, | \, \y)
  \propto p(\btheta) |\mathbf{Q}(\btheta)|^{n/2} \exp \left( - \frac{1}{2} \x^\top \mathbf{Q}(\btheta) \x + \sum_{i = 1}^n \log p(y_i \, | \, \x_{\mathcal{J}_i}, \btheta) \right).
\end{equation*}
We consider approximations to the posterior marginals of each latent random variable $x_i$ and hyperparameter $\theta_j$ given by
\begin{align}
  p(x_i \, | \, \y) &\approx \tilde p(x_i \, | \, \y) = \int \tilde p(x_i \, | \, \btheta, \y) \tilde p(\btheta \, | \, \y) \text{d}\btheta, \quad i \in [N], \label{eq:inla1} \\
  p(\theta_j \, | \, \y) &\approx \tilde p(\theta_j \, | \, \y) = \int \tilde  p(\btheta \, | \, \y) \text{d}\btheta_{-j} \quad j \in [m]. \label{eq:inla2}
\end{align}
Given the negative unnormalised log posterior $- \log p(\y, \x, \btheta)$, we obtain the above posterior marginal approximations $\{ \tilde p(x_i \, | \, \y) \}_{i = 1}^n$ and $\{\tilde p(\theta_j \, | \, \y)\}_{j = 1}^m$ via nested applications of the Laplace approximation and AGHQ.

## Laplace approximation

Let $\tilde p_\texttt{G}(\x \, | \, \btheta, \y) = \mathcal{N}(\x \, | \, \hat \x(\btheta), \hat{\Hb}(\btheta)^{-1})$ be a Gaussian approximation to $p(\x \, | \, \btheta, \y)$ with mode and precision matrix given by
\begin{align*}
\hat \x(\btheta) &= \argmax_\x \log p(\y, \x, \btheta), \\
\hat {\Hb}(\btheta) &= - \frac{\partial^2}{\partial \x \partial \x^\top} \log p(\y, \x, \btheta) \rvert_{\x = \hat \x(\btheta)}.
\end{align*}
Then the Laplace approximation to $p(\btheta, \y)$ is given by
\begin{equation}
\tilde p_\texttt{LA}(\btheta, \y) = \frac{p(\y, \x, \btheta)}{\tilde p_\texttt{G}(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \hat \x(\btheta)}. \label{eq:la}
\end{equation}
Inference in TMB proceeds by optimising Equation \ref{eq:la} to obtain $\hat{\btheta}_\texttt{LA} = \argmax_{\btheta} \tilde p_\texttt{LA}(\btheta, \y)$.
Latent field joint and marginal inferences are then direct from the Gaussian approximation $\tilde p_\texttt{G}(\x \, | \, \hat{\btheta}_\texttt{LA}, \y)$.
Hyperparameter inferences are obtained according to method which should be specified here.

## Gauss-Hermite quadrature

Quadrature rules can be used to approximate the integral of $\tilde p_\texttt{LA}(\btheta, \y)$ via a weighted sum
\begin{equation}
p(\y) \approx \int_{\btheta} \tilde p_\texttt{LA}(\btheta, \y) \text{d}\btheta \approx \sum_{\z \in \mathcal{Q}} p_\texttt{LA}(\z, \y) \omega(\z), \label{eq:quad}
\end{equation}
where $\z \in \mathcal{Q}$ are a set of nodes and $\omega: \mathcal{Q} \to \mathbb{R}$ is a weighting function.
Gauss-Hermite quadrature [GHQ; @davis1975methods] is a quadrature rule where, in the univariate case, the nodes $\mathcal{Q}(1, k) = \{z: H_k(z) = (-1)^k \exp(z^2 / 2) \frac{\text{d}}{\text{d}z^k} \exp(-z^2 / 2) = 0\}$ are selected as zeros of the $k$th Hermite polynomial.
The corresponding weights $\omega: \mathcal{Q}(1, k) \to \mathbb{R}$ are given by $\omega(z) = k! / [H_{k + 1}(z)]^2 \phi(z)$, where $\phi(\cdot)$ is a standard univariate Gaussian density.
GHQ is attractive because it is exact for functions which are a polynomial of total order no more than $2k - 1$ multiplied by a Gaussian density.
Multivariate GHQ rules are typically obtained using the product rule such that $\z = (z_1, \ldots, z_m) \in \mathcal{Q}(m, k) = \mathcal{Q}(1, k)^m$ and $\omega(\z) = \prod_{j = 1}^m \omega(z_j)$.

## Adaptive quadrature

In adaptive Gauss-Hermite quadrature [AHGQ; @naylor1982applications; @tierney1986accurate] the nodes are shifted and rotated to suit the particular integrand.
Repositioning the nodes is especially important in statistical quadrature problems, where the integral depends on data $\y$ such that regions of high density cannot reasonably be known in advance.
To obtain an AGHQ estimate of Equation \ref{eq:quad}, let $\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA}) = - \partial^2 \log p_\texttt{LA}(\hat{\btheta}_\texttt{LA}, \y)$ be the curvature at the mode $\hat{\btheta}_\texttt{LA}$ and $[\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA})]^{-1} = \hat{\mathbf{P}}_\texttt{LA} {\hat{\mathbf{P}}_\texttt{LA}}^\top$ a matrix decomposition of the inverse curvature, then
\begin{equation}
\tilde p_\texttt{AGHQ}(\y) = |\hat{\mathbf{P}}_\texttt{LA}|\sum_{\z \in \mathcal{Q}(m, k)} \tilde p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y) \omega(\z). \label{eq:aghq}
\end{equation}
That is, the unadapted nodes have been shifted by the mode and rotated by a decomposition of the inverse curvature such that $\z \mapsto \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}$.
Two options for the particular matrix decomposition are the Cholesky decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{L}}_\texttt{LA}$ and the spectral decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{E}}_\texttt{LA} \hat{\mathbf{\Lambda}}_\texttt{LA}^{1/2}$ [@jackel2005note].
Figure \ref{fig:aghq} demonstrates GHQ, as well as adaption for these two choices of matrix decomposition.
Equation \ref{eq:aghq} may then be used to normalise the Laplace approximation
\begin{equation*}
\tilde p_\texttt{LA}(\btheta \, | \, \y) = \frac{\tilde p_\texttt{LA}(\btheta, \y)}{\tilde p_\texttt{AGHQ}(\y)}.
\end{equation*}

To obtain inferences for the latent field (Equation \ref{eq:inla1}) the adapted nodes and weights are reused [@rue2009approximate; @stringer2022fast]
\begin{equation}
\tilde p(\x \, | \, \y) = |\hat{\mathbf{P}}_\texttt{LA}| \sum_{\z \in \mathcal{Q}(m, k)} \tilde p_\texttt{G}(\x \, | \, \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y) \tilde p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA} \, | \, \y) \omega(\z). \label{eq:nest}
\end{equation}
Samples from this mixture of Gaussians may be obtained by drawing a node $\z$ with multinomial probabilities $\lambda(\z) = |\hat{\mathbf{P}}_\texttt{LA}| p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA} \, | \, \y) \omega(\z)$, then drawing from the Gaussian $\tilde p_\texttt{G}(\x \, | \, \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y)$.

```{r aghq, fig.cap="The Gauss-Hermite quadrature nodes $\\z \\in \\mathcal{Q}(2, 3)$ for this two dimensional integral with three nodes per dimension are adapted based on the mean and covariance matrix of the target via the Cholesky decomposition or spectral decompostion of the inverse curvature at the mode."}
mu <- c(1, 1.5)
cov <- matrix(c(2, 1, 1, 1), ncol = 2)

obj <- function(theta) {
  mvtnorm::dmvnorm(theta, mean = mu, sigma = cov)
}

grid <- expand.grid(
  theta1 = seq(-2, 5, length.out = 700),
  theta2 = seq(-2, 5, length.out = 700)
)

ground_truth <- cbind(grid, pdf = obj(grid))

plot0 <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = "lightgrey") +
  coord_fixed(xlim = c(-2, 4.5), ylim = c(-2, 4.5), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal() +
  guides(size = FALSE) +
  theme(
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    axis.text.y = element_blank(), axis.ticks.y = element_blank()
  )

gg <- mvQuad::createNIGrid(2, "GHe", 3)

add_points <- function(plot0, gg) {
  plot0 +
    geom_point(
      data = mvQuad::getNodes(gg) %>%
              as.data.frame() %>%
              mutate(weights = mvQuad::getWeights(gg)),
      aes(x = V1, y = V2, size = weights),
      alpha = 0.8,
      col = "#009E73",
      inherit.aes = FALSE
    ) +
    scale_size_continuous(range = c(1, 2))
}

plot1 <- add_points(plot0, gg) +
  labs(size = "")

#' Adapt by the mean
gg2 <- gg
mvQuad::rescale(gg2, m = mu, C = diag(c(1, 1)), dec.type = 2)

plot2 <- add_points(plot0, gg2) +
  labs(size = "")

#' Adapt by the lower Cholesky
gg3 <- gg
mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 2)

plot3 <- add_points(plot0, gg3) +
  labs(size = "")

#' Adapt by the spectral
gg4 <- gg
mvQuad::rescale(gg4, m = mu, C = cov, dec.type = 1)

plot4 <- add_points(plot0, gg4) +
  labs(size = "")

#' PCA-AGHQ
gg5 <- mvQuad::createNIGrid(2, "GHe", level = c(3, 1))
mvQuad::rescale(gg5, m = mu, C = cov, dec.type = 1)

plot5 <- add_points(plot0, gg5) +
  labs(size = "")

(plot1 + plot2) / (plot3 + plot4 + plot5)
```

## Principal components analysis

Use of the product rule requires $|\mathcal{Q}(m, k)| = k^m$ quadrature points.
This quickly becomes intractable as $m$ increases for $k > 1$.
An alternative is to let $\bk = (k_1, \ldots, k_m)$ be a vector of levels for each dimension of $\btheta$.
We may then define $\mathcal{Q}(m, \bk) = \mathcal{Q}(1, k_1) \times \cdots \times \mathcal{Q}(1, k_m)$ of size $|\mathcal{Q}(m, \bk)| = \prod_{j = 1}^m k_j$.
Let $\mathcal{Q}(m, s, k)$ correspond to $\mathcal{Q}(m, \bk)$ with choice of levels $k_j = k, j \leq s$ and $k_j = 1, j > s$ for some $s \leq m$.
Taken together with use of the spectral decomposition, this choice of levels is analogous to a principal components analysis (PCA) approach to AGHQ
\begin{equation}
\tilde p_\texttt{PCA}(\y) = |\hat{\mathbf{E}}_{\texttt{LA}, s} \hat{\mathbf{\Lambda}}_{\texttt{LA}, s}^{1/2}|\sum_{\z \in \mathcal{Q}(m, s, k)} \tilde p_\texttt{LA}(\hat{\mathbf{E}}_{\texttt{LA}, s} \hat{\mathbf{\Lambda}}_{\texttt{LA}, s}^{1/2} \z + \hat{\btheta}_\texttt{LA}, \y) \omega(\z),
\end{equation}
where $\hat{\mathbf{E}}_{\texttt{LA}, s} = \cdots$ and $\hat{\mathbf{\Lambda}}_{\texttt{LA}, s} = \cdots$ are terms which should be described.
The final panel of Figure \ref{fig:aghq} illustrates a case where $m = 2$ and $s = 1$ such that 
We refer to use of this quadrature rule as PCA-AGHQ.

## Sparse rules

Though there are sparse rules which retain the attractive exactness properties of GHQ, using fewer than $k^m$ quadrature points, they use weighting functions which may be negative $\omega(\z) < 0$.
This introduces a problem when trying to produce inferences about the latent field, as the resulting $\lambda(\z)$ may be negative.
We are not aware of any suitable approaches to obtain multinomial samples in such cases.

# Application to data from Malawi\label{sec:results}

We fit the simplified Naomi model (Section \ref{sec:naomi}) to data from Malawi using three inferential approaches.
For each approach, the `TMB` C++ user-template used to specify the log-posterior was the same.
The three approaches were:
1. TMB,
2. PCA-AGHQ, and
3. NUTS: the Hamiltonian Monte Carlo (HMC) algorithm No-U-Turn Sampling (NUTS) using Stan [@carpenter2017stan] via the `tmbstan` package [@monnahan2018no].
The dimension of the latent field was $N = 467$ and the dimension of the hyperparameters was $m = 24$.
Settings used for each inferential method are provided in Table \ref{tab:inference-methods}, and, where relevant, discussed further below.
For the deterministic methods, following inference we simulated hyperparameter and latent field samples.
For all methods, we simulated age-sex-district specific HIV prevalence, ART coverage and HIV incidence from the latent field and hyperparameter posteriors.
<!-- Note: eventually this figure should be generated using the algorithm proposed in the paper -->
Example model outputs from TMB are illustrated in Figure \ref{fig:naomi-results}.
The \textsc{R} [@r] code used to produce all results we describe below is available at `github.com/athowes/elgm-inf`.
We used `orderly` [@orderly] for reproducible research, `ggplot2` for data visualisation [@wickham2016ggplot2] and `rticles` [@allaire2022rticles] for reporting via `rmarkdown` [@allaire2022rmarkdown].

```{r naomi-results, fig.cap="District-level model outputs for adults aged 15-49. Inference conducted with TMB."}
knitr::include_graphics("depends/naomi_results.png")
```

\begin{table}[]
\small
\begin{tabularx}{\textwidth}{p{0.15\linewidth}p{0.15\linewidth}p{0.6\linewidth}}
\toprule
Name & Software & Details \\
\midrule
TMB & \texttt{TMB} & $1000$ samples \\
PCA-AGHQ & \texttt{aghq} & $k = 3, s = 8$ (see Section \ref{sec:pca-settings}), $1000$ samples \\
NUTS & \texttt{tmbstan} & $4$ chains of $20000$ iterations, with the first $10000$ iterations of each chain discarded as warmup, thinned by a factor of $20$. Default NUTS tuning parameters \citep{hoffman2014no}. \\
\bottomrule
\end{tabularx}
\caption{A summary of settings used for each inferential method.}
\label{tab:inference-methods}
\small
\end{table}

## NUTS convergence

We obtained satisfactory NUTS results by gradually increasing the amount of computation until all diagnostics were acceptable.
These include the potential scale reduction factors $\hat R$ [@gelman1992inference; @vehtari2021rank] for each parameter, bulk and tail effective sample sizes, autocorrelation decay plots, univariate traceplots, pairs density plots, and NUTS specific divergent transition and energy assessments [@betancourt2017conceptual].
For full details see Appendix S2.
We treat the NUTS results as a gold-standard.

## PCA-AGHQ settings \label{sec:pca-settings}

We used a Scree plot to select the number of principal components $s < m$ to keep.
Figure showing Scree plot.
Figure showing approximation to precision matrix.

Figure \ref{fig:node-positions} shows the positions of the generated PCA-AGHQ nodes overlaid onto the hyperparameter marginal posteriors obtained NUTS.
For around 12 of the 24 hyperparameters, the marginal was well covered by the nodes.
Although this is an improvement on the $s = 8$ that could be covered naively, there remain many hyperparameters uncovered.
Furthermore, it is likely that the hyperparameters with skewed marginal posteriors are more important to cover well.
We have not yet found a way to take this into account.

```{r node-positions, fig.cap="PCA-AGHQ node positions (green, rug plot) overlaid onto the hyperparameter marginal posteriors (grey, histogram) for each of the 24 hyperparameters."}
knitr::include_graphics("depends/nodes-samples-comparison.png")
```

Demonstrate that the 12 hyperparameters which are well covered are the ones that you would expect given their marginal standard deviations.

One crude approach for assessing appropriateness of the grid is to look at the estimate of $\log p_\texttt{PCA}(\y)$ for a range of settings.
Convergence in $\log p_\texttt{PCA}(\y)$ as $s$ and $k$ are increased may suggest sufficiency has been reached.
Figure shows those values which were computable in a reasonable time.

## Model assessment \label{sec:model-assessment}

### Posterior contraction

Let $\phi$ be a generic model parameter.
To assess the informativeness of the data we compared (Figure \ref{fig:contraction}) the prior variance $\sigma_\text{prior}^2(\phi)$ to the posterior variance $\sigma_\text{posterior}^2(\phi)$ via the posterior contraction [@schad2021toward]
\begin{equation*}
c(\phi) = 1 - \frac{\sigma_\text{posterior}^2(\phi)}{\sigma_\text{prior}^2(\phi)}.
\end{equation*}
For the parameters of length greater than one, we averaged the posterior contraction.

```{r contraction, fig.cap="Posterior contraction."}
knitr::include_graphics("depends/posterior-contraction.png")
```

### Coverage

We assessed the coverage of our estimates via the uniformity of the data within each posterior marginal distribution.
Let $\{\phi_i\}_{i = 1}^n$ be posterior marginal samples.

## Inference comparison \label{sec:inf-comparison}

We used three statistical approaches to assess the accuracy of posterior distributions produced by TMB and PCA-AGHQ as compared with those from NUTS: (1) Kolmogorov-Smirnov tests, (2) Pareto-smoothed importance sampling, and (3) maximum mean discrepancy.

### Kolmogorov-Smirnov tests

The two-sample Kolmogorov-Smirnov (KS) test statistic [@smirnov1948table] is the maximum absolute difference between two empirical cumulative distribution (ECDF) functions $F(\varphi) = \frac{1}{n} \sum_{i = 1}^n \mathbb{I}_{\phi_i \leq \varphi}$.
We compare the KS statistics
\begin{align*}
D_\text{TMB} &= \sup_\varphi | F_\text{NUTS}(\varphi) - F_\text{TMB}(\varphi)|, \\
D_\text{PCA-AGHQ} &= \sup_\varphi | F_\text{NUTS}(\varphi) - F_\text{PCA-AGHQ}(\varphi)|.
\end{align*}
See an illustration of the KS test in Figure \ref{fig:ks} and a summary of the results in Table.

```{r ks, fig.height = 3, fig.cap="Example KS test for one parameter."}
df_compare <- readRDS("depends/beta_alpha.rds") %>%
  filter(method != "adam")

mean <- df_compare %>%
  filter(method == "tmbstan") %>%
  summarise(mean = mean(samples)) %>%
  pull(mean) %>%
  round(digits = 3)

sd <- df_compare %>%
  filter(method == "tmbstan") %>%
  summarise(sd = sd(samples)) %>%
  pull(sd) %>%
  round(digits = 3)

histogram <- df_compare %>%
  mutate(method = fct_recode(method, "PCA-AGHQ" = "aghq", "NUTS"= "tmbstan")) %>%
  ggplot(aes(x = samples, fill = method, col = method)) +
  geom_histogram(aes(y = after_stat(density)), alpha = 0.5, position = "identity", bins = 30) +
  theme_minimal() +
  facet_grid(method~.) +
  labs(x = "beta_alpha", y = "Density", fill = "Method") +
  scale_color_manual(values = colours) +
  scale_fill_manual(values = colours) +
  theme(legend.position = "none")

grid <- seq(from = min(df_compare$samples), to = max(df_compare$samples), length.out = 1000)

tmb_ecdf <- stats::ecdf(filter(df_compare, method == "TMB") %>% pull(samples))
tmb_ecdf_df <- data.frame(x = grid, ecdf = tmb_ecdf(grid), method = "TMB")

aghq_ecdf <- stats::ecdf(filter(df_compare, method == "aghq") %>% pull(samples))
aghq_ecdf_df <- data.frame(x = grid, ecdf = aghq_ecdf(grid), method = "aghq")

tmbstan_ecdf <- stats::ecdf(filter(df_compare, method == "tmbstan") %>% pull(samples))
tmbstan_ecdf_df <- data.frame(x = grid, ecdf = tmbstan_ecdf(grid), method = "tmbstan")

# Add ECDF differences
tmb_ecdf_df$ecdf_diff <- tmbstan_ecdf_df$ecdf - tmb_ecdf_df$ecdf
aghq_ecdf_df$ecdf_diff <- tmbstan_ecdf_df$ecdf - aghq_ecdf_df$ecdf
tmbstan_ecdf_df$ecdf_diff <- 0

absmax <- function(x) x[which.max(abs(x))]

ks_tmb <- absmax(tmb_ecdf_df$ecdf_diff)
ks_aghq <- absmax(aghq_ecdf_df$ecdf_diff)

ecdf_df <- bind_rows(tmb_ecdf_df, aghq_ecdf_df, tmbstan_ecdf_df)

ecdf_df$method <- factor(ecdf_df$method, levels = c("TMB", "aghq", "tmbstan"))

ks_labeller <- function(x) toString(round(abs(x), 2))

ecdf_diff <- ggplot(ecdf_df, aes(x = x, y = ecdf_diff, col = method)) +
  geom_line() +
  geom_abline(intercept = ks_tmb, slope = 0, col = colours[1], linetype = "dashed", alpha = 0.8) +
  annotate("text", x = 1.1 * max(ecdf_df$x), y = ks_tmb, label = ks_labeller(ks_tmb), col = colours[1], alpha = 0.8) +
  geom_abline(intercept = ks_aghq, slope = 0, col = colours[2], linetype = "dashed", alpha = 0.8) +
  annotate("text", x = 1.1 * max(ecdf_df$x), y = ks_aghq, label = ks_labeller(ks_aghq), col = colours[2], alpha = 0.8) +
  scale_color_manual(values = colours) +
  labs(x = "beta_alpha", y = "ECDF difference") +
  guides(col = "none") +
  coord_cartesian(xlim = c(min(ecdf_df$x), max(ecdf_df$x)), clip = "off") +
  theme_minimal() +
  theme(plot.margin = unit(c(1, 3, 1, 1), "lines"))

histogram + ecdf_diff
```

```{r}
ks_summary <- readRDS("depends/ks_summary.rds")

ks_summary %>%
  filter(Type == "Latent field") %>%
  group_by(Parameter) %>%
  summarise(
    `D(TMB)` = round(mean(`KS(TMB, tmbstan)`), 3),
    `D(PCA-AGHQ)` = round(mean(`KS(aghq, tmbstan)`), 3)
  ) %>%
  gt::gt(rowname_col = "Parameter") %>%
  gt::summary_rows(
   columns = everything(),
   fns = list(Average = ~ mean(.x)),
   formatter = gt::fmt_number,
   decimals = 3
  )
```

### Pareto-smoothed importance sampling

As well as marginal distributions, we are interested in assessing the accuracy of joint distributions.
Let $\{\bphi_i\}_{i = 1}^n$ be generic samples from a joint posterior.
Pareto-smoothed importance sampling (PSIS) [@vehtari2015pareto, @yao2018yes] is a method for stabilising the ratios used in importance sampling.
See a summary of the results in Table and Figure \ref{fig:psis}.

```{r psis, fig.cap="Results of Pareto-smoothed importance sampling analysis."}
```

### Maximum mean discrepancy

Another way to compare joint distributions is via the maximum mean discrepancy [MMD; @gretton2006kernel].
Let $\Phi = \{\bphi_i\}_{i = 1}^n$ and $\Psi = \{\bpsi_i\}_{i = 1}^n$ be two sets of samples from a joint posterior obtained using different inference methods.
The MMD can be empirically estimated by
\begin{equation*}
\text{MMD}(\Phi, \Psi) = \sqrt{\frac{1}{n^2} \sum_{i, j = 1}^n k(\bphi_i, \bphi_j) - \frac{2}{n^2} \sum_{i, j = 1}^n k(\bphi_i, \bpsi_j) + \frac{1}{n^2} \sum_{i, j = 1}^n k(\bpsi_i, \bpsi_j)}.
\end{equation*}
See a summary of the results in Table and Figure \ref{fig:mmd}.

```{r mmd, fig.cap="Results of maximum mean discrepancy analysis."}
```

## Case study on exceedance probabilites \label{sec:exceedance}

### Meeting the second 90

The Joint United Nations Programme on HIV/AIDS has developed ambitious fast-track targets for scaling up ART treatment with the goal of "ending the AIDS epidemic by 2030".
Specifically, the "90-90-90 by 2020" fast-track target is that:

* 90% of PLHIV know their status,
* 90% of those are on antiretroviral therapy (ART), and
* 90% of those have suppressed viral load.

Naomi can be used to identify treatment gaps by calculating the probability that the second 90 target has been met $\mathbb{P}(\alpha_i > 0.81)$ for each strata $i$.
We found that both TMB and PCA-AGHQ underestimate the probability that the second 90 target has been met in women (Figure \ref{fig:second90}).
This difficulty may be related to interactions between the household survey and ANC components of the model.

```{r second90, fig.cap="Results of second 90 case-study."}
knitr::include_graphics("depends/second90.png")
```

### Finding strata with high incidence

Some HIV interventions are cost-effective only within high HIV incidence settings, typically defined as greater than 1% incidence per year.
Naomi can be used to assess the probability of a strata having high incidence by evaluating $\mathbb{P}(\lambda_i > 0.01)$.
We found that both TMB and PCA-AGHQ overestimate these exceedance probabilities (Figure \ref{fig:1inc}).
We do not yet have a working hypothesis as to why this is.

```{r 1inc, fig.cap="Results of high incidence case-study."}
knitr::include_graphics("depends/1inc.png")
```

# Discussion\label{sec:conclusions}

We developed an approximate Bayesian inference algorithm motivated by a challenging problem in small-area estimation of HIV.
For the simplified Naomi model in Malawi (Section \ref{sec:results}) our method is demonstrated to be more accurate than TMB, and substantially faster than NUTS.
We anticipate that our method could be added to the Naomi web interface as an alternative to TMB.
This would enable analysts to quickly iterate over model options using a faster, less accurate inference approach, before switching to a slower, more accurate approach once they are happy with the results.

We provide a flexible implementation of the algorithm, building on the `TMB` and `aghq` \textsc{R} packages.
In doing so, we hope our work enables use of deterministic inference algorithms for ELGMs in applied settings, as well as further methodological exploration of their accuracy and limitations.
Among the ELGM-type structures of particular interest in spatial epidemiology, many of which feature in Naomi, are: aggregated Gaussian process models [@nandi2020disaggregation], evidence synthesis models [@amoah2020geostatistical].
Although our method is designed for ELGMs, it is possible to use it outside this class, as it is compatible with any model with a `TMB` \textsc{C++} template.

We demonstrated a Bayesian workflow for deterministic inference methods.
We retained the ability to draw samples from the posterior distributions of interest, facilitating use of posterior predictive checks (Section \ref{sec:model-assessment}) and methods for inference comparison such as Pareto-smoothed importance sampling (Section \ref{sec:inf-comparison}).
The exceedance probabilities case-study (Section \ref{sec:exceedance}) demonstrates the importance of accurate posterior inferences for realistic use-cases of the Naomi model.  

Future work could look to implement our algorithm within probabilistic programming languages, facilitating access by a broader user-base.
This might be possible in Stan by use of the `bridgestan` package [@bridgestan] together with the adjoint-differentiated Laplace approximation of @margossian2020hamiltonian.
<!-- Note: can fit hyperparameters with Laplace on latent field using HMC via tmbstan::tmbstan(laplace = TRUE) -->
<!-- i.e. what Charles Margossian has implemented for Stan is done similarly by default in TMB -->
As well, statistical theory for the algorithm could be established by extension of Theorem 1 in @stringer2022fast.

# Acknowledgements {-}

AH was supported by the EPSRC Centre for Doctoral Training in Modern Statistics and Statistical Machine Learning (EP/S023151/1), and conducted part of this research while an International Visiting Graduate Student at the University of Waterloo.
AH and JWE were supported by the Bill and Melinda Gates Foundation (OPP1190661, OPP1164897).
SRF was supported by the EPSRC (EP/V002910/2).
JWE was supported by UNAIDS and National Institute of Allergy and Infectious Disease of the National Institutes of Health (R01AI136664).
This research was supported by the MRC Centre for Global Infectious Disease Analysis (MR/R015600/1), jointly funded by the UK Medical Research Council (MRC) and the UK Foreign, Commonwealth \& Development Office (FCDO), under the MRC/FCDO Concordat program and is also part of the EDCTP2 programme supported by the European Union.
