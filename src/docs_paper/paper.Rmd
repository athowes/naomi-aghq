---
title: Fast approximate Bayesian inference of HIV indicators using the Naomi small-area estimation model
author:
  - name: Adam 
    surname: Howes
    email: ath19@ic.ac.uk
    label: e1
    addressLabel: A,E
    sepNext: ","
  - name: Alex
    surname: Stringer
    email: alex.stringer@uwaterloo.ca
    label: e2
    addressLabel: B
  - name: Seth R.
    surname: Flaxman
    email: seth.flaxman@cs.ox.ac.uk
    label: e3
    addressLabel: C
    sepNext: ","
  - name: Jeffrey W.
    surname: Eaton
    email: jeaton@hsph.harvard.edu
    label: e4
    addressLabel: D,E
affiliation:
  - label: A
    name: Department of Mathematics, Imperial College London
    authorsLabels: e1
  - label: B
    name: Department of Statistics and Actuarial Science, University of Waterloo
    authorsLabels: e2
  - label: C
    name: Department of Computer Science, University of Oxford
    authorsLabels: e3
  - label: D
    name: Center for Communicable Disease Dynamics, Harvard T.H. Chan School of Public Health, Harvard University
    authorsLabels: e4
  - label: E
    name: MRC Centre for Global Infectious Disease Analysis, School of Public Health, Imperial College London

abstract: |
  | Naomi is a spatial evidence synthesis model used to produce district-level HIV epidemic indicators in sub-Saharan Africa. Multiple outcomes of policy interest, including HIV prevalence, HIV incidence, and antiretroviral therapy treatment coverage are jointly modelled using both household survey data and routinely reported health system data. The model is provided as a tool for countries to input their data to and generate estimates during a yearly process supported by UNAIDS. Currently, inference is conducted using empirical Bayes and a Gaussian approximation via the \texttt{TMB} \textsc{R} package. We propose a new inference method which extends adaptive Gauss-Hermite quadrature to deal with >20 hyperparameters, enabling fast and accurate inference for Naomi and other extended latent Gaussian models. Using data from Malawi, our method provides more accurate inferences than \texttt{TMB}, and is substantially faster to run to Hamiltonian Monte Carlo with the No-U-Turn sampler. By extending the \texttt{aghq} \textsc{R} package we facilitate easy, flexible use of our method when provided a \texttt{TMB} \textsf{C++} template for the model's log-posterior.
keyword-subclass: | 
 \begin{keyword}[class=MSC2020] % It must be define for aap, aop, aos journals. For aoas, sts is not used
 \kwd[Primary ]{00X00}
 \kwd{00X00}
 \kwd[; secondary ]{00X00}
 \end{keyword}
keywords: 
  - Bayesian statistics
  - spatial statistics
  - evidence synthesis
  - small-area estimation
  - approximate inference
  - INLA
  - AGHQ
  - HIV epidemiology

predefined-theoremstyle: true # use in section Environments for Axiom, Theorem, etc
bibliography: citations.bib
biblio-style: imsart-nameyear # alternative: imsart-number
output:
  rticles::ims_article:
    journal: aoas # aap, aoas, aop, aos, sts. See documentation
    toc: false # Please use for articles with 50 pages and more
    includes:
      in_header: preamble.tex
---

<!-- Seth notes that Naomi is a spatial evidence synthesis model used to produce district- level HIV epidemic indicators in sub-Saharan Africa is not the right topic sentence for the abstract -->
<!-- Better to lead with something about the inference method? -->

```{r echo = FALSE}
colours <- c("#56B4E9", "#009E73", "#E69F00")

options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

<!-- Most published papers will not exceed 20 pages -->
<!-- We strongly encourage submission of data sets, computer algorithms and supporting material -->

# Introduction

Accurate estimates of HIV indicators are crucial for mounting an effective public health response to the HIV epidemic.
These estimates should be timely, and at a geographic level at which health systems are planned and delivered.
Producing granular estimates is challenging, in large part due to limitations of the available data sources.
Nationally-representative household surveys provide the most statistically reliable data, but are only conducted every five years or so in most countries, with limited sample size at the district level, due to their high costs to run.
Other data sources, such as routine health surveillance of antenatal care (ANC) clinics, are available in closer to real-time, but based on limited or non-representative samples of the population.
To address these challenges, the Naomi small-area estimation model [@eaton2021naomi] synthesises data from multiple sources to estimate HIV indicators at a district-level, by age and sex.
Modelling multiple data sources jointly mitigates the limitations of any single source, increases statistical power, and can prompts investigation into any conflicts of information.

Software (\url{https://naomi.unaids.org}) has been developed for Naomi, allowing countries to input their data and interactively generate estimates during workshops as a part of a yearly process supported by UNAIDS.
Creation of estimates by country teams, rather than external agencies or researchers, is an important and distinctive feature of the HIV response.
Drawing on expertise closest to the data being modelled improves the accuracy of the process, as well as strengthening trust in the resulting estimates, creating a virtuous cycle of data quality, use and ownership.

Naomi is a complex model, and as such presents a challenging Bayesian inference problem.
As well as hundreds of latent field parameters, Naomi has >20 hyperparameters: substantially more than the small number typically required for use of integrated nested Laplace approximations [INLA; @rue2009approximate].
Moreover, observations depend on multiple structured additive predictors, such that Naomi falls into the class of extended latent Gaussian models [ELGMs; @stringer2022fast].
As such, the model is not easily specified by any software which requires a formula interface.

To allow for interactive review and iteration of model results, the inference procedure should be fast for workshop participants to fit the model on a laptop.
Due to the scale of the model and features of its posterior geometry [@neal2003slice], Markov chain Monte Carlo (MCMC) approaches are therefore prohibitively slow.
Furthermore, it is preferable that use of the inference method across countries to be automatic, and not require substantial statistical expertise, as would be the case for monitoring MCMC convergence and suitability.

To meet these requirements, inference is currently conducted using an empirical Bayes (EB) approach, with a Gaussian approximation to the latent field, via the Template Model Builder (`TMB`) \textsf{R} package [@kristensen2016tmb].
Owing to its speed and flexibility, `TMB` is gaining popularity more broadly, particularly in spatial statistics [@osgood2022statistical].
Inference in `TMB` is based on optimisation of a \textsf{C++} template function, with the option available to use a Laplace approximation to integrate out any subset of the parameters.
For the Naomi model, this subset is the high-dimensional latent field, leaving a smaller number of hyperparameters.
`TMB` uses automatic differentiation [@baydin2017automatic] to calculate the derivatives required for numerical optimisation routines and the Laplace approximation, Taking inspiration from the AD Model Builder package [@fournier2012ad].

Although the approach of `TMB` is fast, within the empirical Bayes framework hyperparameter uncertainty is not properly accounted for in the latent field posterior.
This consideration motivated us to look for an approach closer to full Bayesian inference, which is also flexible enough to be compatible with the model, as well as fast enough to be run in production by country teams.
To obtain fast, accurate Bayesian inferences for the Naomi model we developed an inference method which extends adaptive Gauss-Hermite quadrature to handle many hyperparameters.
We used principal components analysis to create a computationally feasible quadrature grid.
Our method is implemented as an extension of the `aghq` \textsf{R} package [@stringer2021implementing].
As `aghq` is designed to naturally interface with `TMB`, use is simple when provided a \textsf{C++} user template for the log-posterior.

Other work aiming to extend the scope of the INLA method includes the `inlabru` \textsc{R} package [@bachl2019inlabru], INLA within MCMC [@gomez2018markov], and importance sampling with INLA [@berild2022importance], all of which leverage the `R-INLA` \textsc{R} package [@martins2013bayesian].
`inlabru` approximates non-linear predictors using linearisation, which involves making iterative calls to `R-INLA`.
<!-- See https://inlabru-org.github.io/inlabru/articles/method.html -->
<!-- Want to say in particular what the class of models that `inlabru` allows you to fit are -->
<!-- Don't know the answer to this yet, but asking Janine Illian I'm told you don't have to call R-INLA that many times -->
<!-- I think it'd obviously depend on the exact details of the problem at hand -->
INLA within MCMC and importance sampling with INLA are suitable for models which are LGMs conditional on some subset of the parameters being fixed.
<!-- It would or would not be possible to fit the Naomi model in these frameworks. -->

The remainder of this paper is organised as follows.
Section \ref{sec:naomi} outlines the version of the Naomi model that we consider in this paper, and  Section \ref{sec:elgm} describes how it falls within the ELGM framework.
Section \ref{sec:fastinferencemethods} outlines our approach to fast, accurate Bayesian inference for ELGMs using simplified INLA and AGHQ.
As a case study, we compare the accuracy of our inference method to `TMB` and `tmbstan` for the simplified Naomi model fit to data from Malawi, in Section \ref{sec:naomi}.
We also demonstrate a Bayesian workflow, illustrating the applicability of these tools in a deterministic inference setting.
Finally, in Section \ref{sec:conclusions} we discuss our conclusions, how we anticipate our method might be useful for other models, and directions for future research.

# Simplified Naomi model\label{sec:naomi}

@eaton2021naomi specify a joint model linking three small-area estimation models.
We consider a simplified version defined only at the time of the most recent household survey with HIV testing, omitting nowcasting and temporal projection, as these time points involve limited inferences.
An overview of the simplified model is given below, and a more complete mathematical description is provided in Appendix S1.

## Household survey component \label{sec:household}

Consider a country in sub-Saharan Africa where a household survey with complex survey design has taken place.
Let $x \in \mathcal{X}$ index district, $a \in \mathcal{A}$ index five-year age group, and $s \in \mathcal{S}$ index sex.
For ease of notation, let $i$ index the finest district-age-sex division included in the model.
Let $I \subseteq \mathcal{X} \times \mathcal{A} \times \mathcal{S}$ be a set of indices $i$ for which an aggregate observation is reported, with $I \in \mathcal{I}$.

Let $N_i \in \mathbb{N}$ be the known, fixed population size.
We infer the following unknown HIV indicators using linked regression equations: HIV prevalence $\rho_i \in [0, 1]$, the proportion of individuals who are HIV positive; antiretroviral therapy (ART) coverage $\alpha_i \in [0, 1]$, the proportion of people living with HIV who receive ART treatment; and annual HIV incidence rate $\lambda_i > 0$, the yearly rate of new HIV infections occurring.
Independent logistic regression models for HIV prevalence and ART coverage in the general population are specified such that $\text{logit}(\rho_i) = \eta^\rho_i$ and $\text{logit}(\alpha_i) = \eta^\alpha_i$, for certain choice of structured additive predictors.
HIV incidence rate is modelled on the log scale as $\log(\lambda_i) = \eta^\lambda_i$, and depends on adult HIV prevalence and adult ART coverage. 
Let $\kappa_i$ be the proportion recently infected among HIV positive persons.
We link this proportion to HIV incidence via
\begin{equation}
\kappa_i = 1- \exp \left( - \lambda_i \cdot \frac{1 - \rho_i}{\rho_i} \cdot (\Omega_T - \beta_T) - \beta_T \right), \label{eq:kappa}
\end{equation}
where the mean duration of recent infection $\Omega_T$ and the proportion of long-term HIV infections misclassified as recent $\beta_T$ are strongly informed by priors for the particular survey.

These processes are informed by household survey data.
For $\theta \in \{\rho, \alpha, \kappa\}$ let
\begin{equation*}
\hat \theta_I = \frac{\sum_j w_j \cdot\theta_j}{\sum_j w_j}
\end{equation*}
be weighted, aggregate survey observations, with individual responses $\theta_j \in \{0, 1\}$ and design weights $w_j$.
The index $j$ is across all individuals in strata $i \in I$ within the relevant denominator i.e. for ART coverage, only those individuals who are HIV positive.
The observed number of outcomes are then $y^{\theta}_{I} = m^{\theta}_{I} \cdot \hat \theta_{I}$ where
\begin{equation*}
m^{\theta}_I = \frac{\left(\sum_j w_j\right)^2}{\sum_j w_j^2},
\end{equation*}
is the Kish effective sample size [@kish1965survey].
We use a binomial working likelihood
\begin{equation*}
y^{\theta}_{I} \sim \text{xBin}(m^{\theta}_{I}, \theta_{I})
\end{equation*}
to model these aggregate observations, where $\theta_{I}$ are the following weighted aggregates
\begin{equation*}
\rho_{I} = \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}, \quad
\alpha_{I} = \frac{\sum_{i \in I} N_i \rho_i \alpha_i}{\sum_{i \in I} N_i \rho_i}, \quad
\kappa_{I} = \frac{\sum_{i \in I} N_i \rho_i \kappa_i}{\sum_{i \in I} N_i \rho_i}.
\end{equation*}

## ANC testing component \label{sec:anc}

HIV prevalence $\rho^\text{ANC}_i$ and ART coverage $\alpha^\text{ANC}_i$ among pregnant women are modelled as offset from the general population indicators as follows
\begin{align*}
\text{logit}(\rho^\text{ANC}_i) &= \text{logit}(\rho_i) + \eta^{\rho^\text{ANC}}_i, \\
\text{logit}(\alpha^\text{ANC}_i) &= \text{logit}(\alpha_i) + \eta^{\alpha^\text{ANC}}_i.
\end{align*}
These processes are informed by likelihoods specified for aggregate ANC data from the year of the most recent survey.
Of the number of ANC clients with ascertained status $m^{\rho^\text{ANC}}_I$, we model the number of those with positive status $y^{\rho^\text{ANC}}_I$ and the number of those already on ART prior to their first ANC visit $y^{\alpha^\text{ANC}}_I$ using nested binomial likelihoods
\begin{align*}
y^{\rho^\text{ANC}}_I &\sim \text{Bin}(m^{\rho^\text{ANC}}_I, \rho^\text{ANC}_{I}), \\
y^{\alpha^\text{ANC}}_I &\sim \text{Bin}(y^{\rho^\text{ANC}}_I, \alpha^\text{ANC}_{I}),
\end{align*}
As above, we use weighted aggregates
\begin{equation*}
\rho^\text{ANC}_{I} = \frac{\sum_{i \in I} \Psi_i \rho_i^\text{ANC}}{\sum_{i \in I} \Psi_i}, \quad
\alpha^\text{ANC}_{I} = \frac{\sum_{i \in I} \Psi_i \rho_i^\text{ANC} \alpha^\text{ANC}_i}{\sum_{i \in I} \Psi_i \rho_i^\text{ANC}},
\end{equation*}
with $\Psi_i$ the number of pregnant women, which we assume to be fixed.

## ART attendance component \label{sec:art}

People living with HIV sometimes choose to access ART services outside of the district that they reside in.
To account for this, we use multinomial logistic regressions to model the probabilities of accessing services outside the home district.
Briefly, let $\gamma_{x, x'}$ be the probability that a person on ART residing in district $x$ receives ART in district $x'$, and assume $\gamma_{x, x'} = 0$ unless $x = x'$ or the two districts are neighbouring, denoted by $x \sim x'$.
The log-odds $\tilde \gamma_{x, x'} = \text{logit}(\gamma_{x, x'})$ are modelled using a structured additive predictor $\eta_x^{\tilde \gamma}$ which only depends on the home district $x$, such that travel to each neighbouring district, for all age-sex strata, is equally likely.
We then model aggregate ART attendance data $y^{N^\text{ART}}_I$ using a Gaussian approximation to a sum of binomials.
This sum is over both strata $i \in I$ and the number of ART clients travelling from district $x'$ to $x$.
More details regarding this part of the model are provided in Appendix S1.

## Collected together

Let $\mathbf{y} = (y^{\theta}_I)$ for $\theta \in \{\rho, \alpha, \kappa, \rho^\text{ANC}, \alpha^\text{ANC}, N^\text{ART}\}$ and $I \in \mathcal{I}$ be a concatenated vector of observations.
Here, we could attempt to write the Naomi model fully in a small number of equations, but likely it'd be difficult.

# Extended Latent Gaussian models\label{sec:elgm}

We now describe the popular latent Gaussian class of models, and an extension which encapsulates the complexities of Naomi.

## Definitions

Latent Gaussian models [LGMs; @rue2009approximate] are three-stage hierarchical models of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \btheta_1), \quad i \in [n]\\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}),
\end{align*}
where $[n] = \{1, \ldots, n\}$.
The response variable is $\y = (y)_{i \in [n]}$ with likelihood $p(\y \, | \, \bmeta, \btheta_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \btheta_1)$, where $\bmeta = (\eta)_{i \in [n]}$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\btheta_1 \in \mathbb{R}^s$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, linear effects $\beta_j$ of the covariates $z_{ji}$, and unknown functions $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian priors.
It is convenient to collect these parameters into a vector $\x \in \mathbb{R}^N$ called the latent field such that $\x \sim \mathcal{N}(0, \bm{Q}(\btheta_2)^{-1})$ where $\btheta_2 \in \mathbb{R}^{s_2}$ are further parameters, again with $s_2$ assumed small.
Let $\btheta = (\btheta_1, \btheta_2) \in \mathbb{R}^s$ with $m = s_1 + s_2$ be all hyperparameters, with prior $p(\btheta)$.

Extended latent Gaussian models [ELGMs; @stringer2022fast] relax the restriction that there is a one-to-one mapping between the mean response $\bmu$ and structured additive predictor $\bmeta$.
Instead, the structured additive predictor is redefined as $\bmeta = (\eta)_{i \in [N_n]}$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\bmeta$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$.
The inverse link function $g(\cdot)$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\bmeta_{\mathcal{J}_i})$.
Importantly, this mapping allows for the presence of non-linearity in the model.
Put together, ELGMs are then of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \bmeta_{\mathcal{J}_i}, \btheta_1), \quad i \in [n] \\
\mu_i &= \mathbb{E}(y_i \, | \, \bmeta_{\mathcal{J}_i}) = g_i(\bmeta_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j \in [N_n].
\end{align*}

## Naomi framed as an ELGM

Naomi has a lot in common with many LGMS: it is a spatio-temporal model with a large latent field, governed by a smaller number of hyperparameters.
However, Naomi is not an LGM, and instead falls into the ELGM class, for the following reasons:

1. In the household survey component, HIV incidence depends on district-level adult HIV prevalence and ART coverage, such that $\lambda \propto \rho (1 - \omega \cdot \alpha)$, where $\omega = 0.7$ is a fixed constant. This reflects basic HIV epidemiology: HIV incidence is proportional to unsuppresed viral load. As a result, $\log(\lambda_i)$ depends on 28 structured additive predictors (2 sexes $\times$ 7 age groups $\times$ 2 indicators, HIV prevalence and ART coverage).
2. In the household survey component, HIV incidence and HIV prevalence are linked to the proportion recently infected via Equation \ref{eq:kappa}.
3. In the ANC testing component, HIV prevalence and ART coverage depend upon the respective indicators in the household survey component. Although $\text{logit}(\rho_i)$ and $\text{logit}(\alpha_i)$ are Gaussian, nonetheless this introduces dependence of each mean response on two structured additive predictors.
4. Throughout the model components, processes are modelled at the finest distict-age-sex division, but likelihoods are defined for observations aggregated over sets of indices. As such, single observations are related to $|\mathcal{I}|$ structured additive predictors.
5. Individuals taking ART, or who have been recently infected, must be HIV positive. 
6. The ART attendance component uses a multinomial model with softmax link function which takes as input $|\{x': x' \sim x\}| + 1$ structured additive predictors.
7. Multiple link functions are used throughout the model, such that there is no one inverse link function $g$. Instead, 

If Naomi was written out in full mathematical detail (in the section above), it would be easier to make these reasons more concrete.
We should also specify how much of an impact each of these reasons is likely to have on the difficulty of inference.
Perhaps a table format could be good here.

# Fast approximate inference method\label{sec:fastinferencemethods}

The joint posterior of the parameters $(\x, \btheta)$ given data $\y$ for an ELGM is given by
\begin{equation*}
  p(\x, \btheta \, | \, \y)
  \propto p(\btheta) |\mathbf{Q}(\btheta)|^{n/2} \exp \left( - \frac{1}{2} \x^\top \mathbf{Q}(\btheta) \x + \sum_{i = 1}^n \log p(y_i \, | \, \x_{\mathcal{J}_i}, \btheta) \right).
\end{equation*}
We consider approximations to the posterior marginals of each latent random variable $x_i$ and hyperparameter $\theta_j$ given by
\begin{align}
  p(x_i \, | \, \y) &\approx \tilde p(x_i \, | \, \y) = \int \tilde p(x_i \, | \, \btheta, \y) \tilde p(\btheta \, | \, \y) \text{d}\btheta, \quad i \in [N], \label{eq:inla1} \\
  p(\theta_j \, | \, \y) &\approx \tilde p(\theta_j \, | \, \y) = \int \tilde  p(\btheta \, | \, \y) \text{d}\btheta_{-j} \quad j \in [m]. \label{eq:inla2}
\end{align}
Given the negative unnormalised log posterior $- \log p(\y, \x, \btheta)$, we obtain the above posterior marginal approximations $\{ \tilde p(x_i \, | \, \y) \}_{i = 1}^N$ and $\{\tilde p(\theta_j \, | \, \y)\}_{j = 1}^m$ via nested applications of the Laplace approximation and AGHQ.

## Laplace approximation

Let $\tilde p_\texttt{G}(\x \, | \, \btheta, \y) = \mathcal{N}(\x \, | \, \hat \x(\btheta), \hat{\Hb}(\btheta)^{-1})$ be a Gaussian approximation to $p(\x \, | \, \btheta, \y)$ with mode and precision matrix given by
\begin{align}
\hat \x(\btheta) &= \argmax_\x \log p(\y, \x, \btheta), \label{eq:mode} \\
\hat {\Hb}(\btheta) &= - \frac{\partial^2}{\partial \x \partial \x^\top} \log p(\y, \x, \btheta) \rvert_{\x = \hat \x(\btheta)}. \label{eq:precision}
\end{align}
Then the Laplace approximation to $p(\btheta, \y)$ is given by
\begin{equation}
\tilde p_\texttt{LA}(\btheta, \y)
= \frac{p(\y, \x, \btheta)}{\tilde p_\texttt{G}(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \hat \x(\btheta)}
= \sqrt{\frac{\det(\hat {\Hb}(\btheta))}{(2 \pi)^{N}}} p(\y, \hat \x(\btheta), \btheta). \label{eq:la}
\end{equation}
Inference in TMB proceeds by optimising Equation \ref{eq:la} using a gradient-based routine to obtain $\hat{\btheta}_\texttt{LA} = \argmax_{\btheta} \tilde p_\texttt{LA}(\btheta, \y)$.
Each evaluation in the optimisation requires an inner optimisation to obtain $\hat \x(\btheta)$ via Equation \ref{eq:mode}.
Latent field joint and marginal inferences then follow directly from the Gaussian approximation $\tilde p_\texttt{G}(\x \, | \, \hat{\btheta}_\texttt{LA}, \y)$.
Hyperparameter inferences are obtained according to some method which should be specified here as well.

## Gauss-Hermite quadrature

Quadrature rules can be used to approximate the integral of $\tilde p_\texttt{LA}(\btheta, \y)$ via the weighted sum
\begin{equation}
p(\y) \approx \int_{\btheta} \tilde p_\texttt{LA}(\btheta, \y) \text{d}\btheta \approx \sum_{\z \in \mathcal{Q}} p_\texttt{LA}(\z, \y) \omega(\z), \label{eq:quad}
\end{equation}
where $\z \in \mathcal{Q}$ are a set of nodes and $\omega: \mathcal{Q} \to \mathbb{R}$ is a weighting function.
Gauss-Hermite quadrature [GHQ; @davis1975methods] is one such quadrature rule, where in the univariate case the nodes $\mathcal{Q}(1, k) = \{z: H_k(z) = (-1)^k \exp(z^2 / 2) \frac{\text{d}}{\text{d}z^k} \exp(-z^2 / 2) = 0\}$ are selected as zeros of the $k$th Hermite polynomial.
The corresponding weights $\omega: \mathcal{Q}(1, k) \to \mathbb{R}$ are given by $\omega(z) = k! / [H_{k + 1}(z)]^2 \phi(z)$, where $\phi(\cdot)$ is a standard univariate Gaussian density.
GHQ is attractive because it is exact for functions which are a polynomial of total order no more than $2k - 1$ multiplied by a Gaussian density.
We typically expected posterior distributions to be relatively well described by this class of functions.
Multivariate GHQ rules, required to integrate over $\btheta$, are typically obtained using the product rule such that $\z = (z_1, \ldots, z_m) \in \mathcal{Q}(m, k) = \mathcal{Q}(1, k)^m$ and $\omega(\z) = \prod_{j = 1}^m \omega(z_j)$.

<!-- Tie together connection to Laplace for AGHQ k = 1 -->

## Adaptive quadrature

In adaptive Gauss-Hermite quadrature [AHGQ; @naylor1982applications; @tierney1986accurate] the nodes are shifted and rotated to suit the particular integrand.
Repositioning the nodes is especially important in statistical quadrature problems, where the integral depends on data $\y$ such that regions of high density are not known in advance.
To obtain an AGHQ estimate of Equation \ref{eq:quad}, let $\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA}) = - \partial^2 \log p_\texttt{LA}(\hat{\btheta}_\texttt{LA}, \y)$ be the curvature at the mode $\hat{\btheta}_\texttt{LA}$ and $[\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA})]^{-1} = \hat{\mathbf{P}}_\texttt{LA} {\hat{\mathbf{P}}_\texttt{LA}}^\top$ be a matrix decomposition of the inverse curvature, then
\begin{equation}
\tilde p_\texttt{AGHQ}(\y) = |\hat{\mathbf{P}}_\texttt{LA}|\sum_{\z \in \mathcal{Q}(m, k)} \tilde p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y) \omega(\z). \label{eq:aghq}
\end{equation}
That is, the unadapted nodes have been shifted by the mode and rotated by a decomposition of the inverse curvature such that $\z \mapsto \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}$.
Two alternatives for the decomposition are the Cholesky decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{L}}_\texttt{LA}$ and the spectral decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{E}}_\texttt{LA} \hat{\mathbf{\Lambda}}_\texttt{LA}^{1/2}$ [@jackel2005note].
Figure \ref{fig:aghq} demonstrates GHQ, as well as adaption for these two choices of matrix decomposition.
Equation \ref{eq:aghq} may then be used to normalise the Laplace approximation
\begin{equation*}
\tilde p_\texttt{LA}(\btheta \, | \, \y) = \frac{\tilde p_\texttt{LA}(\btheta, \y)}{\tilde p_\texttt{AGHQ}(\y)}.
\end{equation*}

To obtain inferences for the latent field (Equation \ref{eq:inla1}) the adapted nodes and weights are reused [@rue2009approximate; @stringer2022fast]
\begin{equation}
\tilde p(\x \, | \, \y) = |\hat{\mathbf{P}}_\texttt{LA}| \sum_{\z \in \mathcal{Q}(m, k)} \tilde p_\texttt{G}(\x \, | \, \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y) \tilde p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA} \, | \, \y) \omega(\z). \label{eq:nest}
\end{equation}
Samples from this mixture of Gaussians may be obtained by drawing a node $\z$ with multinomial probabilities $\lambda(\z) = |\hat{\mathbf{P}}_\texttt{LA}| p_\texttt{LA}(\hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA} \, | \, \y) \omega(\z)$, then drawing from the Gaussian $\tilde p_\texttt{G}(\x \, | \, \hat{\mathbf{P}}_\texttt{LA} \z + \hat{\btheta}_\texttt{LA}, \y)$.

```{r aghq, results='hide', fig.cap="The Gauss-Hermite quadrature nodes $\\z \\in \\mathcal{Q}(2, 3)$ for a two dimensional integral with three nodes per dimension. Adaption occurs based on the mean and covariance matrix of the target via the Cholesky decomposition or spectral decompostion of the inverse curvature at the mode. The integrand is $f(\\btheta) = \\text{sn}(0.3 \\theta_1, \\alpha = 2) \\cdot \\text{sn}(0.5 \\theta_1 - 0.3 \\theta_2, \\alpha = -2)$, where $\\text{sn}(\\cdot)$ is the standard skewnormal probability density function with shape parameter $\\alpha \\in \\mathbb{R}$."}
TMB::compile("2d.cpp")
dyn.load(TMB::dynlib("2d"))

obj <- TMB::MakeADFun(data = list(), parameters = list(theta1 = 0, theta2 = 0), DLL = "2d")

grid <- expand.grid(
  theta1 = seq(-8, 8, length.out = 400),
  theta2 = seq(-8, 8, length.out = 400)
)

ground_truth <- cbind(grid, pdf = apply(grid, 1, function(x) exp(-1 * obj$fn(x))))

opt <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr,
  control = list(iter.max = 1000, trace = 0)
)

sd_out <- TMB::sdreport(
  obj,
  par.fixed = opt$par,
  getJointPrecision = TRUE
)

mu <- opt$par
cov <- sd_out$cov.fixed

plot0 <- ggplot(ground_truth, aes(x = theta1, y = theta2, z = pdf)) +
  geom_contour(col = "lightgrey") +
  coord_fixed(xlim = c(-8, 8), ylim = c(-8, 8), ratio = 1) +
  labs(x = "", y = "") +
  theme_minimal() +
  guides(size = FALSE) +
  theme(
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    axis.text.y = element_blank(), axis.ticks.y = element_blank()
  )

gg <- mvQuad::createNIGrid(2, "GHe", 3)

add_points <- function(plot0, gg) {
  
  points <- mvQuad::getNodes(gg) %>%
    as.data.frame() %>%
    mutate(weights = mvQuad::getWeights(gg))
  
  colnames(points) <- c("theta1", "theta2", "weights")
  
  plot0 +
    geom_point(
      data = points,
      aes(x = theta1, y = theta2, size = weights),
      alpha = 0.8,
      col = "#009E73",
      inherit.aes = FALSE
    ) +
    scale_size_continuous(range = c(1, 2))
}

plot1 <- add_points(plot0, gg) +
  labs(size = "")

#' Adapt by the mean
gg2 <- gg
mvQuad::rescale(gg2, m = mu, C = diag(c(1, 1)), dec.type = 1)

plot2 <- add_points(plot0, gg2) +
  labs(size = "")

#' Adapt by the lower Cholesky
gg3 <- gg
mvQuad::rescale(gg3, m = mu, C = cov, dec.type = 2)

plot3 <- add_points(plot0, gg3) +
  labs(size = "")

#' Adapt by the spectral
gg4 <- gg
mvQuad::rescale(gg4, m = mu, C = cov, dec.type = 1)

plot4 <- add_points(plot0, gg4) +
  labs(size = "")

#' PCA-AGHQ
gg5 <- mvQuad::createNIGrid(2, "GHe", level = c(3, 1))
mvQuad::rescale(gg5, m = mu, C = cov, dec.type = 1)

plot5 <- add_points(plot0, gg5) +
  labs(size = "")

(plot1 + plot2) / (plot4 + plot5)
```

## Principal components analysis

Use of the product rule requires $|\mathcal{Q}(m, k)| = k^m$ quadrature points.
This quickly becomes intractable as $m$ increases for $k > 1$.
An alternative is to let $\bk = (k_1, \ldots, k_m)$ be a vector of levels for each dimension of $\btheta$.
We may then define $\mathcal{Q}(m, \bk) = \mathcal{Q}(1, k_1) \times \cdots \times \mathcal{Q}(1, k_m)$ of size $|\mathcal{Q}(m, \bk)| = \prod_{j = 1}^m k_j$.
Let $\mathcal{Q}(m, s, k)$ correspond to $\mathcal{Q}(m, \bk)$ with choice of levels $k_j = k, j \leq s$ and $k_j = 1, j > s$ for some $s \leq m$.
Taken together with use of the spectral decomposition, this choice of levels is analogous to a principal components analysis (PCA) approach to AGHQ
\begin{equation}
\tilde p_\texttt{PCA}(\y) = |\hat{\mathbf{E}}_{\texttt{LA}, s} \hat{\mathbf{\Lambda}}_{\texttt{LA}, s}^{1/2}|\sum_{\z \in \mathcal{Q}(m, s, k)} \tilde p_\texttt{LA}(\hat{\mathbf{E}}_{\texttt{LA}, s} \hat{\mathbf{\Lambda}}_{\texttt{LA}, s}^{1/2} \z + \hat{\btheta}_\texttt{LA}, \y) \omega(\z),
\end{equation}
where $\hat{\mathbf{E}}_{\texttt{LA}, s} = \cdots$ and $\hat{\mathbf{\Lambda}}_{\texttt{LA}, s} = \cdots$ are terms which should be described.
The final panel of Figure \ref{fig:aghq} illustrates a case where $m = 2$ and $s = 1$. 
We refer to use of this quadrature rule as PCA-AGHQ.

## Sparse rules

Though there are sparse rules which retain the attractive exactness properties of GHQ, using fewer than $k^m$ quadrature points, they use weighting functions which may be negative $\omega(\z) < 0$.
This introduces a problem when trying to produce inferences about the latent field, as the resulting $\lambda(\z)$ may be negative.
We are not aware of any suitable approaches to obtain multinomial samples in such cases.
It is however, still possible to calculate the normalising constant using a sparse rule, which we do so in Appendix S3, finding it to be related to that from PCA-AGHQ as follows.

# Application to data from Malawi\label{sec:results}

We fit the simplified Naomi model (Section \ref{sec:naomi}) to data from Malawi using three inferential approaches.
The three approaches were:
1. TMB,
2. PCA-AGHQ, and
3. NUTS: the Hamiltonian Monte Carlo (HMC) algorithm No-U-Turn Sampling (NUTS) using Stan [@carpenter2017stan] via the `tmbstan` package [@monnahan2018no].
The `TMB` C++ user-template used to specify the log-posterior was the same for each.
The dimension of the latent field was $N = 467$ and the dimension of the hyperparameters was $m = 24$.
Settings used for each inferential method are provided in Table \ref{tab:inference-methods}, and, where relevant, discussed further below.
For the deterministic methods, following inference we simulated hyperparameter and latent field samples.
For all methods, we simulated age-sex-district specific HIV prevalence, ART coverage and HIV incidence from the latent field and hyperparameter posteriors.
<!-- Note: eventually this figure should be generated using the algorithm proposed in the paper -->
Example model outputs from TMB are illustrated in Figure \ref{fig:naomi-results}.
The \textsc{R} [@r] code used to produce all results we describe below is available at `github.com/athowes/elgm-inf`.
We used `orderly` [@orderly] for reproducible research, `ggplot2` for data visualisation [@wickham2016ggplot2] and `rticles` [@allaire2022rticles] for reporting via `rmarkdown` [@allaire2022rmarkdown].

```{r naomi-results, fig.cap="District-level model outputs for adults aged 15-49. Inference conducted with TMB."}
knitr::include_graphics("depends/naomi_results.png")
```

\begin{table}[]
\small
\begin{tabularx}{\textwidth}{p{0.15\linewidth}p{0.15\linewidth}p{0.6\linewidth}}
\toprule
Name & Software & Details \\
\midrule
TMB & \texttt{TMB} & $1000$ samples \\
PCA-AGHQ & \texttt{aghq} & $k = 3, s = 8$ (see Section \ref{sec:pca-settings}), $1000$ samples \\
NUTS & \texttt{tmbstan} & $4$ chains of $20000$ iterations, with the first $10000$ iterations of each chain discarded as warmup, thinned by a factor of $20$. Default NUTS tuning parameters \citep{hoffman2014no}. \\
\bottomrule
\end{tabularx}
\caption{A summary of settings used for each inferential method.}
\label{tab:inference-methods}
\small
\end{table}

## NUTS convergence

To obtain satisfactory NUTS results we increased the Markov chain lengths until all diagnostics were acceptable.
The results were chains of length 20,000, thinned by a factor of 20 for ease-of-storage.
All potential scale reduction factors [@gelman1992inference; @vehtari2021rank] were then $\hat R < 1.05$.
For full details see Appendix S2.
Though inaccuracies remain possible, we considered the NUTS results to be a gold-standard.

## PCA-AGHQ settings \label{sec:pca-settings}

We used a Scree plot (Figure S$X$) based on the spectral decomposition of $\hat{\Hb}_\texttt{LA}(\hat{\btheta}_\texttt{LA})$ to select the number of principal components $s = 8 < m = 24$ to keep, sufficient to explain close to 90% of total variation.
This resulted in a close reconstruction of the Hessian using the reduced rank approximation (Figure S$X$).

```{r node-positions, fig.cap="PCA-AGHQ node positions (green, rug plot) overlaid onto the hyperparameter marginal posteriors (grey, histogram) for each of the 24 hyperparameters."}
knitr::include_graphics("depends/nodes-samples-comparison.png")

# This plot could be improved with a scale for "coverage"
```

### Visual inspection
 
Overlaying the generated PCA-AGHQ nodes onto the hyperparameter marginal posteriors obtained using NUTS, we found approximately 12 of the 24 hyperparameters had marginals which were well covered (Figure \ref{fig:node-positions}).
Though $12 > s = 8$ there remain many hyperparameters poorly covered.
Coverage was associated with marginal standard deviation (Figure S$X$), which varied particularly according to the hyperparameter scale.
All constrained hyperparameters $\theta$ were transformed to the real line, using either a log ($\theta > 0$) or logit ($\theta \in [0, 1]$) transformation.
As a result, marginal standard deviations for log transformed hyperparameters were systematically smaller than those which were logit transformed.

### Normalising constant assessment

We assessed appropriateness of the grid by comparing the estimate of $\log p_\texttt{PCA}(\y)$ for a range of settings.
Convergence in $\log p_\texttt{PCA}(\y)$ as $s$ and $k$ are increased may suggest a suitable grid has been reached.
Appendix S3 shows those values which we could compute in a reasonable time (less than 24 hours using a high performance computing cluster).

## Model assessment \label{sec:model-assessment}

### Posterior contraction

Let $\phi$ be a generic model parameter.
To assess the informativeness of the data we compared the prior variance $\sigma_\text{prior}^2(\phi)$ to the posterior variance $\sigma_\text{posterior}^2(\phi)$ via the posterior contraction $c(\phi) = 1 - (\sigma_\text{posterior}^2(\phi) / \sigma_\text{prior}^2(\phi))$ [@schad2021toward].
We found that (Figure \ref{fig:contraction})) something something.
For greater interpretability, facet parameters in this plot according to model component.

```{r contraction, fig.cap="A posterior contraction tending toward 1.0 corresponds to a posterior tending towards a Dirac delta function. A posterior contraction of 0.0 corresponds to no change in standard deviation between prior and posterior. A posterior contraction less than 0.0 corresponds to a wider posterior than prior."}
knitr::include_graphics("depends/posterior-contraction.png")
```

### Coverage

We assessed the coverage of our estimates via the uniformity of the data within each posterior marginal distribution.
Let $\{\phi_i\}_{i = 1}^n$ be posterior marginal samples.

## Inference comparison \label{sec:inf-comparison}

To compare the accuracy of posterior distributions produced by TMB and PCA-AGHQ as compared with those from NUTS we assessed (1) marginal point estimates, (2) marginal Kolmogorov-Smirnov and Anderson-Darling tests using the empirical cumulative distribution function (ECDF), (3) joint Pareto-smoothed importance sampling results, and (4) joint maximum mean discrepancy results. 

### Point estimates

```{r}
df_point <- read_csv("depends/mean-sd.csv")

df_point_pct <- df_point %>%
  ungroup() %>%
  group_by(indicator) %>%
  summarise(
    rmse_diff = 100 * diff(rmse) / max(rmse),
    mae_diff = 100 * diff(mae) / max(mae)
  )

rmse_ahgq_mean <- filter(df_point, method == "PCA-AGHQ", indicator == "Posterior mean estimate") %>% pull(rmse) %>% round(2)
rmse_tmb_mean <- filter(df_point, method == "TMB", indicator == "Posterior mean estimate") %>% pull(rmse) %>% round(2)
rmse_diff_mean <- filter(df_point_pct, indicator == "Posterior mean estimate") %>% pull(rmse_diff) %>% round(0)

rmse_ahgq_sd <- filter(df_point, method == "PCA-AGHQ", indicator == "Posterior SD estimate") %>% pull(rmse) %>% round(2)
rmse_tmb_sd <- filter(df_point, method == "TMB", indicator == "Posterior SD estimate") %>% pull(rmse) %>% round(2)
rmse_diff_sd <- filter(df_point_pct, indicator == "Posterior SD estimate") %>% pull(rmse_diff) %>% round(0)
```

The root mean square error (RMSE) between posterior mean estimates from PCA-AGHQ and NUTS (`r rmse_ahgq_mean`) was `r abs(rmse_diff_mean)`% lower than that between TMB and NUTS (`r rmse_tmb_mean`).
For the posterior standard deviation estimates, there was a substantial `r abs(rmse_diff_sd)`% reduction in RMSE: from `r abs(rmse_tmb_sd)` (TMB) to `r abs(rmse_ahgq_sd)` (PCA-AGHQ).
These results, alongside those for the mean absolute error (MAE), are presented in Figure \ref{fig:mean-sd}.

```{r mean-sd, fig.cap="TMT."}
knitr::include_graphics("depends/mean-sd-comparison.png")
```

### Distribution tests

The two-sample Kolmogorov-Smirnov (KS) test statistic [@smirnov1948table] is the maximum absolute difference between two ECDFs $F(\varphi) = \frac{1}{n} \sum_{i = 1}^n \mathbb{I}_{\phi_i \leq \varphi}$.
See an illustration of the KS test in Figure \ref{fig:ks} and a summary of the results in Table.

```{r ks, fig.height = 4, fig.cap="Example KS test for one parameter."}
df_compare <- readRDS("depends/beta_alpha.rds") %>%
  filter(method != "adam")

mean <- df_compare %>%
  filter(method == "tmbstan") %>%
  summarise(mean = mean(samples)) %>%
  pull(mean) %>%
  round(digits = 3)

sd <- df_compare %>%
  filter(method == "tmbstan") %>%
  summarise(sd = sd(samples)) %>%
  pull(sd) %>%
  round(digits = 3)

histogram <- df_compare %>%
  mutate(method = fct_recode(method, "PCA-AGHQ" = "aghq", "NUTS"= "tmbstan")) %>%
  ggplot(aes(x = samples, fill = method, col = method)) +
  geom_histogram(aes(y = after_stat(density)), alpha = 0.5, position = "identity", bins = 30) +
  theme_minimal() +
  facet_grid(method~.) +
  labs(x = "beta_alpha", y = "Density", fill = "Method") +
  scale_color_manual(values = colours) +
  scale_fill_manual(values = colours) +
  theme(legend.position = "none")

grid <- seq(from = min(df_compare$samples), to = max(df_compare$samples), length.out = 1000)

tmb_ecdf <- stats::ecdf(filter(df_compare, method == "TMB") %>% pull(samples))
tmb_ecdf_df <- data.frame(x = grid, ecdf = tmb_ecdf(grid), method = "TMB")

aghq_ecdf <- stats::ecdf(filter(df_compare, method == "aghq") %>% pull(samples))
aghq_ecdf_df <- data.frame(x = grid, ecdf = aghq_ecdf(grid), method = "aghq")

tmbstan_ecdf <- stats::ecdf(filter(df_compare, method == "tmbstan") %>% pull(samples))
tmbstan_ecdf_df <- data.frame(x = grid, ecdf = tmbstan_ecdf(grid), method = "tmbstan")

# Add ECDF differences
tmb_ecdf_df$ecdf_diff <- tmbstan_ecdf_df$ecdf - tmb_ecdf_df$ecdf
aghq_ecdf_df$ecdf_diff <- tmbstan_ecdf_df$ecdf - aghq_ecdf_df$ecdf
tmbstan_ecdf_df$ecdf_diff <- 0

absmax <- function(x) x[which.max(abs(x))]

ks_tmb <- absmax(tmb_ecdf_df$ecdf_diff)
ks_aghq <- absmax(aghq_ecdf_df$ecdf_diff)

ecdf_df <- bind_rows(tmb_ecdf_df, aghq_ecdf_df, tmbstan_ecdf_df)

ecdf_df$method <- factor(ecdf_df$method, levels = c("TMB", "aghq", "tmbstan"))

ks_labeller <- function(x) toString(round(abs(x), 2))

ecdf_diff <- ggplot(ecdf_df, aes(x = x, y = ecdf_diff, col = method)) +
  geom_line() +
  geom_abline(intercept = ks_tmb, slope = 0, col = colours[1], linetype = "dashed", alpha = 0.8) +
  annotate("text", x = 1.1 * max(ecdf_df$x), y = ks_tmb, label = ks_labeller(ks_tmb), col = colours[1], alpha = 0.8) +
  geom_abline(intercept = ks_aghq, slope = 0, col = colours[2], linetype = "dashed", alpha = 0.8) +
  annotate("text", x = 1.1 * max(ecdf_df$x), y = ks_aghq, label = ks_labeller(ks_aghq), col = colours[2], alpha = 0.8) +
  scale_color_manual(values = colours) +
  labs(x = "beta_alpha", y = "ECDF difference") +
  guides(col = "none") +
  coord_cartesian(xlim = c(min(ecdf_df$x), max(ecdf_df$x)), clip = "off") +
  theme_minimal() +
  theme(plot.margin = unit(c(1, 3, 1, 1), "lines"))

histogram + ecdf_diff
```

### Pareto-smoothed importance sampling

Let $\{\bphi_i\}_{i = 1}^n$ be joint posterior samples.
Pareto-smoothed importance sampling [PSIS; @vehtari2015pareto, @yao2018yes] is a method for stabilising the ratios used in importance sampling.
Results for the PSIS analysis are pending.

### Maximum mean discrepancy

Let $\Phi = \{\bphi_i\}_{i = 1}^n$ and $\Psi = \{\bpsi_i\}_{i = 1}^n$ be two sets of joint posterior samples, and $k$ be a kernel.
The maximum mean discrepancy [MMD; @gretton2006kernel] can be empirically estimated by
\begin{equation*}
\text{MMD}(\Phi, \Psi) = \sqrt{\frac{1}{n^2} \sum_{i, j = 1}^n k(\bphi_i, \bphi_j) - \frac{2}{n^2} \sum_{i, j = 1}^n k(\bphi_i, \bpsi_j) + \frac{1}{n^2} \sum_{i, j = 1}^n k(\bpsi_i, \bpsi_j)}.
\end{equation*}
We set $k(\bphi_i, \bphi_j) = \exp(-\sigma \lVert \bphi_i - \bphi_j \rVert^2)$ with $\sigma$ estimated from data using the `kernlab` \textsc{R} package [@karatzoglou2019package].
We the MMD from PCA-AGHQ (0.071) was 11% smaller than that of TMB (0.080) as compared with NUTS.

## Case study on exceedance probabilites \label{sec:exceedance}

### Meeting the second 90

Ambitious fast-track targets for scaling up ART treatment have been developed by UNAIDS, with the goal of "ending the AIDS epidemic by 2030".
Specifically, meeting the "90-90-90" fast-track target requires that 90% of people living with HIV know their status, 90% of those are on ART, and 90% of those have suppressed viral load.
Naomi can be used to identify treatment gaps by calculating the probability that the second 90 target has been met $\mathbb{P}(\alpha_i > 0.9^2 = 0.81)$ for each strata $i$.
We found that for women both TMB and PCA-AGHQ underestimate these exceedance probabilities (Figure \ref{fig:exceedance}, first row).
We hypothesise that the sex related discrepancy occurs due to interactions between the household survey and ANC components of the model creating a more challenging posterior geometry.

### Finding strata with high incidence

Some HIV interventions are cost-effective only within high HIV incidence settings, typically defined as greater than 1% incidence per year.
Naomi can be used to assess the probability of a strata having high incidence by evaluating $\mathbb{P}(\lambda_i > 0.01)$.
We found that both TMB and PCA-AGHQ overestimate these exceedance probabilities (Figure \ref{fig:exceedance}, second row).
This is surprising, in that we expect inferences from NUTS to be more heavy-tailed than those from TMB or PCA-AGHQ.

```{r exceedance, fig.cap="Both TMB and PCA-AGHQ are meaningfully inaccurate as compared with NUTS for estimating exceedance probabilities. For the second 90 target (top row) the inaccuracy varies substantially by sex."}
knitr::include_graphics("depends/exceedance.png")
```

# Discussion\label{sec:conclusions}

We developed an approximate Bayesian inference algorithm, combining AGHQ with PCA, motivated by a challenging problem in small-area estimation of HIV.
For the simplified Naomi model in Malawi (Section \ref{sec:results}) we demonstrated the method to be more accurate, across a broad range of metrics, than TMB, and substantially faster than NUTS.
PCA-AGHQ could be added to the Naomi web interface as an alternative to TMB, enabling analysts to quickly iterate over model options using a fast inference approach, before switching to a more accurate approach once they are happy with the results.

We found that posterior exceedance probabilities (Section \ref{sec:exceedance}) from the approximate methods to be systematically inaccurate, with the potential to meaningfully mislead policy.

PCA-AGHQ was implemented using the `TMB` and `aghq` \textsc{R} packages.

We hope that our work further encourages use of deterministic inference algorithms for ELGMs in applied settings, as well as methodological exploration of their accuracy and limitations.
Among the ELGM-type structures of particular interest in spatial epidemiology are aggregated Gaussian process models [@nandi2020disaggregation] and evidence synthesis models [@amoah2020geostatistical].

## Future directions

### Improving the quadrature grid

Part of our motivation for PCA-AGHQ was to place more quadrature nodes in those dimensions which are in-some-sense more important.
While PCA is a sensible way to approach this goal, there are avenues where it overlooks potential benefits, or does not behave as one might hope.
The first challenge we identified was using PCA when the dimensions have different scales. 
Specifically, we found logit-scale hyperparamters to be systematically favoured over those on the log-scale.
Second, the amount of variation explained for the Hessian matrix is not of directly interest, rather the effect of the different dimensions on the relevant outputs.
Using measures of importance from sensitivity analysis, such as Shapley values [@shapley1953value] may be preferable.
Third, it is more important to allocate quadrature nodes to those marginals which are non-Gaussian.
This is because the Laplace approximation is exact when the integrand is Gaussian, so a single quadrature node is sufficiently.
The difficulty is, of course, knowing in advance which marginals will be non-Gaussian.
This could be done if there were a cheap way to obtain posterior means, which could then be compared to posterior modes obtained using optimisation.
Another approach would be to measure the fit of marginal samples from a cheap approximation, like TMB.
The main challenge is that the measurements have to for marginals, ruling out approaches like PSIS which operate on joint distributions [@yao2018yes].

### Computational speed-ups

Integration over a moderate number of hyperparameters posed a challenge, and led us to use a quadrature grids with a large number of nodes.
However, computation at each node is independent, such that the run-time of the algorithm could potentially be significantly improved by parallel computing.
Further computational speed-ups might be obtained using graphics processing units (GPUs) speciallised for the relevant matrix operations.

### Comparison to other MCMC algorithms

Blocked Gibbs sampling [@geman1984stochastic] or slice sampling [@neal2003slice], may be better suited than NUTS to sampling from Naomi.
These algorithms are available, and customisable, including e.g. choice of block structure within the `NIMBLE` probabilistic programming language [@de2017programming].

### Implementation into probabilistic programming languages

Though gaining in popularity, the user-base of `TMB` remains relatively small.
Furthermore, for users unfamiliar with C++, it can be challenging to use.
As such, it could be beneficial to implement AGHQ within other probabilistic programming languages.
Implementation in `NIMBLE` could be relatively straightforward, as it (for version >1.0.0) includes functionality for automatic differentiation and Laplace approximation, built using `CppAD` like `TMB`.
Similarly, implementation in Stan could be possible by use of the `bridgestan` package [@bridgestan] together with the adjoint-differentiated Laplace approximation of @margossian2020hamiltonian.
<!-- Note: can fit hyperparameters with Laplace on latent field using HMC via tmbstan::tmbstan(laplace = TRUE) -->
<!-- What Charles Margossian has implemented for Stan is done similarly by default in TMB -->

### Statistical theory

@stringer2022fast (Theorem 1) bound the total variation error of AGHQ, establishing convergence in probability of coverage probabilities under the approximate posterior to those under the true posterior.
It's possible that similar theory could be established for PCA-AGHQ, or more generally AGHQ with varying numbers of nodes per dimension.

### Laplace marginals

See Appendix S4.

# Acknowledgements {-}

AH was supported by the EPSRC Centre for Doctoral Training in Modern Statistics and Statistical Machine Learning (EP/S023151/1), and conducted part of this research while an International Visiting Graduate Student at the University of Waterloo.
AH and JWE were supported by the Bill and Melinda Gates Foundation (OPP1190661, OPP1164897).
SRF was supported by the EPSRC (EP/V002910/2).
JWE was supported by UNAIDS and National Institute of Allergy and Infectious Disease of the National Institutes of Health (R01AI136664).
This research was supported by the MRC Centre for Global Infectious Disease Analysis (MR/R015600/1), jointly funded by the UK Medical Research Council (MRC) and the UK Foreign, Commonwealth \& Development Office (FCDO), under the MRC/FCDO Concordat program and is also part of the EDCTP2 programme supported by the European Union.
