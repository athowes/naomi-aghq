---
title: Simplified integrated nested Laplace approximation for extended latent Gaussian models
author:
  - name: Adam 
    surname: Howes
    email: ath19@ic.ac.uk
    label: e1
    addressLabel: A
    sepNext: "," # use "," or \and; the last author not need this field
  - name: Alex
    surname: Stringer
    email: alex.stringer@uwaterloo.ca
    label: e2
    addressLabel: B
affiliation:
  - label: A
    name: Department of Mathematics, Imperial College London
    authorsLabels: e1
  - label: B
    name: Department of Statistics and Actuarial Science, University of Waterloo
    authorsLabels: e2
abstract: |
  | Naomi is a spatial evidence synthesis model used by countries in sub-Saharan Africa to produce HIV epidemic indicators. We combine the simplified integrated nested Laplace approximation approach of @wood2020simplified with adaptive Gaussian hermite quadrature to enable fast and accurate inference for Naomi and other extended latent Gaussian models. Using data from Malawi, we compare our inference method to other approaches. We provide an easy to use implementation as a part of the `aghq` \textsf{R} package allowing flexible use of the INLA method for a broader class of models, including any with a `TMB` template.
keyword-subclass: | 
 \begin{keyword}[class=MSC2020] % It must be define for aap, aop, aos journals. For aoas, sts is not used
 \kwd[Primary ]{00X00}
 \kwd{00X00}
 \kwd[; secondary ]{00X00}
 \end{keyword}
keywords: 
  - Spatial statistics
  - INLA

predefined-theoremstyle: true # use in section Environments for Axiom, Theorem, etc
bibliography: citations.bib
biblio-style: imsart-nameyear # alternative: imsart-number
output:
  rticles::ims_article:
    journal: aoas # aap, aoas, aop, aos, sts. See documentation
    toc: false # Please use for articles with 50 pages and more
    includes:
      in_header: preamble.tex
---

# Introduction

We are motivated by a challenging inference problem in HIV surveillance.
Mounting an effective public health response to the epidemic requires accurate, timely and sufficiently fine-scale estimates of HIV indicators.
No single data source is sufficient to base these estimates on.
For example, although nationally-representative household surveys are the most reliable data source, in most countries they only occur infrequently.
As such, the Naomi small-area estimation model [@eaton2021naomi] synthesises data from multiple sources to estimate HIV prevalence, HIV incidence, and coverage of antiretroviral treatment (ART) at a district-level.
Software (\url{https://naomi.unaids.org}) has been developed for Naomi, which allows countries to input their data and generate estimates in a yearly process supported by UNAIDS.
For this reason, any inference method used must be fast enough to run in production by country teams, ruling out prohibitively slow Markov chain Monte Carlo (MCMC) approaches.
Inference is currently conducted using the Template Model Builder (`TMB`) \textsf{R} package [@kristensen2016tmb].
`TMB` implements an empirical Bayes approach, and is gaining popularity in spatial statistics for its speed and flexibility [@osgoodzimmerman2021statistical].

Using simulation we found inferences from `TMB` for the Naomi model to sometimes be inaccurate.
The `aghq` \textsf{R} package [@stringer2021implementing] for adaptive Gaussian Hermite quadrature (AGHQ), significantly improved performance.
We develop a new inference methodology combining the simplified integrated nested Laplace approximation (INLA) approach of @wood2020simplified with AGHQ.

<!-- Furthermore, as Naomi falls into the class of extended latent Gaussian models (ELGMs) [@stringer2022fast] it cannot be fit using the integrated nested Laplace approximation (INLA)  method [@rue2009approximate] as implemented by the `R-INLA` \textsf{R} package. -->

# The Naomi model

@eaton2019joint specify a joint model linking small-area estimation models of HIV prevalence from household surveys, HIV prevalence from antenatal care clinics, and antiretroviral therapy (ART) coverage from routine health data collection.
This model forms the basis of the Naomi small-area estimation model, described fully in @eaton2021naomi. 
Modelling data from multiple sources concurrently is attractive as it increases statistical power, mitigates the biases of any single source, and prompts investigation into any data conflicts.
The model is comprised of three components, described as follows.

## Household survey component

Consider a country partitioned into $n$ areas indexed by $i$.
Suppose a simple random household survey of $m^\text{HS}_i$ people is conducted in each area, and $y^\text{HS}_i$ HIV positive cases are observed.
Cases may be modelled using a binomial logistic regression model
\begin{align}
y^\text{HS}_i &\sim \text{Bin}(m^\text{HS}_i, \rho^\text{HS}_i), \\
\text{logit}(\rho^\text{HS}_i) &\sim \mathcal{N}(\beta_\phi, \sigma_\phi^2),
\end{align}
where HIV prevalence $\rho^\text{HS}_i$ is modelled by a Gaussian with mean $\beta_\phi$ and standard deviation $\sigma_\phi$.

## ANC component

Routinely collected data from pregnant women attending antenatal care clinics (ANCs) is another important source of information about the HIV epidemic.
Suppose that of $m^\text{ANC}_i$ women attending ANC, $y^\text{ANC}_i$ are HIV positive.
Then an analogous binomial logistic regression model
\begin{align}
y^\text{ANC}_i &\sim \text{Bin}(m^\text{ANC}_i, \rho^\text{ANC}_i), \\
\text{logit}(\rho^\text{ANC}_i) &= \text{logit}(\rho^\text{HS}_i) + b_i, \\
b_i &\sim \mathcal{N}(\beta_b, \sigma_b^2),
\end{align}
may be used to describe HIV prevalence amongst the sub-population of women attending ANCs.
Reflecting the fact that prevalence in ANCs is related but importantly different to prevalence in the general population, bias terms $b_i$ are used to offset ANC prevalence from HIV prevalence on the logit scale.

## ART component

The number of people receiving treatment at district health facilities $A_i$ provides further information about HIV prevalence.
Districts with high prevalence are likely to have a greater number of people receiving treatment, and vice versa.
ART coverage, defined to be the proportion of people living with HIV (PLHIV) currently on ART on district $i$, is given by $\alpha_i = A_i / \rho^\text{HS}_i N_i$, where $N_i$ is the total population of district $i$ and assumed to be constant.
As such, ART coverage may also be modelled using a binomial logistic regression model
\begin{align}
A_i &\sim \text{Bin}(N_i, \rho^\text{HS}_i \alpha_i), \\
\text{logit}(\alpha_i) &\sim \mathcal{N}(\beta_\alpha, \sigma_\alpha^2),
\end{align}
where the proportion of people receiving ART is $\rho^\text{HS}_i \alpha_i$.
Here we assume no travel between districts to receive treatment.

## Joint model

Let $\y = (\y^\text{HS}, \y^\text{ANC}, \bm{A})$ be the complete response vector.
In this section, we would like to set-up notation for the joint model, helping to show that it isn't a latent Gaussian model, but is an extended latent Gaussian model.
When introducing LGM and ELGM below, we may refer back to this model as example.
For example, we can note that each of the components individually might be latent Gaussian (perhaps for the ART component it depends if you consider $\rho^\text{HS}_i$ to be fixed) but when combined they are no longer.

# Fast inference methods

## Integrated nested Laplace approximation\label{sec:inla}

INLA [@rue2009approximate] is an approximate Bayesian inference method based on nested Gaussian approximations and numerical integration, and designed for use with latent Gaussian models (LGMs) of the form
\begin{alignat}{2}
&\text{(Observations)}     &        y_i &\sim p(y_i \, | \, x_i, \btheta), \quad i = 1, \ldots, n, \label{eq:data} \\
&\text{(Latent field)}     &        \x &\sim \mathcal{N}(\x \, | \, \mathbf{0}, \mathbf{Q}(\btheta)^{-1}), \label{eq:process} \\
&\text{(Parameters)}       & \qquad \btheta &\sim p(\btheta), \label{eq:parameters}
\end{alignat}
where $\text{dim}(\y) = \text{dim}(\x) = n$ and $\text{dim}(\btheta) = m$, and $m < n$.
For such models, the joint posterior of $(\x, \btheta)$ is given by
\begin{equation}
  p(\x, \btheta \, | \, \y) 
  \propto p(\btheta) |\mathbf{Q}(\btheta)|^{n/2} \exp \left( - \frac{1}{2} \x^\top \mathbf{Q}(\btheta) \x + \sum_{i = 1}^n \log p(y_i \, | \, x_i, \btheta) \right). \label{eq:lgmjointposterior}
\end{equation}
Rather than approximating the above full posterior, the INLA method instead approximates the posterior marginals of each latent random variable $x_i$ and parameter $\theta_j$ given by
\begin{align}
  p(x_i \, | \, \y) &= \int p(x_i, \btheta \, | \, \y) \text{d} \btheta = \int p(x_i \, | \, \btheta, \y) p(\btheta \, | \, \y) \text{d}\btheta, \quad i = 1, \dots, n, \label{eq:inla1} \\
  p(\theta_j \, | \, \y) &= \int p(\btheta \, | \, \y) \text{d}\btheta_{-j} \quad j = 1, \ldots, m. \label{eq:inla2}
\end{align}
An approximation is made to each of the two quantities, $p(\btheta \, | \, \y)$ and $p(x_i \, | \, \btheta, \y)$, nested inside the above integrals: (i) $p(\btheta \, | \, \y) \approx \tilde p(\btheta \, | \, \y)$ and (ii) $p(x_i \, | \, \btheta, \y) \approx \tilde p(x_i \, | \, \btheta, \y)$.
We discuss these two approximations in turn below.

### Approximation (i) \label{sec:approxi}

The posterior marginal of the parameters $p(\btheta \, | \, \y)$ appears in both Equations \eqref{eq:inla1} and \eqref{eq:inla2}.
This distribution is approximated by $\tilde p(\btheta \, | \, \y)$ and represented by a set of $K$ integration points $\{ \btheta^{(k)} \}$ and area-weights $\{ \Delta^{(k)} \}$.
The first step is to rewrite $p(\btheta \, | \, \y)$ as
\begin{equation}
  p(\btheta \, | \, \y) 
  = \frac{p(\x, \btheta \, | \, \y)}{p(\x \, | \, \btheta, \y)}
  \propto \frac{p(\y, \x, \btheta)}{p(\x \, | \, \btheta, \y)}. \label{eq:hyperpost}
\end{equation}
Approximation (i) then uses a Gaussian approximation to the denominator given by
\begin{equation}
p(\x \, | \, \btheta, \y) \approx p_G(\x \, | \, \btheta, \y) \triangleq \mathcal{N}(\x \, | \, \hat{\bm{\mu}}(\btheta), \hat{\mathbf{Q}}(\btheta)^{-1}). \label{eq:gaussianx}
\end{equation}
This approximation is accurate as the Gaussian prior on the latent field $\x$ makes the posterior distribution, given by
\begin{equation}
p(\x \, | \, \btheta, \y) \propto
\exp \left( - \frac{1}{2} \x^\top \mathbf{Q}(\btheta) \x + \sum_{i = 1}^n \log p(y_i \, | \, x_i, \btheta) \right),
\end{equation}
close to being Gaussian because $\y$ tends not to be strongly informative and the observation distribution $p(\y \, | \, \x, \btheta)$ is usually well-behaved [@blangiardo2015spatial].
<!-- If $\y$ is uninformative about $\x$ then how can we hope for useful inference? Perhaps what the author (Marta) meant is that the Gaussian distribution is sufficiently flexible to accommodate the updates made by $\y$. -->
As $p(\btheta \, | \, \y)$ does not depend on $\x$, any value may be chosen to evaluate the right hand side of Equation \ref{eq:hyperpost}.
Taking $\x = \hat{\bm{\mu}}(\btheta)$, the value where the Gaussian approximation is most accurate, gives the final approximation as
\begin{equation}
\tilde p(\btheta \, | \, \y) \propto \frac{p(\y, \x, \btheta)}{p_G(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \hat{\bm{\mu}}(\btheta)} = \frac{p(\y, \hat{\bm{\mu}}(\btheta), \btheta)}{\det(\hat{\bm{Q}}(\btheta))^{1/2}}, \label{eq:hypermarginal}
\end{equation}
where the equality is because $p_G(\x \, | \, \btheta, \y)$ is evaluated at its mode $\hat{\bm{\mu}}(\btheta)$.

### Approximation (ii)

We now move on to the approximation $p(x_i \, | \, \btheta, \y) \approx \tilde p(x_i \, | \, \btheta, \y)$.
Having used used the Gaussian approximation $p(\x \, | \, \btheta, \y) \approx p_G(\x \, | \, \btheta, \y)$ in Section \ref{sec:approxi} above, a natural approach, and that taken by \citet{rue2007approximate}, is to marginalise this distribution directly to obtain
\begin{equation}
\tilde p(x_i \, | \, \btheta, \y) = \mathcal{N}(x_i \, | \, \hat{\mu}_i(\btheta), 1 / \hat{q}_i(\btheta)),
\end{equation}
where the marginal mean $\hat{\mu}_i(\btheta)$ and precision $\hat{q}_i(\btheta)$ are recovered directly from the relevant entries of $\hat{\bm{\mu}}(\btheta)$ and $\hat{\bm{Q}}(\btheta)$ respectively.
However, although this approximation is fast, it tends not to be accurate, as it involves evaluating the Gaussian approximation away from its mode.
As a result, although this method is available in \texttt{R-INLA} it is generally not advised.
Instead, \citet{rue2009approximate} propose two methods, a Laplace approximation and a simplified version which is less computationally demanding.
The full Laplace approximation is
\begin{align}
  p(x_i \, | \, \btheta, \y) &= p(\x \, | \, \btheta, \y) \times \frac{1}{p(\x_{-i} \, | \, x_i, \btheta, \y)} \\
  &= \frac{p(\x, \btheta \, | \, \y)}{p(\btheta \, | \, \y)} \times \frac{1}{p(\x_{-i} \, | \, x_i, \btheta, \y)} \\
  &\propto \frac{p(\x, \btheta \, | \, \y)}{p(\x_{-i} \, | \, x_i, \btheta, \y)} \\
  &\approx \frac{p(\x, \btheta \, | \, \y)}{p_G(\x_{-i} \, | \, x_i, \btheta, \y)} \Big\rvert_{\x_{-i} = \hat{\bm{\mu}}_{-i}(x_i, \btheta)}
  = \tilde p_{LA}(x_i \, | \, \btheta, \y), \label{eq:laplaceapproximation}
\end{align}
where
\begin{equation}
p_G(\x_{-i} \, | \, x_i, \btheta, \y) = \mathcal{N}(\x_{-i} \, | \, \hat{\bm{\mu}}_{-i}(x_i, \btheta), \hat{\bm{Q}}_{-i}(x_i, \btheta)),
\end{equation}
is the Gaussian approximation to $\x_{-i} \, | \, x_i, \btheta, \y$ and $\hat{\bm{\mu}}_{-i}(x_i, \btheta)$ is its modal configuration.\footnote{Note that $\hat{\bm{\mu}}(\btheta)$ is the mode of the Gaussian approximation to the full latent field given $\btheta$ and $\hat{\bm{\mu}}_{-i}(x_i, \btheta)$ is not the same as $\hat{\bm{\mu}}_{-i}(\btheta)$.}
The set of distributions $\{p(\x_{-i} \, | \, x_i, \btheta, \y)\}_{i = 1}^n$ are usually reasonably Gaussian so this approximation tends to work well.
However, the Gaussian approximation $p_G(\x_{-i} \, | \, x_i, \btheta, \y)$ must be recomputed for each value of $i$, which is often computationally prohibitive.
Therefore, two modifications to Equation \eqref{eq:laplaceapproximation} are proposed by \citet{rue2009approximate} to reduce the computational cost:
\begin{enumerate}
\item
Avoiding having to find the mode via optimisation by using the approximation $\hat{\bm{\mu}}_{-i}(x_i, \btheta) \approx \mathbb{E}_{p_G(\x \, | \, \btheta, \y)}(\x_{-i} \, | \, x_i)$
\item
As only those $x_j$ close to $x_i$ should have an impact on the marginal of $x_i$, then by selecting some subset $R_i(\btheta)$ of nodes $j$ to impact $j$ the matrix which needs to be factorised can be reduced in dimension to be $| R_i(\btheta) | \times | R_i(\btheta) |$ rather than $n \times n$
\end{enumerate}

## Simplified INLA and connection to ELGMs

@wood2020simplified propose a simplified version of the INLA algorithm which relaxes the sparsity assumptions on the latent field required for the modifications to Equation \eqref{eq:laplaceapproximation} to be efficient.
Doing so facilitates inference for ELGMs [@stringer2022fast] which we define below.
Informally, ELGMs build on LGMs by allowing each element of the linear predictor $\mu_i$ to depend on any subset of elements from the latent field.

## Implementation within `aghq`\label{sec:aghq}

### Adaptive Gaussian Hermite quadrature

### Template Model Builder

`TMB` [@kristensen2016tmb] is an \textsf{R} package for fitting random effect models, also known as latent variable models, hierarchical models or a host of other names.
In `TMB` inference is based upon optimisation of a target function.
This makes it very flexible, and able to handle non-linear, non-Gaussian random effect models.

The approach of `TMB` is inspired by the AD Model Builder (ADMB) package [@fournier2012ad], where the "AD" stands for automatic differentiation, a technique for calculating derivates of functions by repeated application of the chain rule.
AD is popular in machine learning [@baydin2017automatic], including as the basis for backpropagation algorithm, and is beginning to gain popularity in statistics, including as a part of Stan [@carpenter2017stan].
`TMB` uses the derivatives from AD for multiple purposes including calculation of the Hessian used in Gaussian approximations and for numerical optimisation routines.

Consider unobserved latent random effects $\x \in \mathbb{R}^n$ and parameters $\btheta \in \mathbb{R}^m$.^[@kristensen2016tmb use the notation $u$ for random effects and $\theta$ for parameters. We aim for consistency with Section \ref{sec:inla}.]
Let $\ell(\x, \btheta) \triangleq - \log p(\y \, | \, \x, \btheta)$ be the negative joint log-likelihood.
In `TMB`, the user writes C++ code to evaluate this negative log-likelihood function $\ell$.
A standard maximum likelihood approach is to optimise
\begin{equation}
L_\ell(\btheta) \triangleq \int_{\mathbb{R}^n} p(\y \, | \, \x, \btheta) \text{d}\x = \int_{\mathbb{R}^n} \exp(-\ell(\x, \btheta)) \text{d}\x \label{eq:flikelihood}
\end{equation}
with respect to $\btheta$ to find the maximum likelihood estimator (MLE) $\hat \btheta$.
Taking a superficially more Bayesian approach than above, instead of $\ell$, the user may instead write a function to evaluate the negative joint penalised log-likelihood given by
\begin{equation}
f(\x, \btheta) 
\triangleq - \log p(\y \, | \, \x, \btheta) p(\x, \btheta)
= \ell(\x, \btheta) - \log p(\x, \btheta),
\end{equation}
equivalent up to an additive constant to the negative log-posterior.
<!-- \begin{equation} -->
<!-- f(\x, \btheta)  -->
<!-- = - \log p(\y, \x, \btheta)  -->
<!-- = - \log p(\x, \btheta \, | \, \y) - C, -->
<!-- \end{equation} -->
<!-- where $C = \log p(\y)$ is the log evidence. -->
Using $f$ in place of $\ell$, then the penalised likelihood is proportional to the posterior marginal of $\btheta$
\begin{equation}
L_f(\btheta) 
\triangleq \int_{\mathbb{R}^n} \exp(-f(\x, \btheta)) \text{d}\x 
\propto \int_{\mathbb{R}^n} p(\x, \btheta \, | \, \y) \text{d}\x = p(\btheta \, | \, \y). \label{eq:1}
\end{equation}
Integrating out the random effects directly, as in Equation \ref{eq:1} above, is usually intractable because $\x$ is high-dimensional, so \citet[Equation 3]{kristensen2016tmb} use a Laplace approximation $L^\star_f(\btheta)$ based instead upon integrating out a Gaussian approximation to the random effects.
This Laplace approximation is analogous to the INLA approximation $\tilde p(\btheta \, | \, \y)$ given in Section \ref{sec:approxi}.
\begin{equation*}
f''_{\x\x}(\hat{\bm{\mu}}(\btheta), \btheta) = - \frac{\partial^2}{\partial \x^2} \log p(\y, \x, \btheta) \Big\rvert_{\x = \hat{\bm{\mu}}(\btheta)}
= - \frac{\partial^2}{\partial \x^2} \log p(\x \, | \, \btheta, \y) \Big\rvert_{\x = \hat{\bm{\mu}}(\btheta)}
= \hat{\bm{Q}}(\btheta).
\end{equation*}
Inference proceeds by optimising $L^\star_f(\btheta)$ via minimisation of
\begin{equation}
-\log L^\star_f(\btheta) \propto \frac{1}{2} \log \det (\hat{\bm{Q}}(\btheta)) + f(\hat{\bm{\mu}}(\btheta), \btheta)  \label{eq:nllaplace},
\end{equation}
where $\propto$ is used to mean proportional up to an additive constant.
The parameters of the Gaussian approximation (Equation \ref{eq:gaussianx}), are found in terms of $f$ via $\hat{\bm{\mu}}(\btheta) = \arg \min_\x f(\x, \btheta)$ and $\hat{\bm{Q}}(\btheta) = f''_{\x\x}(\hat{\bm{\mu}}(\btheta), \btheta)$ and must be recomputed for each value of $\btheta$.
Obtaining $\hat{\bm{\mu}}(\btheta)$ is known as the inner optimisation step.

# Application to the Naomi model\label{sec:results}

The \textsc{R} [@r] code used to implement the model and produce all results we describe is available at `github.com/athowes/naomi-inf`.

# Discussion\label{sec:conclusions}

# Appendix {#appendix .unnumbered}

# Acknowledgements {#acknowledgements .unnumbered}

AH was supported by the EPSRC Centre for Doctoral Training in Modern Statistics and Statistical Machine Learning (EP/S023151/1).

# References {#references .unnumbered}
