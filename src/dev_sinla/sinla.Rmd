---
title: "Implementing simplified INLA into `aghq`"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** Wood (2020) simplify the INLA method by approximating the latent field posterior marginals using a reduced cost Laplace approximation which does not rely on sparsity assumptions. In spatio-temporal statistics, usually the latent field corresponds to spatio-temporal locations -- so these marginal posteriors are of central scientific interest.
  
  **Task** We implement the method in R making use of Template Model Builder (`TMB`) for Gaussian approximations, in a way compatible with the `aghq` package for adaptive Gauss-Hermite quadrature. We are interested in efficiently approximating latent field posterior marginals $p(x_i \, | \, \theta, y) \approx \tilde p(x_i \, | \, \theta, y)$ for each index $i$. To build up to implementing the simplified INLA approach, we will start with two simpler methods (1) taking the marginal of a joint Gaussian approximation (2) doing a full Laplace approximation.
---

```{r}
cbpalette <- multi.utils::cbpalette()
```

# Background

The AGHQ/ELGM approximation to the joint posterior of the latent field is the following object
$$
\tilde p(x \, | \, y) = \sum_z \tilde p_\text{G}(x \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z),
$$
where $\theta(z) \in \Theta$ and $w_k(z) \in \mathbb{R}$ are AGHQ nodes and weights generated by exploration of $\tilde p_\text{LA}(\theta \, | \, y)$.
In particular, let
$$
\mathcal{Q}(1, k) = \{z_d \in \mathbb{R}: H_k(z_d) = 0\}
$$
where $H_k(z_d)$ are the Hermite polynomials for $k \in \mathbb{N}$, and
$$
w_k(z_d) = \frac{k!}{H_{k + 1}(z_d)^2 \times \phi(z_d)}, 
$$
Define $\mathcal{Q}(d, k) = \mathcal{Q}(1, k)^d$ with nodes $z$, of which there will be $k^d$.
Then $\theta(z) = \hat \theta + Lz$ where $\hat \theta$ is the mode (of $p_\text{LA}(\theta \, | \, y)$) and $L$ is the lower Cholesky triangle of the curvature (at the mode) such that $LL^\top = H^{-1}$.

This can be evaluated at any point, though in practice we work with it by drawing samples $x \sim p_\text{G}(x \, | \, \theta, y)$ using a method from sampling from a Gaussian given its mean and precision from Rue (in particular you solve a linear equation using the lower Cholesky triange of the curvature at a randomly chosen quadrature point).
For the $i$th marginal, we just take the appropriate samples from the joint.
The approximation is therefore
$$
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z),
$$
where $p_\text{G}(x_i \, | \, \theta, y)$ just takes the $i$th elements of the mean vector and $(i, i)$th element of the covariance matrix.

To understand how this is implemented in `aghq`, the two most important functions are `aghq::marginal_laplace_tmb` and `aghq::sample_marginal`.
`aghq::marginal_laplace_tmb` calculates an object representing $\tilde p(x \, | \, \theta, y)$.
`aghq::sample_marginal` draws samples from it.
For a line-by-line walkthrough of how these functions work, see the notebook [Code walkthrough](https://athowes.github.io/elgm-inf/walkthrough.html) (which uses the model from the notebook [Epilepsy example](https://athowes.github.io/elgm-inf/epil.html)).

Now, we want to create a new approximation swapping out $\tilde p_\text{G}(x_i \, | \, \theta, y)$ from the above for the simplified INLA approximation $p_{\text{SINLA}}(x_i \, | \, \theta, y)$ as follows
$$
\tilde p(x_i \, | \, \theta, y) = \sum_z \tilde p_{\text{SINLA}}(x_i \, | \, \theta, y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z).
$$
The goal of doing this is that the new approximation should be more accurate than taking marginals of a joint Gaussian approximation.
At the extreme, you could just compute a Laplace approximation for each marginal, integrating out all other elements of the latent field, $x_{-i}$, together with the fixed effects $\theta$.
This would be an accurate approach, but not computationally feasible for large models.
What we hope to achieve is an approximation which gets to most of the accuracy benefits of this new Laplace approximation approach, but without the high computational cost.

# Ground truth

To provide a ground truth to compare the methods that follow to, we generate "gold-standard" inferences using a relatively long run of MCMC as implemented by `tmbstan`.

We begin by loading in testing data, as generated by [`prev-anc-art_sim`](https://github.com/athowes/elgm-inf/tree/master/src/prev-anc-art_sim).
`sim_data` contains many replicates of simulated data, so here we just take the first one, as given by:

```{r}
sim_data <- readRDS("depends/sim_data.rds")
data <- sim_data[[1]]
dat <- list(n = data$n, y_prev = data$y_prev, m_prev = data$m_prev)
dat
```

```{r}
param <- list(
  beta_prev = -2,
  phi_prev = rep(0, data$n),
  log_sigma_phi_prev = -1
)

compile("model1.cpp")
dyn.load(dynlib("model1"))
```

We create an objective function `h` (see Appendix for `TMB` template) integrating the random effects $x$ (which here correspond to the spatial random effects `phi_prev`):

```{r}
h <- MakeADFun(
  data = dat,
  parameters = param,
  random = "phi_prev",
  DLL = "model1",
  silent = TRUE
)
```

Fit the model using MCMC:

```{r}
mcmc <- tmbstan::tmbstan(h, chains = 4, iter = 5000)
```

What does the posterior marginal $p(\phi_1 \, | \, y)$ look like?

```{r}
extract(mcmc, pars = "phi_prev[1]") %>%
  as.data.frame() %>%
  ggplot(aes(x = phi_prev.1.)) +
    geom_histogram(aes(y = ..count.. / sum(..count..)), alpha = 0.8, fill = cbpalette[7]) +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior")
```

# Marginal of joint Gaussian approximation

## $p(\theta \, | \, y)$

The `aghq` package currently approximates latent field posterior marginals by taking marginals of the joint Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$ (this is done implicitly, via sampling).
This is same approach used by `method = "gaussian"` in `R-INLA`.

The Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$ is previously used in computing the Laplace approximation to $p(\theta \, | \, y)$
$$
\begin{equation}
\tilde p(\theta \, | \, y) \propto \frac{p(x, \theta, y)}{p_\text{G}(x \, | \, \theta, y)} \Big\rvert_{x = \hat{\mu}(\theta)} = \frac{p(y, \hat{\mu}(\theta), \theta)}{\det(2 \pi \hat{Q}(\theta))^{1/2}}, (\#eq:pthetagiveny)
\end{equation}
$$
such that one easy way to calculate it is to use set `random = "x"` in the `TMB` template to integrate the latent field out, then recover the Gaussian approximation from there, as we have done in the function `h` above.
Let's optimise `h`:

```{r}
its <- 1000

opt <- nlminb(
  start = h$par,
  objective = h$fn,
  gradient = h$gr,
  control = list(iter.max = its, trace = 0)
)
```

The values `opt$par` below are the values of $\theta$ which maximise $\tilde p(\theta \, | \, y)$:

```{r}
opt$par
```

We can also look at the parameters (that is the mean vector and precision matrix) of the Gaussian approximation at the mode above (`opt$par`).
Note that in `aghq`, what we would do is create a quadrature grid, then get the mean vector and precision matrix for each value of the quadrature grid over $\theta$.
By just sticking to the mode of $\theta$, we are effectively doing `aghq` with $k = 1$ in each dimension, i.e. one grid point, corresponding to empirical Bayes type of inference.

```{r}
#' Notice that last.par contains opt$par as the fixed effects
#' It's the last.par because the last thing we've done with the template is the optimsation
#' This seems a pretty shakey way to go about things, but here we are...
mm <- h$env$last.par

#' These are the indices of the random effects that we want
h$env$random

#' Note that I'm calling this mean, but because it's a Gaussian the mean is the same as the mode
mean <- mm[h$env$random]
length(mean) #' The length is 30
mean

#' Hessian evaluated at the mean (or equivalently mode)
Q <- h$env$spHess(mm, random = TRUE)
dim(Q) #' With dimension [30 x 30]
image(Q)
```

## Reproducing output from `h`

As a check, we would like to reproduce `h` using the right hand side of Equation \@ref(eq:pthetagiveny).
To do this, we start by creating an objective function `f` (see Appendix for `TMB` template) to evaluate $- \log p(x, \theta, y)$ (without any Laplace approximations!).
This is what we will be using for the numerator of Equation \@ref(eq:pthetagiveny):

```{r}
f <- MakeADFun(
  data = dat,
  parameters = param,
  DLL = "model1",
  silent = TRUE
)
```

```{r}
input_param <- param
```

For the `phi_prev` parameters, we want to be using the `mean` that we previously computed, then evaluating everything using `f`.
We multiply by -1 because `f$fn` gives the negative log-likelihood, and we want just the log-likelihood:

```{r}
input_param$phi_prev <- mean
numer <- -1 * f$fn(unlist(input_param))
numer
```

Then for the denominator, it should be as follows:

```{r}
denom <- 0.5 * log(det(2 * pi * Q))
denom
```

On the negative log-likelihood scale, the number we get is:

```{r}
-1 * (numer - denom)
```

As compared with what we got from `h`.
They're not the same!
Something is wrong here:

```{r}
h$fn(input_param[c("beta_prev", "log_sigma_phi_prev")])
```

## Moving to $\tilde \tilde p(x_i \, | \, y)$

Let's have a go at computing $\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z)$ so that we can compare to the posterior distribution from `tmbstan`.

We have simplified matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{G}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{G}(x_i \, | \, \hat \theta, y)
\end{align}
$$
We're going to evaluate this by taking samples from the full Gaussian, the subsetting them:

```{r}
#' To write
```

# Full Laplace approximation

Another approach we might take is to calculate the full Laplace approximation
$$
\begin{align*}
  \tilde p(x_i \, | \, \theta, y) &\propto \frac{p(x, \theta, y)}{p_\text{G}(x_{-i} \, | \, x_i, \theta, y)} \Big\rvert_{x_{-i} = \hat{\mu}_{-i}(x_i, \theta)} \\
  &= \frac{p(y, x_i, \hat{\mu}_{-i}(x_i, \theta), \theta)}{\det(2 \pi \hat{Q}_{-i}(x_i, \theta))^{1/2}}
\end{align*}
$$
This is not practical in some settings as it involves recomputing the Gaussian approximation $p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ for each value of $(x_i, \theta)$ which can be too computationally expensive.
With of a bit of effort, we can implement this in `TMB` by passing in data where elements $i$ and $-i$ of the latent field are named as such (see Appendix for template).

```{r}
compile("model1index.cpp")
dyn.load(dynlib("model1index"))
```

The `prepare_dat` function takes `dat` and makes it compatible with `model1index` for particular choice of marginal `i`.

```{r}
prepare_dat <- function(dat, i) {
  dat[["y_prev_i"]] <- dat$y_prev[i]
  dat[["y_prev_minus_i"]] <- dat$y_prev[-i]
  dat[["m_prev_i"]] <- dat$m_prev[i]
  dat[["m_prev_minus_i"]] <- dat$m_prev[-i]
  dat[c("n", "y_prev_i", "y_prev_minus_i", "m_prev_i", "m_prev_minus_i")]
}
```

Now loop over doing the Laplace approximation for every index.
In the call to `MakeADFun` we set `random = "phi_prev_minus_i"` to integrate all of the random effects but that of the particular index.

```{r message=FALSE, warning=FALSE, results=FALSE}
objs <- list()
means <- list()
Qs <- list()

for(i in 1:dat$n) {
  dat_i <- prepare_dat(dat, i)

  #' Starting parameters for TMB are the same for every index of the loop
  param_i <- list(
    beta_prev = 0,
    phi_prev_i = 0,
    phi_prev_minus_i = rep(0, data$n - 1),
    log_sigma_phi_prev = 0
  )

  #' random are integrated out with a Laplace approximation
  obj <- MakeADFun(
    data = dat_i,
    parameters = param_i,
    random = "phi_prev_minus_i",
    DLL = "model1index"
  )

  its <- 1000

  opt <- nlminb(
    start = obj$par,
    objective = obj$fn,
    gradient = obj$gr,
    control = list(iter.max = its, trace = 0)
  )

  sd_out <- sdreport(
    obj,
    par.fixed = opt$par,
    getJointPrecision = TRUE
  )

  #' Parameters of the Gaussian approximation
  mean <- with(obj$env, last.par[random])
  Q <- obj$env$spHess(with(obj$env, last.par), random = TRUE)
  
  objs[[i]] <- obj
  means[[i]] <- mean
  Qs[[i]] <- Q
}
```

Let's consider the first marginal, that is $\tilde p_\text{LA}(x_1 \, | \, \theta, y)$.
The mean and precision matrix for the Gaussian approximation $\tilde p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ are:

```{r message=FALSE, warning=FALSE}
i <- 1
obj <- objs[[i]]
mean <- means[[i]] #' The length is 30 - 1
mean

Q <- Qs[[i]]
dim(Q) #' With dimension [(30 - 1) x (30 - 1)]
image(Q)
```

`obj$fn` should take as input $(x_i, \theta)$:

```{r}
transform_param <- function(param) {
  list(
    beta_prev = param$beta_prev,
    phi_prev_i = param$phi_prev[i],
    phi_prev_minus_i = param$phi_prev[-i],
    log_sigma_phi_prev = param$log_sigma_phi_prev
  )
}

transformed_param <- transform_param(param)
transformed_param$phi_prev_minus_i <- NULL #' Remove the integrated out effects
transformed_param

obj$fn(transformed_param)
```

We should be able to reproduce the output of `obj$fn` using the mean and precision matrix, together with the function `f`.
Recall that `f` takes as input the complete $(x, \theta)$ ($y$ is already fixed).
For example:

```{r}
param
f$fn(unlist(param))
```

We update `param` to include $\hat{\mu}_{-i}(x_i, \theta)$, so that it's only element `r i` of `phi_prev` which remains as before:

```{r}
param$phi_prev[-i] <- mean
param
```

The numerator of the Laplace approximation is (n.b. this is on the log-scale, which is where we will do the calculations):

```{r}
numer <- -1 * f$fn(unlist(param))
numer
```

And the denominator of the Laplace approximation (again on the log-scale) is:

```{r}
denom <- 0.5 * log(det(2 * pi * Q))
denom
```

```{r}
-1 * (numer - denom)
```

Compare this to the output from `obj`:

```{r}
obj$fn(transformed_param)
```

# Simplified INLA

$$
\hat \mu_{-i}(x_i, \theta) = [\hat \mu(\theta)]_{-i} + \hat Q(\theta)^{-1}_{-i, i} \hat Q(\theta)_{i, i}(x_i - [\hat \mu(\theta)]_{i})
$$

# Appendix

## `model1.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1.cpp')}
```

## `model1index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1index.cpp')}
```
