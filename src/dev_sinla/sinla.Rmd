---
title: "Implementing simplified INLA into `aghq`"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** Wood (2020) simplify the INLA method by approximating the latent field posterior marginals using a reduced cost Laplace approximation which does not rely on sparsity assumptions. In spatio-temporal statistics, usually the latent field corresponds to spatio-temporal locations -- so these marginal posteriors are of central scientific interest.
  
  **Task** We implement the method in R making use of Template Model Builder (`TMB`) for Gaussian approximations, in a way compatible with the `aghq` package for adaptive Gauss-Hermite quadrature. We are interested in efficiently approximating latent field posterior marginals $p(x_i \, | \, \theta, y) \approx \tilde p(x_i \, | \, \theta, y)$ for each index $i$. To build up to implementing the simplified INLA approach, we will start with two simpler methods (1) taking the marginal of a joint Gaussian approximation (2) doing a full Laplace approximation.
---

**Warning: this report was created during the process of learning, and for that reason is not concise or well-structured. I would not recommend you reading it, but if you do proceed at your own caution!**

```{r echo=FALSE}
cbpalette <- multi.utils::cbpalette()
```

# Background

An AGHQ approximation to the joint posterior of the latent field is given by
$$
\tilde p(x \, | \, y) = \sum_z \tilde p_\text{G}(x \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) \omega(z), (\#eq:pxgiveny)
$$
where $\theta(z) \in \Theta$ and $\omega(z) \in \mathbb{R}$ are nodes and weights generated by adaption to $\tilde p_\text{LA}(\theta \, | \, y)$.
In particular, let
$$
\mathcal{Q}(1, k) = \{z_d \in \mathbb{R}: H_k(z_d) = 0\}
$$
be the set of nodes in one-dimension, exactly the zeros of the Hermite polynomials $H_k(z_d)$ for $k \in \mathbb{N}$, and
$$
\omega(z_d) = \frac{k!}{H_{k + 1}(z_d)^2 \times \phi(z_d)}, 
$$
be the associated weights.
Define $\mathcal{Q}(d, k) = \mathcal{Q}(1, k)^d$ to be the set of nodes $z$ in $d$-dimensions, of which there will be $k^d$.
Then these points are adapted via $\theta(z) = \hat \theta + Lz$, where $\hat \theta$ is the mode and $L$ is the lower Cholesky triangle of the curvature at the mode such that $LL^\top = H^{-1}$.

This can be evaluated at any point, though in practice we work with it by drawing samples $x \sim p_\text{G}(x \, | \, \theta, y)$ using a method from sampling from a Gaussian given its mode and precision from Rue (in particular you solve a linear equation using the lower Cholesky triangle of the curvature at a randomly chosen quadrature point).
For the $i$th marginal, we just take the appropriate samples from the joint.
The approximation is therefore
$$
\begin{equation}
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) \omega(z), (\#eq:pxigiveny)
\end{equation}
$$
where $p_\text{G}(x_i \, | \, \theta, y)$ just takes the $i$th elements of the mode vector and $(i, i)$th element of the covariance matrix.

To understand how this is implemented in `aghq`, the two most important functions are `aghq::marginal_laplace_tmb` and `aghq::sample_marginal`.
`aghq::marginal_laplace_tmb` calculates an object representing $\tilde p(x \, | \, \theta, y)$.
`aghq::sample_marginal` draws samples from this Gaussian, given values of $z$ drawn from a multinomial distribution, in order to evaluate Equation \@ref(eq:pxigiveny).
For a line-by-line walkthrough of how these functions work, see the notebook [Code walkthrough](https://athowes.github.io/naomi-aghq/walkthrough.html) (which uses the model from the notebook [Epilepsy example](https://athowes.github.io/naomi-aghq/epil.html)).

The goal of this notebook ([Implementing simplified INLA into `aghq`](https://athowes.github.io/naomi-aghq/sinla.html)) is to create a new approximation swapping out $\tilde p_\text{G}(x_i \, | \, \theta, y)$ from the above for the simplified INLA approximation $\tilde p_{\text{SINLA}}(x_i \, | \, \theta, y)$ as follows
$$
\tilde p(x_i \, | \, y) = \sum_z \tilde p_{\text{SINLA}}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) \omega(z).
$$
We want to do this because the new approximation should be more accurate than taking marginals of a joint Gaussian approximation.
At the extreme, you could just compute a Laplace approximation for each marginal, integrating out all other elements of the latent field, $x_{-i}$, together with the fixed effects $\theta$.
This would be an accurate approach, but not computationally feasible for large models.
What we hope to achieve is an approximation which gets to most of the accuracy benefits of the Laplace approximation approach, but without the high computational cost.

# Ground truth {#ground-truth}

We will use Model 1 from the [Prevalence, ANC, ART model inference case-study](https://athowes.github.io/naomi-aghq/prev-anc-art.html) as a running example.
To provide a ground truth to compare the methods that follow to, we generate "gold-standard" inferences using a relatively long run of MCMC as implemented by `tmbstan`.

We begin by loading in testing data, as generated by [`prev-anc-art_sim`](https://github.com/athowes/naomi-aghq/tree/master/src/prev-anc-art_sim).
`sim_data` contains many replicates of simulated data.
Here we only need one, so will will just take the first as given by:

```{r}
sim_data <- readRDS("depends/sim_data.rds")
data <- sim_data[[1]]
dat <- list(n = data$n, y_prev = data$y_prev, m_prev = data$m_prev)
dat
```

Compile and load the template (see Appendix \@ref(appendix) for the `TMB` code):

```{r}
compile("model1.cpp")
dyn.load(dynlib("model1"))
```

We create an objective function `h` integrating the random effects $x$ (which here correspond to the spatial random effects `phi_prev`).
The values of `param` intitialise the objective function:

```{r}
param <- list(
  beta_prev = -2,
  phi_prev = rep(0, data$n),
  log_sigma_phi_prev = -1
)

h <- MakeADFun(
  data = dat,
  parameters = param,
  random = "phi_prev",
  DLL = "model1",
  silent = TRUE
)
```

It is simple to fit the model using MCMC via the `tmbstan` package.
We use 4 `chains` each of 5000 `iter`.
Note that although `h` is passed to `tmbstan::tmbstan`, the Laplace approximation is disabled by setting `laplace = FALSE` by default (useful to know in case it's ever of interest to sample from the Laplace approximation).

```{r}
mcmc <- tmbstan::tmbstan(h, chains = 4, iter = 5000, refresh = 0)
```

The posterior marginal $p(\phi_1 \, | \, y)$ can be examined by extracting the corresponding Markov chains, and plotting a histogram of the draws:

```{r message=FALSE, warning=FALSE}
samples_tmbstan <- extract(mcmc, pars = "phi_prev[1]")[[1]]

plot_pdf <- ggplot(data.frame(x = samples_tmbstan), aes(x = x)) +
    geom_histogram(aes(y = ..density..), alpha = 0.8, fill = cbpalette[7]) +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior PDF")

ecdf_tmbstan <- ecdf(samples_tmbstan)
grid <- seq(min(samples_tmbstan), max(samples_tmbstan), by = 0.1)

plot_cdf <- data.frame(x = grid, y = ecdf_tmbstan(grid)) %>%
  ggplot(aes(x = x, y = y)) +
    geom_line(col = cbpalette[7]) +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior CDF")

cowplot::plot_grid(plot_cdf, plot_pdf)
```

# Marginal of joint Gaussian approximation {#gaussian-marginal}

Now we will compute Equation \@ref(eq:pxgiveny) taking the marignals of a joint Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$.
This is the approach currently used in the `aghq` package (done implicitly, via sampling).
This approach is also used by `method = "gaussian"` in `R-INLA`.

## $\tilde p(\theta \, | \, y)$ 

The Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$ is previously used in computing the Laplace approximation to $p(\theta \, | \, y)$
$$
\begin{equation}
\tilde p(\theta \, | \, y) \propto \frac{p(x, \theta, y)}{\tilde p_\text{G}(x \, | \, \theta, y)} \Big\rvert_{x = \hat x(\theta)} = \frac{p(y, \hat x(\theta), \theta)}{\det \left(\frac{1}{2 \pi} H(\theta) \right)^{1/2}}, (\#eq:pthetagiveny)
\end{equation}
$$
such that one easy way to calculate it is to set `random = "x"` in the `TMB` template to integrate the latent field out, then recover the Gaussian approximation from there, as we have done in the function `h` above.
Let's optimise `h` to obtain estimates $\hat \theta$ which maximise $\tilde p(\theta \, | \, y)$:

```{r}
its <- 1000

opt_theta <- nlminb(
  start = h$par,
  objective = h$fn,
  gradient = h$gr,
  control = list(iter.max = its, trace = 0)
)
```

The values `opt_theta$par` below correspond to $\hat \theta$:

```{r}
opt_theta$par
```

We can also look at the parameters -- that is, mode vector and precision matrix -- of the Gaussian approximation at the mode `opt_theta$par` above.
Note that in `aghq`, what we would do is create a quadrature grid, then get the mode vector and precision matrix for each value of the quadrature grid over $\theta$.
By just sticking to the mode of $\theta$, we are effectively doing `aghq` with $k = 1$ in each dimension, i.e. one grid point, corresponding to empirical Bayes type of inference.

```{r}
#' Notice that last.par contains opt_theta$par as the fixed effects
#' It's the last.par because the last thing we've done with the template is the optimsation
#' This seems a pretty shakey way to go about things, but here we are...
mm <- h$env$last.par

#' These are the indices of the random effects that we want
h$env$random

x_hat <- mm[h$env$random]
x_hat <- rlang::duplicate(x_hat) #' Insurance...
length(x_hat) #' The length is 30

#' Hessian evaluated at the mode
x_H <- h$env$spHess(mm, random = TRUE)
x_H <- rlang::duplicate(x_H)
dim(x_H) #' With dimension [30 x 30]
```

## Additional checks {.tabset .tabset-fade}

### Description {-}

These tabs contain additional sanity checks used to build understanding.
Feel free to skip them.

### Reproducing output from `h` {-}

We would like to reproduce `h` using the right hand side of Equation \@ref(eq:pthetagiveny).
To do this, we start by creating an objective function `f` (see Appendix \@ref(appendix) for the `TMB` code) to evaluate $- \log p(x, \theta, y)$ (without any Laplace approximations!).
This is what we will be using for the numerator of Equation \@ref(eq:pthetagiveny):

```{r}
f <- MakeADFun(
  data = dat,
  parameters = param,
  DLL = "model1",
  silent = TRUE
)
```

Given $\theta$, we compute the `mode` and `H` of $x$ (corresponding to `phi_prev`) using `h`.
We then concatenate and evaluate using `f` to get the numerator, multiplying by -1 because `f$fn` gives the negative log-likelihood, and we want the log-likelihood.
In the denominator, we use `determinant` on the log-scale as it is more numerically stable than e.g. `log(det(...))`.

```{r}
calculate_rhs <- function(theta) {
  #' Calculate mode and H for those theta
  #' TMB template remembers what you've called -- spooky
  h$fn(theta)
  mm <- h$env$last.par
  mode <- mm[h$env$random]
  H <- h$env$spHess(mm, random = TRUE)
  
  #' Append the calculated latent field mode
  param <- theta
  param$phi_prev <- as.numeric(mode)
  param <- param[c("beta_prev", "phi_prev", "log_sigma_phi_prev")]
  
  #' Evaluate the log-numerator of the RHS
  numer <- -1 * f$fn(unlist(param))
  
  #' Evaluate the log-denominator of the RHS
  denom <- 0.5 * (determinant(H, logarithm = TRUE)$modulus - (nrow(H) * log(2 * pi)))
  
  #' Return their difference
  numer - denom
}

theta <- param
theta$phi_prev <- NULL #' Setting the value of x to be NULL

calculate_rhs(theta)
```

As compared with what we got from `h`:

```{r}
-1 * h$fn(unlist(theta))
```

## $\tilde p(x_i \, | \, y)$

Now let's have a go at computing Equation \@ref(eq:pxigiveny) so that we can compare to the posterior distribution from `tmbstan` we obtained in Section \@ref(ground-truth).
As described above, we simplify matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{G}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{G}(x_i \, | \, \hat \theta, y)
\end{align}
$$

We can evaluate this for $i = 1$ via:

```{r}
i <- 1
mode_i <- as.numeric(x_hat[i])
var_i <- solve(x_H)[i, i]
```

Because this is a Gaussian distribution, we do not have do normalise it (as would usually be the case) because we know it's a density.
We can compare it to our MCMC results as follows:

```{r message=FALSE, warning=FALSE}
plot_pdf2 <- plot_pdf +
  stat_function(
    data = data.frame(x = c(0.5, 3.5)), aes(x, col = "Joint Gaussian marginal"), fun = dnorm, n = 101, 
    args = list(mean = mode_i, sd = sqrt(var_i))
  ) +
  labs(x = "prev_phi[1]", y = "Posterior", col = "Approximation") +
  scale_colour_manual(values = cbpalette) +
  theme_minimal()

M <- 1000
samples_gaussian <- rnorm(M, mode_i, sqrt(var_i)) 

ecdf_gaussian <- ecdf(samples_gaussian)
grid <- seq(min(samples_gaussian), max(samples_gaussian), by = 0.1)

plot_cdf2 <- plot_cdf +
  geom_line(
    data = data.frame(x = grid, y = ecdf_gaussian(grid)),
    aes(x = x, y = y), col = cbpalette[1]
  )

cowplot::plot_grid(plot_cdf2, plot_pdf2, rel_widths = c(1, 1.5))
```

```{r}
ks_gaussian_tmbstan <- inf.utils::ks_test(samples_tmbstan, samples_gaussian)
ks_gaussian_tmbstan
```

The maximum difference between the two ECDFS is `r ks_gaussian_tmbstan$D` which occurs at the point `r ks_gaussian_tmbstan$l`.

# Full Laplace approximation {#full-laplace}

In this section we will continue to use the approximation \@ref(eq:pthetagiveny) for $\tilde p(\theta \, | \, y)$.

## For one index $i$

### $\tilde p(x_i \, | \, \theta, y)$

For the approximation to the latent field marginal posteriors, another approach we could take is to calculate the full Laplace approximation
$$
\begin{align*}
  \tilde p(x_i \, | \, \theta, y) &\propto \frac{p(x, \theta, y)}{p_\text{G}(x_{-i} \, | \, x_i, \theta, y)} \Big\rvert_{x_{-i} = \hat x_{-i}(x_i, \theta)} (\#eq:pxigiventhetay) \\
  &= \frac{p(y, x_i, \hat x_{-i}(x_i, \theta), \theta)}{\det(\frac{1}{2 \pi} H_{-i}(x_i, \theta))^{1/2}}
\end{align*}
$$
integrating out $x_{-i}$ with a Gaussian approximation.
This is not practical in some settings as it involves recomputing the Gaussian approximation $p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ for each value of $(x_i, \theta)$ which can be too computationally expensive.
We will still go through the steps for implementing it here though for demonstration.
With of a bit of effort, it can be implemented in `TMB` by passing in data where elements $i$ and $-i$ of the latent field are explicitly named (see Appendix \@ref(appendix) for the `TMB` code):

```{r}
compile("model1_index.cpp")
dyn.load(dynlib("model1_index"))
```

The `prepare_dat` function takes `dat` and makes it compatible with `model1_index` for particular choice of marginal `i`.

```{r}
prepare_dat <- function(dat, i) {
  dat[["y_prev_i"]] <- dat$y_prev[i]
  dat[["y_prev_minus_i"]] <- dat$y_prev[-i]
  dat[["m_prev_i"]] <- dat$m_prev[i]
  dat[["m_prev_minus_i"]] <- dat$m_prev[-i]
  dat[c("n", "y_prev_i", "y_prev_minus_i", "m_prev_i", "m_prev_minus_i")]
}
```

Let's consider the first marginal, that is $\tilde p_\text{LA}(x_1 \, | \, \theta, y)$.
In the call to `MakeADFun` we set `random = "phi_prev_minus_i"` to integrate all of the random effects but that of the particular index.

```{r message=FALSE, warning=FALSE, results=FALSE}
i <- 1

param_i <- list(
  beta_prev = 0,
  phi_prev_i = 0,
  phi_prev_minus_i = rep(0, data$n - 1),
  log_sigma_phi_prev = 0
)

dat_i <- prepare_dat(dat, i)

#' random are integrated out with a Laplace approximation
obj <- MakeADFun(
  data = dat_i,
  parameters = param_i,
  random = "phi_prev_minus_i",
  DLL = "model1_index",
  silent = TRUE
)

opt <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr,
  control = list(iter.max = its, trace = 0)
)
```

The mode and precision matrix for the Gaussian approximation $\tilde p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ are:

```{r}
get_mode_and_H <- function(obj) {
  mm <- obj$env$last.par
  mode <- mm[obj$env$random]
  mode <- rlang::duplicate(mode)
  H <- obj$env$spHess(mm, random = TRUE)
  H <- rlang::duplicate(H)
  return(list(mode = mode, H = H))
}

xminusi <- get_mode_and_H(obj)
```

Check here that `obj$fn` indeed takes as input $(x_i, \theta)$:

```{r}
transform_param <- function(param) {
  list(
    beta_prev = param$beta_prev,
    phi_prev_i = param$phi_prev[i],
    phi_prev_minus_i = param$phi_prev[-i],
    log_sigma_phi_prev = param$log_sigma_phi_prev
  )
}

transformed_param <- transform_param(param)
transformed_param$phi_prev_minus_i <- NULL #' Remove the integrated out effects

#' transformed_param contains certain values for (x_i, theta) suitable for input to obj$fn
obj$fn(transformed_param)
```

### $\tilde p(x_i \, | \, y)$ {.tabset .tabset-fade}

Let's have a go at computing Equation \@ref(eq:pxigiveny) so that we can compare to the posterior distribution from `tmbstan`.
We simplify matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to^[Note that I'm uncertain if $\hat \theta$ (a constant) should still be included in the LHS of this.]:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{LA}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{LA}(x_i \, | \, \hat \theta, y)
\end{align}
$$

A basic way we could approach this is to define a grid of points that we would like to evaluate the posterior at.
We can then create a dataframe where each row contains the values `opt_theta$par` for $\theta$ and a grid value for $x_i$.
Each row can then be passed into `-obj$fn()` to calculate the log-posterior.
Note that it's crucial the arguments are in the right order when we pass them to `obj$fn()` -- this has caused headaches!

```{r}
spacing <- 0.05
x_grid <- seq(from = -5, to = 5, by = spacing)
x_grid_length <- length(x_grid)

df <- t(opt_theta$par) %>%
  as.data.frame() %>%
  slice(rep(1:n(), each = x_grid_length)) %>%
  mutate(phi_prev_i = x_grid) %>%
  rowwise() %>%
  mutate(logpost = -obj$fn(c(beta_prev, phi_prev_i, log_sigma_phi_prev)))
```

We've computed the unnormalised log-posterior -- how can we go from that to a normalised posterior?
Given these grid of points, we can do the trapezoid rule here.

#### Safe exponential trapezoid rule {-}

The safe exponential function computes $\exp(x - \text{max}(x))$.

```{r}
safe_exp <- function(x) exp(x - max(x))
```

A simple way to write the trapezoid rule is as follows:

```{r eval=FALSE}
trapezoid_rule <- function(x, spacing) {
  0.5 * spacing * (2 * sum(x) - x[1] - x[2])
}
```

It can also be defined like this, which is more convenient for thinking about it like other quadrature rules based on weights:

```{r}
trapezoid_rule <- function(x, spacing) {
  w <- rep(spacing, length(x))
  w[1] <- w[1] / 2
  w[length(x)] <- w[length(x)] / 2
  sum(w * x)
}
```

```{r}
df$post <- safe_exp(df$logpost)
norm_trapezoid <- trapezoid_rule(df$post, spacing = spacing)
df$post_norm <- df$post / norm_trapezoid
```

Check that this method correctly computes the normalising constant for a density (which is known to integrate to one):

```{r}
abs(trapezoid_rule(dnorm(seq(-10, 10, by = 0.1)), spacing = 0.1) - 1) < 10E-6
```

#### `logSumExp` trapezoid rule {-}

What about with another method?
We can calculate the logarithm of the normalising constant directly, then just subtract it from the log-posterior:

$$
\begin{align}
\tilde f(x) &= \frac{f(x)}{\int f(x) \text{d}x} = \frac{f(x)}{C}
\implies \\
\log\tilde f(x) &= \log f(x) - \log C
\end{align}
$$

The `matrixStats::logSumExp` function computes $\text{LSE}(x) = \log(\exp(x_1) + \cdots + \exp(x_n))$ in a computationally stable way.
Another way to compute the trapezoid rule, taking input on the log-scale, is as follows:

```{r}
trapezoid_rule_log <- function(x, spacing) {
  w <- rep(spacing, length(x))
  w[1] <- w[1] / 2
  w[length(x)] <- w[length(x)] / 2
  matrixStats::logSumExp(log(w) + x)
}

norm_trapezoid_log <- trapezoid_rule_log(df$logpost, spacing)
df$post_norm_alt <- exp(df$logpost - norm_trapezoid_log)
```

Check that this method correctly computes the normalising constant for a density (which is known to integrate to one):

```{r}
abs(trapezoid_rule_log(dnorm(seq(-10, 10, by = 0.1), log = TRUE), spacing = 0.1) - log(1)) < 10E-6
```

### Normalisation with `aghq`

The two ways of calculating the normalised posterior above are the same up to numerical error:

```{r}
max(abs(df$post_norm - df$post_norm_alt)) < 10E-6
```

So how might we approach this in a more sophisticated way?
We could do the normalisation directly in `aghq`!

We could start by normalising $\tilde p_\text{LA}(\theta, y)$.
Note that sometimes we have been calling this $\tilde p_\text{LA}(\theta \, | \, y)$ because they are equivalent up to a constant of proportionality.
Define
$$
\tilde p_\text{AQ}(\theta \, | \, y) = \frac{\tilde p_\text{LA}(\theta, y)}{\tilde p_\text{AQ}(y)}
$$
or on a log-scale
$$
\log \tilde p_\text{AQ}(\theta \, | \, y) = \log \tilde p_\text{LA}(\theta, y) - \log \tilde p_\text{AQ}(y)
$$

First we have to add a numerical Hessian that's missing from `h`:

```{r}
h$he <- function(theta) numDeriv::jacobian(h$gr, theta, method = "Richardson")
```

Now we can obtain $\log \tilde p_\text{AQ}(y)$:

```{r}
quad <- aghq::aghq(ff = h, k = 3, startingvalue = c(0, 0), control = aghq::default_control_tmb())
quad$normalized_posterior$lognormconst
```

Remember that `h` is the negative log-likelihood, so we want to add the normalising constant rather than subtract it:

```{r}
h_norm <- function(theta) h$fn(theta) + quad$normalized_posterior$lognormconst
```

Now we can put this into
$$
\tilde p(x_i, \hat \theta \, | \, y) = \tilde p_\text{LA}(x_i \, | \, \hat \theta, y) \tilde p_\text{AQ}(\hat \theta \, | \, y)
$$

But perhaps there is no reason to, because we will just have to normalise it again, so why do it twice?
Let's just try to normalise it directly.
What we need to do is create a function in `TMB` where we fix some of the inputs to the function.
This can be done using the `map` argument of `MakeADFun`.
You set each parameter to `factor(NA)` that you want to be fixed, then they will retain the value you input in `parameters`:

```{r}
obj$par %>% names()

map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))

param_i_fixed_theta <- param_i

theta_names <- names(opt_theta$par)
for(theta in theta_names) {
  param_i_fixed_theta[[theta]] <- opt_theta$par[[theta]]
}

dat_i <- prepare_dat(dat, i)

obj_fixed_theta <- MakeADFun(
  data = dat_i,
  parameters = param_i_fixed_theta,
  random = "phi_prev_minus_i",
  DLL = "model1_index",
  silent = TRUE,
  map = map_fixed_theta
)
```

Now we can normalise this function $p_\text{LA}(x_i, \theta = \hat \theta, y)$ using `aghq`:

```{r}
quad <- aghq::aghq(ff = obj_fixed_theta, k = 3, startingvalue = 0, control = aghq::default_control_tmb())
```

The value of the normalising constant is (approximately) the same as we get using our trapezoid rule:

```{r}
abs(quad$normalized_posterior$lognormconst - norm_trapezoid_log) < 10E-3
```

### Comparison

We now add our Laplace approximation to the existing joint Gaussian marginal and MCMC results:

```{r message=FALSE, warning=FALSE}
plot_pdf3 <- plot_pdf2 +
  geom_line(data = df, aes(x = phi_prev_i, y = post_norm, col = "Laplace"))

df$ecdf <- cumsum(df$post_norm) * c(0, diff(df$phi_prev_i))

plot_cdf3 <- plot_cdf2 +
  geom_line(
    data = df, aes(x = phi_prev_i, y = ecdf), col = cbpalette[2]
  )

cowplot::plot_grid(plot_cdf3, plot_pdf3, rel_widths = c(1, 1.5))
```

We use the inverse CDF method to sample from the Laplace marginal, then we can calculate the KS statistic:

```{r}
samples_laplace <- approxfun(df$ecdf, df$phi_prev_i)(runif(M))

ks_laplace_tmbstan <- inf.utils::ks_test(samples_tmbstan, samples_laplace)

kable_data <- data.frame(
  "Distance" = c(ks_gaussian_tmbstan$D, ks_laplace_tmbstan$D),
  "Location" = c(ks_gaussian_tmbstan$l, ks_laplace_tmbstan$l)
)

rownames(kable_data) <- c("$\\text{KS}(\\texttt{tmbstan}, \\texttt{gaussian})$", "$\\text{KS}(\\texttt{tmbstan}, \\texttt{laplace})$")

kableExtra::kable(kable_data, booktabs = TRUE, escape = FALSE, align = "c")
```

## Looping over index $i$

So we've done the approximation for the first index $i = 1$!
Now let's try computing $\tilde p(x_i \, | \, y)$ for every index $i$, using a for-loop to generate the required Laplace approximations, and tidying up and condensing the code used:

```{r}
xi_laplace_marginal <- function(i) {
  dat_i <- prepare_dat(dat, i)
  map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))
  
  param_i_fixed_theta <- list(
    beta_prev = 0, 
    phi_prev_i = 0, 
    phi_prev_minus_i = rep(0, data$n - 1), 
    log_sigma_phi_prev = 0
  )
  
  for(theta in names(opt_theta$par)) param_i_fixed_theta[[theta]] <- opt_theta$par[[theta]]
  
  obj_fixed_theta <- MakeADFun(
    data = dat_i,
    parameters = param_i_fixed_theta,
    random = "phi_prev_minus_i",
    DLL = "model1_index",
    silent = TRUE,
    map = map_fixed_theta
  )
  
  quad <- aghq::aghq(ff = obj_fixed_theta, k = 3, startingvalue = 0, control = aghq::default_control_tmb())
  pdf_and_cdf <- aghq::compute_pdf_and_cdf(quad)[[1]]
  
  return(pdf_and_cdf)
}

laplace_df <- lapply(1:dat$n, xi_laplace_marginal) %>%
  bind_rows(.id = "index") %>%
  mutate(index = as.numeric(index))

M <- 1000

samples_laplace <- lapply(1:dat$n, function(i) {
  laplace_df_i <- filter(laplace_df, index == i)
  samples <- approxfun(x = laplace_df_i$cdf, y = laplace_df_i$theta, ties = "ordered")(runif(M))
  data.frame("value" = samples)
}) %>%
  bind_rows(.id = "index")
```

Let's also get everything we need for the joint Gaussian marginal for each index:

```{r}
x_Sigma <- solve(x_H)

gaussian_df <- lapply(1:dat$n, function(i) {
  x <- seq(-5, 5, length.out = 1000)
  mode_i <- as.numeric(x_hat[i])
  sd_i <- as.numeric(sqrt(x_Sigma[i, i]))
  data.frame(index = i, x = x, pdf = dnorm(x, mean = mode_i, sd = sd_i))
}) %>%
  bind_rows()

samples_gaussian <- lapply(1:dat$n, function(i) {
  mode_i <- as.numeric(x_hat[i])
  sd_i <- as.numeric(sqrt(x_Sigma[i, i]))
  samples <- rnorm(M, mode_i, sd_i)
  data.frame("value" = samples)
}) %>%
  bind_rows(.id = "index")
```

## Comparison

```{r}
samples_tmbstan <- extract(mcmc, pars = "phi_prev") %>%
  as.data.frame() %>%
  pivot_longer(
    cols = everything(),
    names_to = "index",
    names_prefix = "phi_prev.",
    names_transform = as.integer
  )

plot <- ggplot(samples_tmbstan, aes(x = value)) +
  geom_histogram(aes(y = ..density..), alpha = 0.8, fill = cbpalette[7]) +
  facet_wrap(~index) +
  theme_minimal() +
  labs(x = "phi_prev", y = "Posterior PDF")

plot2 <- plot +
  geom_line(data = gaussian_df, aes(x = x, y = pdf), col = cbpalette[1])
```


```{r}
plot3 <- plot2 +
  geom_line(data = laplace_df, aes(x = theta, y = pdf), col = cbpalette[2])

plot3
```

### KS test

```{r}
ks_gaussian_tmbstan <- lapply(1:length(x_hat), function(i) {
  samples_gaussian_i <- filter(samples_gaussian, index == i)$value
  samples_tmbstan_i <- filter(samples_tmbstan, index == i)$value
  inf.utils::ks_test(samples_gaussian_i, samples_tmbstan_i)
})

ks_laplace_tmbstan <- lapply(1:length(x_hat), function(i) {
  samples_laplace_i <- filter(samples_laplace, index == i)$value
  samples_tmbstan_i <- filter(samples_tmbstan, index == i)$value
  inf.utils::ks_test(samples_laplace_i, samples_tmbstan_i)
})

ks_results <- bind_rows(
  bind_rows(ks_gaussian_tmbstan, .id = "index") %>%
    mutate(type = "gaussian"),
  bind_rows(ks_laplace_tmbstan, .id = "index") %>%
    mutate(type = "laplace")
)

ks_results %>%
  select(index, D, type) %>%
  pivot_wider(
    names_from = type,
    values_from = D
  ) %>%
  ggplot(aes(x = laplace, y = gaussian)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    lims(x = c(0, 1), y = c(0, 1)) +
    labs(x = "KS(laplace, tmbstan)", y = "KS(gaussian, tmbstan)") +
    theme_minimal()

#' Table of average KS distance by method
ks_results %>%
  group_by(type) %>%
  summarise(Distance = mean(D)) %>%
  select(Distance) %>%
  mutate(
    Method = c("$\\text{KS}(\\texttt{tmbstan}, \\texttt{gaussian})$",
               "$\\text{KS}(\\texttt{tmbstan}, \\texttt{laplace})$"),
    .before = Distance
  ) %>%
  kableExtra::kable(booktabs = TRUE, escape = FALSE, align = "c")
```

# Extension beyond empirical Bayes

In Sections \@ref(gaussian-marginal) and \@ref(full-laplace) when calculating $\tilde p(x_i \, | \, y)$ we fixed $\theta = \hat \theta$ ($K = 1$, empirical Bayes) rather than integrating $\theta$ out ($K > 1$).
In this section we will use `ahgq` to integrate over $\theta$.

## Marginal of joint Gaussian approximation {#aghq-gaussian-marginal}

We're going to start by normalising $\tilde p_\text{LA}(\theta, y)$.
We did this above in the empirical Bayes case, but found that it wasn't strictly necessary. 

```{r}
h$he <- function(theta) numDeriv::jacobian(h$gr, theta, method = "Richardson")
quad <- aghq::aghq(ff = h, k = 3, startingvalue = c(0, 0), control = aghq::default_control_tmb())
quad$normalized_posterior$lognormconst
h_norm <- function(theta) h$fn(theta) + quad$normalized_posterior$lognormconst
```

We can get the mode and Hessian of the random effects at each quadrature point as follows:

```{r}
distinctthetas <- quad$normalized_posterior$nodesandweights[, grep('theta', colnames(quad$normalized_posterior$nodesandweights))]
modesandhessians <- distinctthetas
thetanames <- make.unique(names(h$par), sep = "")
colnames(modesandhessians) <- thetanames

modesandhessians$mode <- vector(mode = "list", length = nrow(distinctthetas))
modesandhessians$H <- vector(mode = "list", length = nrow(distinctthetas))

for(i in 1:nrow(distinctthetas)) {
  theta <- as.numeric(modesandhessians[i, thetanames])
  h$fn(theta)
  mm <- h$env$last.par
  modesandhessians[i, "mode"] <- list(list(mm[h$env$random]))
  H <- h$env$spHess(mm, random = TRUE)
  H <- rlang::duplicate(H)
  modesandhessians[i, "H"] <- list(list(H))
}

modesandhessians$weights <- quad$normalized_posterior$nodesandweights$weights
```

Now we want to evaluate
$$
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) \omega(z).
$$

To start with, let's evaluate this on the log-scale for `i = 1`, `x = x_test_value` and the first value of `z`:

* The weight $\omega(z)$ will be `modesandhessians$weights[z]`
* The normalised Laplace approximation $\tilde p_\text{LA}(\theta(z) \, | \, y)$ will be `- h_norm(as.numeric(modesandhessians[z, thetanames]))`
* For the normalised Gaussian approximation $p_\text{G}(x_i \, | \, \theta(z), y)$ on the log-scale we get the relevant subcomponents of the joint Gaussian then use `ndorm` with `log = TRUE`

```{r}
x_test_value <- 0

#' Note that in some sense z actually runs over nodes, not indices
z <- 1 
i <- 1

mode_i <- as.numeric(modesandhessians$mode[[z]][i])
Sigma <- solve(modesandhessians$H[[z]])
sd_i <- as.numeric(sqrt(Sigma[i, i]))

dnorm(x_test_value, mean = mode_i, sd = sd_i, log = TRUE) + (- h_norm(theta)) + log(modesandhessians$weights[z])
```

Now let's sum over `z` so that we can obtain the log-probability for our `x_test_value` in marginal `i`:

```{r}
gaussian_marginal <- function(x) {
  lp <- vector(mode = "numeric", length = nrow(modesandhessians))

  for(z in 1:nrow(modesandhessians)) {
    theta <- as.numeric(modesandhessians[z, thetanames])
    mode_i <- as.numeric(modesandhessians$mode[[z]][i])
    Sigma <- solve(modesandhessians$H[[z]])
    sd_i <- as.numeric(sqrt(Sigma[i, i]))
    lp[z] <- dnorm(x, mean = mode_i, sd = sd_i, log = TRUE) + (- h_norm(theta)) + log(modesandhessians$weights[z])
  }

  matrixStats::logSumExp(lp)
}

gaussian_marginal(x_test_value)
```

To pick a set of $x_i$ to evaluate the marginal at, we will use an approximation to the AGHQ nodes.
We will take the hyperparameters to be fixed at their mode, then choose the appropriate marginals of the joint Gaussian:

```{r}
#' Create a function that takes nodesandhessians and outputs the spline nodes for a particular marginal
spline_nodes <- function(modesandhessians, i, k = 7) {
  #' Get the row number of modesandhessians which contains that theta mode
  #' This is bad code (relying on objects outside function)
  theta_mode_location <- which.max(quad$normalized_posterior$nodesandweights$logpost_normalized)
  
  #' These are the mode and standard deviation from the Gaussian approximation at the node which is the mode of the Laplace approximation
  #' (Say that three times fast...)
  mode_i <- modesandhessians[[theta_mode_location, "mode"]][i]
  sd_i <- sqrt(1 / modesandhessians[[theta_mode_location, "H"]][i, i])

  #' Create Gauss-Hermite quadrature
  gg <- mvQuad::createNIGrid(dim = 1, type = "GHe", level = k)

  #' Adapt to mode_i and sd_i
  mvQuad::rescale(gg, m = mode_i, C = sd_i^2)

  #' These are our set of x input values
  gg$nodes
}

#' The log-probabilities at the set of input values
nodes <- spline_nodes(modesandhessians, i = 1, k = 7)
lps <- sapply(nodes, gaussian_marginal)

plot_marginal_spline <- function(nodes, lps) {
  #' Build a Lagrange polynomial interpolant of the marginal posterior
  ss <- splines::interpSpline(nodes, lps, bSpline = TRUE, sparse = TRUE)
  interpolant <- function(x) { as.numeric(stats::predict(ss, x)$y) }
  finegrid <- seq(-5, 5, by = 0.1)
  df <- data.frame(x = finegrid, y = exp(interpolant(finegrid)))

  ggplot(df, aes(x = x, y = y)) +
    geom_line() +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior")
}

plot_marginal_spline(nodes, lps)
```

## Full Laplace approximation (old way)

As before, but with the full Laplace approximation in place of the Gaussian.
Much of what we computed above in Section \@ref(aghq-gaussian-marginal) is still relevant.
The only thing that has changed is that we're now using $\tilde p_\text{LA}(x_i \, | \, \theta(z), y)$ rather than $\tilde p_\text{G}(x_i \, | \, \theta(z), y)$ in
$$
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{LA}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) \omega(z).
$$

```{r}
x_test_value <- 0
i <- 1
z <- 1
```

As before, the weight $\omega(z)$ is `modesandhessians$weights[z]` and the normalised Laplace approximation $\tilde p_\text{LA}(\theta(z) \, | \, y)$ is `- h_norm(as.numeric(modesandhessians[z, thetanames]))`.
Getting to the difference, the normalised Laplace approximation $p_\text{LA}(x_i \, | \, \theta(z), y)$ on the log-scale is:

```{r}
dat_i <- prepare_dat(dat, i)
map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))
  
param_i_fixed_theta <- list(
  beta_prev = 0, 
  phi_prev_i = 0, 
  phi_prev_minus_i = rep(0, data$n - 1),
  log_sigma_phi_prev = 0
)

for(theta_string in thetanames) param_i_fixed_theta[[theta_string]] <- modesandhessians[z, thetanames][[theta_string]]

obj_fixed_theta <- MakeADFun(
  data = dat_i,
  parameters = param_i_fixed_theta,
  random = "phi_prev_minus_i",
  DLL = "model1_index",
  silent = TRUE,
  map = map_fixed_theta
)

#' Note that this is only a one-dimensional marginal, so perhaps we can afford to put k > 3 here
quad <- aghq::aghq(ff = obj_fixed_theta, k = 7, startingvalue = 0, control = aghq::default_control_tmb())

obj_fixed_theta_norm <- function(x) obj_fixed_theta$fn(x) + quad$normalized_posterior$lognormconst

- obj_fixed_theta_norm(x_test_value)
```

Putting it together on the log-scale:

```{r}
as.numeric(- obj_fixed_theta_norm(x_test_value)) + (- h_norm(theta)) + log(modesandhessians$weights[z])
```

Sum over `z`:

```{r}
old_laplace_marginal <- function(x) {
  lp <- vector(mode = "numeric", length = nrow(modesandhessians))

  for(z in 1:nrow(modesandhessians)) {
    theta <- as.numeric(modesandhessians[z, thetanames])
  
    dat_i <- prepare_dat(dat, i)
    map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))
  
    param_i_fixed_theta <- list(
      beta_prev = 0, 
      phi_prev_i = 0, 
      phi_prev_minus_i = rep(0, data$n - 1),
      log_sigma_phi_prev = 0
    )

    for(theta_string in thetanames) param_i_fixed_theta[[theta_string]] <- modesandhessians[z, thetanames][[theta_string]]

    obj_fixed_theta <- MakeADFun(
      data = dat_i,
      parameters = param_i_fixed_theta,
      random = "phi_prev_minus_i",
      DLL = "model1_index",
      silent = TRUE,
      map = map_fixed_theta
    )
    
    quad <- aghq::aghq(ff = obj_fixed_theta, k = 7, startingvalue = 0, control = aghq::default_control_tmb())
    obj_fixed_theta_norm <- function(xx) obj_fixed_theta$fn(xx) + quad$normalized_posterior$lognormconst
    
    lp[z] <- as.numeric(- obj_fixed_theta_norm(x)) + (- h_norm(theta)) + log(modesandhessians$weights[z])
  }

  matrixStats::logSumExp(lp)
}

old_laplace_marginal(x_test_value)

lps <- sapply(nodes, old_laplace_marginal)
plot_marginal_spline(nodes, lps)
```

## Full Laplace approximation (new way)

The method above should work, but it requires a lot of computation.
Instead, we might be able to do this more cost effectively by integrating $\theta$ out *before* normalising $\tilde p_\text{LA}(x_i, \theta(z), y)$.
This would look like first obtaining
$$
\tilde p(x_i, y) = \sum_z \tilde p_\text{LA}(x_i, \theta(z), y) \omega(z),
$$
then normalising by the previously obtained $\tilde p_\text{AQ}(y)$ as follows
$$
\tilde p(x_i \, | \, y) = \frac{\tilde p(x_i, y)}{\tilde p_\text{AQ}(y)}.
$$

```{r}
#' Make sure the log normalising constant is available
quad <- aghq::aghq(ff = h, k = 3, startingvalue = c(0, 0), control = aghq::default_control_tmb())
quad$normalized_posterior$lognormconst

laplace_marginal <- function(x) {
  lp <- vector(mode = "numeric", length = nrow(modesandhessians))

  for(z in 1:nrow(modesandhessians)) {
    theta <- as.numeric(modesandhessians[z, thetanames])
  
    dat_i <- prepare_dat(dat, i)
    map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))
  
    param_i_fixed_theta <- list(
      beta_prev = 0, 
      phi_prev_i = 0, 
      phi_prev_minus_i = rep(0, data$n - 1),
      log_sigma_phi_prev = 0
    )

    for(theta_string in thetanames) param_i_fixed_theta[[theta_string]] <- modesandhessians[z, thetanames][[theta_string]]

    obj_fixed_theta <- MakeADFun(
      data = dat_i,
      parameters = param_i_fixed_theta,
      random = "phi_prev_minus_i",
      DLL = "model1_index",
      silent = TRUE,
      map = map_fixed_theta
    )

    lp[z] <- as.numeric(- obj_fixed_theta$fn(x)) + log(modesandhessians$weights[z])
  }

  matrixStats::logSumExp(lp) - quad$normalized_posterior$lognormconst
}

laplace_marginal(x_test_value)

lps <- sapply(nodes, laplace_marginal)
plot_marginal_spline(nodes, lps)
```

## Full Laplace method (new new method)

Try doing this without a new `MakeADFun` for each $z$:

```{r}
new_laplace_marginal <- function(x, i) {
  dat_i <- prepare_dat(dat, i)
  
  param_i <- list(
    beta_prev = 0, 
    phi_prev_i = 0, 
    phi_prev_minus_i = rep(0, data$n - 1),
    log_sigma_phi_prev = 0
  )

  obj <- MakeADFun(
    data = dat_i,
    parameters = param_i,
    random = "phi_prev_minus_i",
    DLL = "model1_index",
    silent = TRUE
  )

  lp <- vector(mode = "numeric", length = nrow(modesandhessians))

  for(z in 1:nrow(modesandhessians)) {
    theta <- as.numeric(modesandhessians[z, thetanames])
    lp[z] <- as.numeric(- obj$fn(c(theta[1], x, theta[2]))) + log(modesandhessians$weights[z])
  }

  matrixStats::logSumExp(lp) - quad$normalized_posterior$lognormconst
}
```

We get the same value as we did with `laplace_marginal`!

```{r}
new_laplace_marginal(x_test_value, i = 1)

lps <- sapply(nodes, new_laplace_marginal, i = 1)
plot_marginal_spline(nodes, lps)
```
Now let's loop this over $i$ so we can compare to the other methods.
The hypothesis is that this will be better than the empirical Bayes methods.

```{r}
spline_nodes(modesandhessians, i = 1)

new_laplace_marginal_spline_aghq <- function(i) {
  x <- spline_nodes(modesandhessians, i = i)
  lps <- sapply(x, new_laplace_marginal, i = i)
  ss <- splines::interpSpline(x, lps, bSpline = TRUE, sparse = TRUE)
  interpolant <- function(x) { as.numeric(stats::predict(ss, x)$y) }
  finegrid <- seq(-5, 5, by = 0.1)
  df <- data.frame(x = finegrid, y = exp(interpolant(finegrid)))
  return(df)
}

aghq_laplace_df <- lapply(1:dat$n, new_laplace_marginal_spline_aghq) %>%
  bind_rows(.id = "index") %>%
  mutate(index = as.numeric(index))

plot3 +
  geom_line(data = aghq_laplace_df, aes(x = x, y = y), col = cbpalette[3])

#' Need to edit this so that there are cdf and theta columns in aghq_laplace_df
# samples_aghq_laplace <- lapply(1:dat$n, function(i) {
#   aghq_laplace_df_i <- filter(aghq_laplace_df, index == i)
#   samples <- approxfun(x = aghq_laplace_df_i$cdf, y = aghq_laplace_df_i$theta, ties = "ordered")(runif(M))
#   data.frame("value" = samples)
# }) %>%
#   bind_rows(.id = "index")
```

<!-- # Simplified INLA -->

<!-- In Section \@ref(full-laplace) we directly calculate the Laplace approximation to $\int p(x, y, \theta) \text{d} \theta_{-i}$. -->
<!-- Suppose that -->
<!-- $$ -->
<!-- x \sim \mathcal{N}(\hat x(\theta), H(\theta)^{-1}) -->
<!-- $$ -->
<!-- then by the conditional distributions of a multivariate Gaussian distribution $x_{-i} \, | \, x_i$ is Gaussian with mode -->
<!-- $$ -->
<!-- \hat x(\theta)_{-i} + \Sigma(\theta)_{-i, i} \Sigma(\theta)^{-1}_{i, i} (x_i - \hat x (\theta)_i) (\#eq:conditionalmode) -->
<!-- $$ -->
<!-- and precision matrix -->
<!-- $$ -->
<!-- H(\theta)_{-i, -i} (\#eq:conditionalprecision) -->
<!-- $$ -->

<!-- We follow Wood (2019) Section 2. -->
<!-- In this section we omit dependence on $\theta$ such that: $\hat x = \hat x(\theta)$, $H = H(\theta)$, $\hat x_{-i}(x_i) = \hat x_{-i}(x_i, \theta)$ and $H(x_i) = H_{-i, -i}(x_i, \theta)$. -->

<!-- Let $\mathcal{D}_{2j}$ denote a $2j \times 2j$ diagonal matrix with leading diagonal $-1, 1, -1, 1, \ldots$. -->
<!-- For example, with $j = 3$ we have: -->

<!-- ```{r} -->
<!-- D2j <- function(j) { -->
<!--   diag((-1)^{1:2 * j}, nrow = 2 * j) -->
<!-- } -->

<!-- D2j(3) -->
<!-- ``` -->

<!-- Let $u_0$ be a zero column matrix. -->
<!-- I am guessing that this means $u_j$ is a column matrix containing $j$ as each entry. -->
<!-- It's not specified what dimension $u_0$ or $u_j$ should be, but since the algorithm contains producing by $\mathcal{D}_{2j}$ presumably their lengths must be $2j$. -->

<!-- ```{r} -->
<!-- uj <- function(j) { -->
<!--  matrix(j, nrow = 2 * j, ncol = 1) -->
<!-- } -->

<!-- uj(3) -->
<!-- ``` -->

<!-- The algorithm contains $u_j \mathcal{D}_{2j} u_j^\top$. -->
<!-- If $u_j$ is a column matrix of dimension $\text{length}(u_j) \times 1$ then it's not going to be conformable with a $2j \times 2j$ matrix. -->
<!-- Is it actually a row matrix of dimension $1 \times \text{length}(u_j)$? -->
<!-- And $\text{length}(u_j) = 2j$ as discussed above? -->

<!-- Let $\Delta_j^i$ denote $\Delta_j$ with an extra zero inserted at element $i$. -->
<!-- But it hasn't been defined what $\Delta_j$ is yet. -->

<!-- ## Step 1: Compute the Cholesky factor -->

<!-- The first task is to "compute the Cholesky factor of $H_0 := H_{-i, -i}$ by update of the Cholesky factor of $H$". -->

<!-- ```{r} -->
<!-- i <- 1 -->

<!-- mm <- h$env$last.par -->
<!-- mode <- mm[h$env$random] -->
<!-- H <- h$env$spHess(mm, random = TRUE) -->
<!-- H0 <- H[-i, -i]  -->

<!-- L0 <- chol(H0) -->
<!-- ``` -->

<!-- We have that $H_0 = L_0 {L_0}^\top$: -->

<!-- ```{r} -->
<!-- sum(abs((L0 %*% t(L0)) - H0)) < 10E-9 -->
<!-- ``` -->

<!-- ## Step 2. Loop -->

<!-- For each $x_i$ in a grid of evaluation points (how does one chose this grid?) repeat the following: -->

<!-- ```{r} -->
<!-- x_grid -->
<!-- ``` -->

<!-- ### a) -->

<!-- Use Newton's method with fixed Hessian $H_{-i, -i}$ to find $\hat x_{-i}(x_i)$ starting from \@ref(eq:conditionalmode). -->
<!-- We start by computing the conditional mode for the first grid point: -->

<!-- ```{r} -->
<!-- Sigma <- solve(H) -->
<!-- conditional_mode <- mode[-i] + Sigma[-i, i] * Sigma[i, i]^{-1} * (x_grid[1] - mode[i]) -->
<!-- ``` -->

<!-- Note that for this model the latent field is IID, so the conditional mode is the same as the mode. -->
<!-- Now we use Newton's method to find $\hat x_{-i}(x_i)$. -->
<!-- This requires some function $f(\hat x_{-i}(x_i), \ldots) = 0$. -->

<!-- ### b) -->

<!-- Compute a set of $J$ steps $\{\Delta_j\}$ -->

<!-- ```{r} -->
<!-- J <- 1 -->
<!-- ``` -->

<!-- ### c) -->

<!-- ```{r} -->
<!-- j <- 1 -->
<!-- ``` -->

<!-- For $j = 1, \ldots, J - 1$ compute -->
<!-- $$ -->
<!-- q = H_0 \Delta_j + u_j \mathcal{D}_{2j}u_j^\top \Delta_j -->
<!-- $$ -->
<!-- and -->
<!-- $$ -->
<!-- g = \Delta_x \log p(\hat x_{-i}(x_i) + \Delta_j^i, y, \theta). -->
<!-- $$ -->
<!-- Then compute the matrix -->
<!-- $$ -->
<!-- u_{j + 1} = \{q(\Delta_j^\top j)^{-1/2}, g_{-i}(\Delta_j^\top g_{-i})^{-1/2}, u_j\}. -->
<!-- $$ -->

<!-- ### d) -->

<!-- Compute the determinant approximation -->
<!-- $$ -->
<!-- \det(H_1) = \det(H_0) \det(I_{2J} + u_J^\top H_0^{-1} u_J \mathcal{D}_{2j}). -->
<!-- $$ -->

<!-- ### e) -->

<!-- Compute $\tilde p(x_i \, | \, \theta, y) = \ldots$. -->

<!-- ## Step 3. Renormalise $\tilde p(x_i \, | \, \theta, y)$ -->

<!-- Renormalise $\tilde p(x_i \, | \, \theta, y)$. -->

# Appendix {#appendix}

## Resources

* [INLA and other approaches to GAMs](https://www.maths.ed.ac.uk/~swood34/inla-etc.pdf)
* [mgcv::ginla](https://github.com/cran/mgcv/blob/d9fe307c8ed1f8f1ed436c85e4ac415c1febeef4/R/inla.r)
* [How important are Cholesky factorisations for performing computations with large Gaussians?](https://dpsimpson.github.io/pages/talks/Krylov_SUQ_2013.pdf)

## Comparison of notation

| Description | Here          | Wood (2019) notation | Notes    |
|:------------|:--------------|:---------------------|:---------|
| Random effects | $x$        | $\beta$              | Stringer (2021) use $w$
| Random effects which maximise (the / some) objective for given $\theta$ | $\hat x (\theta)$ | $\hat \beta$ | Wood omits dependence on $\theta$ |
| Random effects which maximise (the / some) objective for given $\theta$, subject to $x_i$ being fixed | $(x_i, \hat x_{-i}(x_i, \theta))$ | $\tilde \beta$ | We have $\hat x_{-i}(x_i, \theta))$ having dimension $n - 1$ |
| Hessian / precision matrix of the negative log-likelihood with respect to $x$ at $\hat x(\theta)$ | $H(\theta)$ | $H$ | Unsure if it's preferable to use Hessian or precision notation here |

## `model1.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1.cpp')}
```

## `model1_index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1_index.cpp')}
```

# Original computing environment

```{r}
sessionInfo()
```
