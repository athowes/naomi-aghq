---
title: "Implementing simplified INLA"
author:
- name: Adam Howes
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** Wood (2020) simplify the INLA method by approximating latent field posterior marginals using a reduced cost Laplace approximation which does not rely on sparsity assumptions.
  **Task** We implement the method in R making use of Template Model Builder for Gaussian approximations.
---

We are interested in efficiently approximating latent field posterior marginals $p(x_i \, | \, \theta, y) \approx \tilde p(x_i \, | \, \theta, y)$ for each index $i$.
To build up to implementing the simplified INLA approach, we will start with two simpler methods (1) taking the marginal of a joint Gaussian approximation (2) doing a full Laplace approximation.

The AGHQ/ELGM approximation to the joint posterior is
$$
\tilde p(x \, | \, y) = \sum_z \tilde p_\text{G}(x \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z).
$$
This can be evaluated at any point, though in practice we work with it by drawing samples $x \sim p_\text{G}(x \, | \, \theta, y)$ using a method from sampling from a Gaussian given its mean and precision from Rue.
For the $i$th marginal, we just take the appropriate samples from the joint.
The approximation is therefore
$$
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z),
$$
where $p_\text{G}(x_i \, | \, \theta, y)$ just takes the $i$th elements of the mean vector and $(i, i)$th element of the covariance matrix.

`aghq::marginal_laplace_tmb` will calculate an object representing $\tilde p(x \, | \, \theta, y)$.
Then `aghq::sample_marginal` will draw samples from it.

We want to create a new approximation swapping out $\tilde p_\text{G}(x_i \, | \, \theta, y)$ from the above for the simplified INLA approximation
$$
\tilde p(x_i \, | \, \theta, y) = \sum_z \tilde p_{\text{SINLA}}(x_i \, | \, \theta, y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z).
$$

# Marginal of joint Gaussian approximation

The `aghq` package currently approximates latent field posterior marginals by taking marginals of the joint Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$.
This is same approach used by `method = "gaussian"` in `R-INLA`.
The Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$ is previously used in computing the Laplace approximation to $p(\theta \, | \, y)$
$$
\tilde p(\theta \, | \, y) \propto \frac{p(x, \theta, y)}{p_\text{G}(x \, | \, \theta, y)} \Big\rvert_{x = \hat{\mu}(\theta)} = \frac{p(y, \hat{\mu}(\theta), \theta)}{\det(2 \pi \hat{Q}(\theta))^{1/2}},
$$
such that one easy way to calculate it is to use set `random = "x"` in the `TMB` template to integrate the latent field out, then recover the Gaussian approximation from there.

We begin by loading in testing data, as generated by [`prev-anc-art_sim`](https://github.com/athowes/elgm-inf/tree/master/src/prev-anc-art_sim).
`sim_data` contains many replicates of simulated data, so we just take the first one:

```{r}
sim_data <- readRDS("depends/sim_data.rds")
data <- sim_data[[1]]
dat <- list(n = data$n, y_prev = data$y_prev, m_prev = data$m_prev)
dat
```

We create an objective function `f` (see Appendix for `TMB` template) to evaluate $p(x, \theta, y)$ (without any Laplace approximations!):

```{r message=FALSE, warning=FALSE}
param <- list(
  beta_prev = 0,
  phi_prev = rep(0, data$n),
  log_sigma_phi_prev = 0
)

compile("model1.cpp")
dyn.load(dynlib("model1"))

f <- MakeADFun(
  data = dat,
  parameters = param,
  DLL = "model1"
)
```

We also create an objective function `h` (see Appendix for `TMB` template) integrating the random effects $x$ (which here correspond to the spatial random effects `phi_prev`):

```{r message=FALSE, warning=FALSE}
h <- MakeADFun(
  data = dat,
  parameters = param,
  random = "phi_prev",
  DLL = "model1"
)
```

# Full Laplace approximation

Another approach we might take is to calculate the full Laplace approximation
$$
\begin{align*}
  \tilde p(x_i \, | \, \theta, y) &\propto \frac{p(x, \theta, y)}{p_\text{G}(x_{-i} \, | \, x_i, \theta, y)} \Big\rvert_{x_{-i} = \hat{\mu}_{-i}(x_i, \theta)} \\
  &= \frac{p(y, x_i, \hat{\mu}_{-i}(x_i, \theta), \theta)}{\det(2 \pi \hat{Q}_{-i}(x_i, \theta))^{1/2}}
\end{align*}
$$
This is not practical in some settings as it involves recomputing the Gaussian approximation $p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ for each value of $(x_i, \theta)$ which can be too computationally expensive.
With of a bit of effort, we can implement this in `TMB` by passing in data where elements $i$ and $-i$ of the latent field are named as such (see Appendix for template).

```{r}
compile("model1index.cpp")
dyn.load(dynlib("model1index"))
```

The `prepare_dat` function takes `dat` and makes it compatible with `model1index` for particular choice of marginal `i`.

```{r}
prepare_dat <- function(dat, i) {
  dat[["y_prev_i"]] <- dat$y_prev[i]
  dat[["y_prev_minus_i"]] <- dat$y_prev[-i]
  dat[["m_prev_i"]] <- dat$m_prev[i]
  dat[["m_prev_minus_i"]] <- dat$m_prev[-i]
  dat[c("n", "y_prev_i", "y_prev_minus_i", "m_prev_i", "m_prev_minus_i")]
}
```

Now loop over doing the Laplace approximation for every index.
In the call to `MakeADFun` we set `random = "phi_prev_minus_i"` to integrate all of the random effects but that of the particular index.

```{r message=FALSE, warning=FALSE, results=FALSE}
#' To store obj
template <- list() 

for(i in 1:dat$n) {
  dat_i <- prepare_dat(dat, i)

  #' Starting parameters for TMB are the same for every index of the loop
  param_i <- list(
    beta_prev = 0,
    phi_prev_i = 0,
    phi_prev_minus_i = rep(0, data$n - 1),
    log_sigma_phi_prev = 0
  )

  #' random are integrated out with a Laplace approximation
  obj <- MakeADFun(
    data = dat_i,
    parameters = param_i,
    random = "phi_prev_minus_i",
    DLL = "model1index"
  )

  its <- 1000

  opt <- nlminb(
    start = obj$par,
    objective = obj$fn,
    gradient = obj$gr,
    control = list(iter.max = its, trace = 0)
  )

  sd_out <- sdreport(
    obj,
    par.fixed = opt$par,
    getJointPrecision = TRUE
  )

  template[[i]] <- obj
}
```

Let's consider the first marginal, that is $\tilde p_\text{LA}(x_1 \, | \, \theta, y)$.
The mean and precision matrix for the Gaussian approximation $\tilde p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ are:

```{r message=FALSE, warning=FALSE}
i <- 1
obj <- template[[i]]

mean <- with(obj$env, last.par[random])
length(mean) #' The length is 30 - 1
mean

Q <- obj$env$spHess(with(obj$env, last.par), random = TRUE)
dim(Q) #' With dimension [(30 - 1) x (30 - 1)]
image(Q)
```

`obj$fn` should take as input $(x_i, \theta)$:

```{r}
transform_param <- function(param) {
  list(
    beta_prev = param$beta_prev,
    phi_prev_i = param$phi_prev[i],
    phi_prev_minus_i = param$phi_prev[-i],
    log_sigma_phi_prev = param$log_sigma_phi_prev
  )
}

transformed_param <- transform_param(param)
transformed_param$phi_prev_minus_i <- NULL #' Remove the integrated out effects
transformed_param

obj$fn(transformed_param)
```

We can reproduce the output of `obj$fn` using the mean and precision matrix, together with the function `f`.
Recall that `f` takes as input the complete $(x, \theta)$ ($y$ is already fixed).
For example:

```{r}
f$fn(unlist(param))
```

We update `param` to include $\hat{\mu}_{-i}(x_i, \theta)$:

```{r}
param$phi_prev[-i] <- mean
param

```

Such that the numerator of the Laplace approximation is (n.b. I think this is on the log-scale, which is where we will do the calculations):

```{r}
numer <- f$fn(unlist(param))
numer
```

And the denominator of the Laplace approximation (on the log-scale) is:

```{r}
denom <- log(sqrt(det(2 * pi * Q)))
denom
```

```{r}
numer - denom
```

Compare this to the output from `obj`:

```{r}
obj$fn(transformed_param)
```

# Simplified INLA

$$
\hat \mu_{-i}(x_i, \theta) = [\hat \mu(\theta)]_{-i} + \hat Q(\theta)^{-1}_{-i, i} \hat Q(\theta)_{i, i}(x_i - [\hat \mu(\theta)]_{i})
$$

# Appendix

## `model1.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1.cpp')}
```

## `model1index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1index.cpp')}
```
