---
title: "Implementing simplified INLA into `aghq`"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** Wood (2020) simplify the INLA method by approximating the latent field posterior marginals using a reduced cost Laplace approximation which does not rely on sparsity assumptions. In spatio-temporal statistics, usually the latent field corresponds to spatio-temporal locations -- so these marginal posteriors are of central scientific interest.
  
  **Task** We implement the method in R making use of Template Model Builder (`TMB`) for Gaussian approximations, in a way compatible with the `aghq` package for adaptive Gauss-Hermite quadrature. We are interested in efficiently approximating latent field posterior marginals $p(x_i \, | \, \theta, y) \approx \tilde p(x_i \, | \, \theta, y)$ for each index $i$. To build up to implementing the simplified INLA approach, we will start with two simpler methods (1) taking the marginal of a joint Gaussian approximation (2) doing a full Laplace approximation.
---

```{r echo=FALSE}
cbpalette <- multi.utils::cbpalette()
```

# Background

The AGHQ/ELGM approximation to the joint posterior of the latent field is the following object
$$
\begin{equation}
\tilde p(x \, | \, y) = \sum_z \tilde p_\text{G}(x \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z), (\#eq:pxgiveny)
\end{equation}
$$
where $\theta(z) \in \Theta$ and $w_k(z) \in \mathbb{R}$ are AGHQ nodes and weights generated by "adaption" to $\tilde p_\text{LA}(\theta \, | \, y)$.
In particular, let
$$
\mathcal{Q}(1, k) = \{z_d \in \mathbb{R}: H_k(z_d) = 0\}
$$
be the set of nodes in one-dimension, exactly the zeros of the Hermite polynomials $H_k(z_d)$ for $k \in \mathbb{N}$, and
$$
w_k(z_d) = \frac{k!}{H_{k + 1}(z_d)^2 \times \phi(z_d)}, 
$$
be the associated weights.
Define $\mathcal{Q}(d, k) = \mathcal{Q}(1, k)^d$ to be the set of nodes $z$ in $d$-dimensions, of which there will be $k^d$.
Then these points are adapted via $\theta(z) = \hat \theta + Lz$, where $\hat \theta$ is the mode (of $p_\text{LA}(\theta \, | \, y)$) and $L$ is the lower Cholesky triangle of the curvature (at the mode) such that $LL^\top = H^{-1}$.

This can be evaluated at any point, though in practice we work with it by drawing samples $x \sim p_\text{G}(x \, | \, \theta, y)$ using a method from sampling from a Gaussian given its mean and precision from Rue (in particular you solve a linear equation using the lower Cholesky triangle of the curvature at a randomly chosen quadrature point).
For the $i$th marginal, we just take the appropriate samples from the joint.
The approximation is therefore
$$
\begin{equation}
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z), (\#eq:pxigiveny)
\end{equation}
$$
where $p_\text{G}(x_i \, | \, \theta, y)$ just takes the $i$th elements of the mean vector and $(i, i)$th element of the covariance matrix.

To understand how this is implemented in `aghq`, the two most important functions are `aghq::marginal_laplace_tmb` and `aghq::sample_marginal`.
`aghq::marginal_laplace_tmb` calculates an object representing $\tilde p(x \, | \, \theta, y)$.
`aghq::sample_marginal` draws samples from this Gaussian, given values of $z$ drawn from a multinomial distribution, in order to evaluate Equation \@ref(eq:pxigiveny).
For a line-by-line walkthrough of how these functions work, see the notebook [Code walkthrough](https://athowes.github.io/elgm-inf/walkthrough.html) (which uses the model from the notebook [Epilepsy example](https://athowes.github.io/elgm-inf/epil.html)).

The goal of this notebook ([Implementing simplified INLA into `aghq`](https://athowes.github.io/elgm-inf/sinla.html)) is to create a new approximation swapping out $\tilde p_\text{G}(x_i \, | \, \theta, y)$ from the above for the simplified INLA approximation $p_{\text{SINLA}}(x_i \, | \, \theta, y)$ as follows
$$
\tilde p(x_i \, | \, \theta, y) = \sum_z \tilde p_{\text{SINLA}}(x_i \, | \, \theta, y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z).
$$
We want to do this because the new approximation should be more accurate than taking marginals of a joint Gaussian approximation.
At the extreme, you could just compute a Laplace approximation for each marginal, integrating out all other elements of the latent field, $x_{-i}$, together with the fixed effects $\theta$.
This would be an accurate approach, but not computationally feasible for large models.
What we hope to achieve is an approximation which gets to most of the accuracy benefits of the Laplace approximation approach, but without the high computational cost.

# Ground truth {#ground-truth}

We will use Model 1 from the [Prevalence, ANC, ART model inference case-study](https://athowes.github.io/elgm-inf/prev-anc-art.html) as a running example.
To provide a ground truth to compare the methods that follow to, we generate "gold-standard" inferences using a relatively long run of MCMC as implemented by `tmbstan`.

We begin by loading in testing data, as generated by [`prev-anc-art_sim`](https://github.com/athowes/elgm-inf/tree/master/src/prev-anc-art_sim).
`sim_data` contains many replicates of simulated data.
Here we only need one, so will will just take the first as given by:

```{r}
sim_data <- readRDS("depends/sim_data.rds")
data <- sim_data[[1]]
dat <- list(n = data$n, y_prev = data$y_prev, m_prev = data$m_prev)
dat
```

Compile and load the template (see Appendix \@ref(appendix) for the `TMB` code):

```{r}
compile("model1.cpp")
dyn.load(dynlib("model1"))
```

We create an objective function `h` integrating the random effects $x$ (which here correspond to the spatial random effects `phi_prev`).
The values of `param` intitialise the objective function:

```{r}
param <- list(
  beta_prev = -2,
  phi_prev = rep(0, data$n),
  log_sigma_phi_prev = -1
)

h <- MakeADFun(
  data = dat,
  parameters = param,
  random = "phi_prev",
  DLL = "model1",
  silent = TRUE
)
```

It is simple to fit the model using MCMC via the `tmbstan` package.
We use 4 `chains` each of 5000 `iter`.
Note that although `h` is passed to `tmbstan::tmbstan`, the Laplace approximation is disabled by setting `laplace = FALSE` by default (useful to know in case it's ever of interest to sample from the Laplace approximation).

```{r}
mcmc <- tmbstan::tmbstan(h, chains = 4, iter = 5000, refresh = 0)
```

The posterior marginal $p(\phi_1 \, | \, y)$ can be examined by extracting the corresponding Markov chains, and plotting a histogram of the draws:

```{r message=FALSE, warning=FALSE}
samples_tmbstan <- extract(mcmc, pars = "phi_prev[1]")[[1]]

plot_pdf <- ggplot(data.frame(x = samples_tmbstan), aes(x = x)) +
    geom_histogram(aes(y = ..density..), alpha = 0.8, fill = cbpalette[7]) +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior PDF")

ecdf_tmbstan <- ecdf(samples_tmbstan)
grid <- seq(min(samples_tmbstan), max(samples_tmbstan), by = 0.1)

plot_cdf <- data.frame(x = grid, y = ecdf_tmbstan(grid)) %>%
  ggplot(aes(x = x, y = y)) +
    geom_line(col = cbpalette[7]) +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior CDF")

cowplot::plot_grid(plot_cdf, plot_pdf)
```

# Marginal of joint Gaussian approximation {#gaussian-marginal}

Now we will compute Equation \@ref(eq:pxgiveny) taking the marignals of a joint Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$.
This is the approach currently used in the `aghq` package (done implicitly, via sampling).
This approach is also used by `method = "gaussian"` in `R-INLA`.

## $\tilde p(\theta \, | \, y)$ 

The Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$ is previously used in computing the Laplace approximation to $p(\theta \, | \, y)$
$$
\begin{equation}
\tilde p(\theta \, | \, y) \propto \frac{p(x, \theta, y)}{p_\text{G}(x \, | \, \theta, y)} \Big\rvert_{x = \mu(\theta)} = \frac{p(y, \mu(\theta), \theta)}{\det \left(\frac{1}{2 \pi} Q(\theta) \right)^{1/2}}, (\#eq:pthetagiveny)
\end{equation}
$$
such that one easy way to calculate it is to set `random = "x"` in the `TMB` template to integrate the latent field out, then recover the Gaussian approximation from there, as we have done in the function `h` above.

Let's optimise `h` to obtain estimates $\hat \theta$ which maximise $\tilde p(\theta \, | \, y)$:

```{r}
its <- 1000

opt_theta <- nlminb(
  start = h$par,
  objective = h$fn,
  gradient = h$gr,
  control = list(iter.max = its, trace = 0)
)
```

The values `opt_theta$par` below correspond to $\hat \theta$:

```{r}
opt_theta$par
```

We can also look at the parameters -- that is, mean vector and precision matrix -- of the Gaussian approximation at the mode `opt_theta$par` above.
Note that in `aghq`, what we would do is create a quadrature grid, then get the mean vector and precision matrix for each value of the quadrature grid over $\theta$.
By just sticking to the mode of $\theta$, we are effectively doing `aghq` with $k = 1$ in each dimension, i.e. one grid point, corresponding to empirical Bayes type of inference.

```{r}
#' Notice that last.par contains opt_theta$par as the fixed effects
#' It's the last.par because the last thing we've done with the template is the optimsation
#' This seems a pretty shakey way to go about things, but here we are...
mm <- h$env$last.par

#' These are the indices of the random effects that we want
h$env$random

#' Note that I'm calling this mean, but because it's a Gaussian the mean is the same as the mode
x_mean <- mm[h$env$random]
x_mean <- rlang::duplicate(x_mean) #' Insurance...
length(x_mean) #' The length is 30

#' Hessian evaluated at the mean (or equivalently mode)
x_Q <- h$env$spHess(mm, random = TRUE)
x_Q <- rlang::duplicate(x_Q)
dim(x_Q) #' With dimension [30 x 30]
```

## Additional checks {.tabset .tabset-fade}

### Description {-}

These tabs contain additional sanity checks used to build understanding.
Feel free to skip them.

### Reproducing output from `h` {-}

We would like to reproduce `h` using the right hand side of Equation \@ref(eq:pthetagiveny).
To do this, we start by creating an objective function `f` (see Appendix \@ref(appendix) for the `TMB` code) to evaluate $- \log p(x, \theta, y)$ (without any Laplace approximations!).
This is what we will be using for the numerator of Equation \@ref(eq:pthetagiveny):

```{r}
f <- MakeADFun(
  data = dat,
  parameters = param,
  DLL = "model1",
  silent = TRUE
)
```

Given $\theta$, we compute the `mean` and `Q` of $x$ (corresponding to `phi_prev`) using `h`.
We then concatenate and evaluate using `f` to get the numerator, multiplying by -1 because `f$fn` gives the negative log-likelihood, and we want the log-likelihood.
In the denominator, we use `determinant` on the log-scale as it is more numerically stable than e.g. `log(det(...))`.

```{r}
calculate_rhs <- function(theta) {
  #' Calculate mean and Q for those theta
  #' TMB template remembers what you've called -- spooky
  h$fn(theta)
  mm <- h$env$last.par
  mean <- mm[h$env$random]
  Q <- h$env$spHess(mm, random = TRUE)
  
  #' Append the calculated latent field mean
  param <- theta
  param$phi_prev <- as.numeric(mean)
  param <- param[c("beta_prev", "phi_prev", "log_sigma_phi_prev")]
  
  #' Evaluate the log-numerator of the RHS
  numer <- -1 * f$fn(unlist(param))
  
  #' Evaluate the log-denominator of the RHS
  denom <- 0.5 * (determinant(Q, logarithm = TRUE)$modulus - (nrow(Q) * log(2 * pi)))
  
  #' Return their difference
  numer - denom
}

theta <- param
theta$phi_prev <- NULL #' Setting the value of x to be NULL

calculate_rhs(theta)
```

As compared with what we got from `h`:

```{r}
-1 * h$fn(unlist(theta))
```

## $\tilde p(x_i \, | \, y)$

Now let's have a go at computing Equation \@ref(eq:pxigiveny) so that we can compare to the posterior distribution from `tmbstan` we obtained in Section \@ref(ground-truth).
As described above, we simplify matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{G}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{G}(x_i \, | \, \hat \theta, y)
\end{align}
$$

We can evaluate this for $i = 1$ via:

```{r}
i <- 1
mean_i <- as.numeric(x_mean[i])
var_i <- solve(x_Q)[i, i]
```

Because this is a Gaussian distribution, we do not have do normalise it (as would usually be the case) because we know it's a density.
We can compare it to our MCMC results as follows:

```{r message=FALSE, warning=FALSE}
plot_pdf2 <- plot_pdf +
  stat_function(
    data = data.frame(x = c(0.5, 3.5)), aes(x, col = "Joint Gaussian marginal"), fun = dnorm, n = 101, 
    args = list(mean = mean_i, sd = sqrt(var_i))
  ) +
  labs(x = "prev_phi[1]", y = "Posterior", col = "Approximation") +
  scale_colour_manual(values = cbpalette) +
  theme_minimal()

M <- 1000
samples_gaussian <- rnorm(M, mean_i, sqrt(var_i)) 

ecdf_gaussian <- ecdf(samples_gaussian)
grid <- seq(min(samples_gaussian), max(samples_gaussian), by = 0.1)

plot_cdf2 <- plot_cdf +
  geom_line(
    data = data.frame(x = grid, y = ecdf_gaussian(grid)),
    aes(x = x, y = y), col = cbpalette[1]
  )

cowplot::plot_grid(plot_cdf2, plot_pdf2, rel_widths = c(1, 1.5))
```

```{r}
ks_gaussian_tmbstan <- inf.utils::ks_test(samples_tmbstan, samples_gaussian)
ks_gaussian_tmbstan
```

The maximum difference between the two ECDFS is `r ks_gaussian_tmbstan$D` which occurs at the point `r ks_gaussian_tmbstan$l`.

# Full Laplace approximation {#full-laplace}

In this section we will continue to use the approximation \@ref(eq:pthetagiveny) for $\tilde p(\theta \, | \, y)$.

## For one index $i$

### $\tilde p(x_i \, | \, \theta, y)$

For the approximation to the latent field marginal posteriors, another approach we could take is to calculate the full Laplace approximation
$$
\begin{align*}
  \tilde p(x_i \, | \, \theta, y) &\propto \frac{p(x, \theta, y)}{p_\text{G}(x_{-i} \, | \, x_i, \theta, y)} \Big\rvert_{x_{-i} = \mu_{-i}(x_i, \theta)} (\#eq:pxigiventhetay) \\
  &= \frac{p(y, x_i, \mu_{-i}(x_i, \theta), \theta)}{\det(\frac{1}{2 \pi} Q_{-i}(x_i, \theta))^{1/2}}
\end{align*}
$$
integrating out $x_{-i}$ with a Gaussian approximation.
This is not practical in some settings as it involves recomputing the Gaussian approximation $p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ for each value of $(x_i, \theta)$ which can be too computationally expensive.
We will still go through the steps for implementing it here though for demonstration.
With of a bit of effort, it can be implemented in `TMB` by passing in data where elements $i$ and $-i$ of the latent field are explicitly named (see Appendix \@ref(appendix) for the `TMB` code):

```{r}
compile("model1index.cpp")
dyn.load(dynlib("model1index"))
```

The `prepare_dat` function takes `dat` and makes it compatible with `model1index` for particular choice of marginal `i`.

```{r}
prepare_dat <- function(dat, i) {
  dat[["y_prev_i"]] <- dat$y_prev[i]
  dat[["y_prev_minus_i"]] <- dat$y_prev[-i]
  dat[["m_prev_i"]] <- dat$m_prev[i]
  dat[["m_prev_minus_i"]] <- dat$m_prev[-i]
  dat[c("n", "y_prev_i", "y_prev_minus_i", "m_prev_i", "m_prev_minus_i")]
}
```

Let's consider the first marginal, that is $\tilde p_\text{LA}(x_1 \, | \, \theta, y)$.
In the call to `MakeADFun` we set `random = "phi_prev_minus_i"` to integrate all of the random effects but that of the particular index.

```{r message=FALSE, warning=FALSE, results=FALSE}
i <- 1

param_i <- list(
  beta_prev = 0,
  phi_prev_i = 0,
  phi_prev_minus_i = rep(0, data$n - 1),
  log_sigma_phi_prev = 0
)

dat_i <- prepare_dat(dat, i)

#' random are integrated out with a Laplace approximation
obj <- MakeADFun(
  data = dat_i,
  parameters = param_i,
  random = "phi_prev_minus_i",
  DLL = "model1index",
  silent = TRUE
)

opt <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr,
  control = list(iter.max = its, trace = 0)
)
```

The mean and precision matrix for the Gaussian approximation $\tilde p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ are:

```{r}
get_mean_and_Q <- function(obj) {
  mm <- obj$env$last.par
  mean <- mm[obj$env$random]
  mean <- rlang::duplicate(mean)
  Q <- obj$env$spHess(mm, random = TRUE)
  Q <- rlang::duplicate(Q)
  return(list(mean = mean, Q = Q))
}

xminusi <- get_mean_and_Q(obj)
```

We check here that `obj$fn` indeed takes as input $(x_i, \theta)$:

```{r}
transform_param <- function(param) {
  list(
    beta_prev = param$beta_prev,
    phi_prev_i = param$phi_prev[i],
    phi_prev_minus_i = param$phi_prev[-i],
    log_sigma_phi_prev = param$log_sigma_phi_prev
  )
}

transformed_param <- transform_param(param)
transformed_param$phi_prev_minus_i <- NULL #' Remove the integrated out effects
transformed_param

obj$fn(transformed_param)
```

### $\tilde p(x_i \, | \, y)$ {.tabset .tabset-fade}

Let's have a go at computing Equation \@ref(eq:pxigiveny) so that we can compare to the posterior distribution from `tmbstan`.
We simplify matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to^[Note that I'm uncertain if $\hat \theta$ (a constant) should still be included in the LHS of this.]:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{LA}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{LA}(x_i \, | \, \hat \theta, y)
\end{align}
$$

A basic way we could approach this is to define a grid of points that we would like to evaluate the posterior at.
We can then create a dataframe where each row contains the values `opt_theta$par` for $\theta$ and a grid value for $x_i$.
Each row can then be passed into `-obj$fn()` to calculate the log-posterior.
Note that it's crucial the arguments are in the right order when we pass them to `obj$fn()` -- this has caused headaches!

```{r}
spacing <- 0.05
x_grid <- seq(from = 0.5, to = 3.5, by = spacing)
x_grid_length <- length(x_grid)

df <- t(opt_theta$par) %>%
  as.data.frame() %>%
  slice(rep(1:n(), each = x_grid_length)) %>%
  mutate(phi_prev_i = x_grid) %>%
  rowwise() %>%
  mutate(logpost = -obj$fn(c(beta_prev, phi_prev_i, log_sigma_phi_prev)))
```

We've computed the unnormalised log-posterior -- how can we go from that to a normalised posterior?
Given these grid of points, we can do the trapezoid rule here.

#### Safe exponential trapezoid rule {-}

The safe exponential function computes $\exp(x - \text{max}(x))$.

```{r}
safe_exp <- function(x) exp(x - max(x))
```

A simple way to write the trapezoid rule is as follows:

```{r eval=FALSE}
trapezoid_rule <- function(x, spacing) {
  0.5 * spacing * (2 * sum(x) - x[1] - x[2])
}
```

It can also be defined like this, which is more convenient for thinking about it like other quadrature rules based on weights:

```{r}
trapezoid_rule <- function(x, spacing) {
  w <- rep(spacing, length(x))
  w[1] <- w[1] / 2
  w[length(x)] <- w[length(x)] / 2
  sum(w * x)
}
```

```{r}
df$post <- safe_exp(df$logpost)
norm_trapezoid <- trapezoid_rule(df$post, spacing = spacing)
df$post_norm <- df$post / norm_trapezoid
```

Check that this method correctly computes the normalising constant for a density (which is known to integrate to one):

```{r}
abs(trapezoid_rule(dnorm(seq(-10, 10, by = 0.1)), spacing = 0.1) - 1) < 10E-6
```

#### `logSumExp` trapezoid rule {-}

What about with another method?
We can calculate the logarithm of the normalising constant directly, then just subtract it from the log-posterior:

$$
\begin{align}
\tilde f(x) &= \frac{f(x)}{\int f(x) \text{d}x} = \frac{f(x)}{C}
\implies \\
\log\tilde f(x) &= \log f(x) - \log C
\end{align}
$$

The `matrixStats::logSumExp` function computes $\text{LSE}(x) = \log(\exp(x_1) + \cdots + \exp(x_n))$ in a computationally stable way.
Another way to compute the trapezoid rule, taking input on the log-scale, is as follows:

```{r}
trapezoid_rule_log <- function(x, spacing) {
  w <- rep(spacing, length(x))
  w[1] <- w[1] / 2
  w[length(x)] <- w[length(x)] / 2
  matrixStats::logSumExp(log(w) + x)
}

norm_trapezoid_log <- trapezoid_rule_log(df$logpost, spacing)
df$post_norm_alt <- exp(df$logpost - norm_trapezoid_log)
```

Check that this method correctly computes the normalising constant for a density (which is known to integrate to one):

```{r}
abs(trapezoid_rule_log(dnorm(seq(-10, 10, by = 0.1), log = TRUE), spacing = 0.1) - log(1)) < 10E-6
```

### Normalisation with `aghq`

The two ways of calculating the normalised posterior above are the same up to numerical error:

```{r}
max(abs(df$post_norm - df$post_norm_alt)) < 10E-6
```

So how might we approach this in a more sophisticated way?
We could do the normalisation directly in `aghq`!

We could start by normalising $\tilde p_\text{LA}(\theta, y)$.
Note that sometimes we have been calling this $\tilde p_\text{LA}(\theta \, | \, y)$ because they are equivalent up to a constant of proportionality.
Define
$$
\tilde p_\text{AQ}(\theta \, | \, y) = \frac{\tilde p_\text{LA}(\theta, y)}{\tilde p_\text{AQ}(y)}
$$
or on a log-scale
$$
\log \tilde p_\text{AQ}(\theta \, | \, y) = \log \tilde p_\text{LA}(\theta, y) - \log \tilde p_\text{AQ}(y)
$$

First we have to add a numerical Hessian that's missing from `h`:

```{r}
h$he <- function(theta) numDeriv::jacobian(h$gr, theta, method = "Richardson")
```

Now we can obtain $\log \tilde p_\text{AQ}(y)$:

```{r}
quad <- aghq::aghq(ff = h, k = 3, startingvalue = c(0, 0), control = aghq::default_control_tmb())
quad$normalized_posterior$lognormconst
```

Remember that `h` is the negative log-likelihood, so we want to add the normalising constant rather than subtract it:

```{r}
h_norm <- function(theta) h$fn(theta) - quad$normalized_posterior$lognormconst
```

Now we can put this into
$$
\tilde p(x_i, \hat \theta \, | \, y) = \tilde p_\text{LA}(x_i \, | \, \hat \theta, y) \tilde p_\text{AQ}(\hat \theta \, | \, y)
$$

But perhaps there is no reason to, because we will just have to normalise it again, so why do it twice?
Let's just try to normalise it directly.
What we need to do is create a function in `TMB` where we fix some of the inputs to the function.
This can be done using the `map` argument of `MakeADFun`.
You set each parameter to `factor(NA)` that you want to be fixed, then they will retain the value you input in `parameters`:

```{r}
obj$par %>% names()

map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))

param_i_fixed_theta <- param_i

theta_names <- names(opt_theta$par)
for(theta in theta_names) {
  param_i_fixed_theta[[theta]] <- opt_theta$par[[theta]]
}

dat_i <- prepare_dat(dat, i)

obj_fixed_theta <- MakeADFun(
  data = dat_i,
  parameters = param_i_fixed_theta,
  random = "phi_prev_minus_i",
  DLL = "model1index",
  silent = TRUE,
  map = map_fixed_theta
)
```

Now we can normalise this function $p_\text{LA}(x_i, \theta = \hat \theta, y)$ using `aghq`:

```{r}
quad <- aghq::aghq(ff = obj_fixed_theta, k = 3, startingvalue = 0, control = aghq::default_control_tmb())
```

The value of the normalising constant is (approximately) the same as we get using our trapezoid rule:

```{r}
abs(quad$normalized_posterior$lognormconst - norm_trapezoid_log) < 10E-3
```

### Comparison

We now add our Laplace approximation to the existing joint Gaussian marginal and MCMC results:

```{r message=FALSE, warning=FALSE}
plot_pdf3 <- plot_pdf2 +
  geom_line(data = df, aes(x = phi_prev_i, y = post_norm, col = "Laplace"))

df$ecdf <- cumsum(df$post_norm) * c(0, diff(df$phi_prev_i))

plot_cdf3 <- plot_cdf2 +
  geom_line(
    data = df, aes(x = phi_prev_i, y = ecdf), col = cbpalette[2]
  )

cowplot::plot_grid(plot_cdf3, plot_pdf3, rel_widths = c(1, 1.5))
```

We use the inverse CDF method to sample from the Laplace marginal, then we can calculate the KS statistic:

```{r}
samples_laplace <- approxfun(df$ecdf, df$phi_prev_i)(runif(M))

ks_laplace_tmbstan <- inf.utils::ks_test(samples_tmbstan, samples_laplace)

kable_data <- data.frame(
  "Distance" = c(ks_gaussian_tmbstan$D, ks_laplace_tmbstan$D),
  "Location" = c(ks_gaussian_tmbstan$l, ks_laplace_tmbstan$l)
)

rownames(kable_data) <- c("$\\text{KS}(\\texttt{tmbstan}, \\texttt{gaussian})$", "$\\text{KS}(\\texttt{tmbstan}, \\texttt{laplace})$")

kableExtra::kable(kable_data, booktabs = TRUE, escape = FALSE, align = "c")
```

## Looping over index $i$

So we've done the approximation for the first index $i = 1$!
Now let's try computing $\tilde p(x_i \, | \, y)$ for every index $i$, using a for-loop to generate the required Laplace approximations, and tidying up and condensing the code used:

```{r}
xi_laplace_marginal <- function(i) {
  dat_i <- prepare_dat(dat, i)
  map_fixed_theta <- list(beta_prev = factor(NA), log_sigma_phi_prev = factor(NA))
  
  param_i_fixed_theta <- list(
    beta_prev = 0, 
    phi_prev_i = 0, 
    phi_prev_minus_i = rep(0, data$n - 1), 
    log_sigma_phi_prev = 0
  )
  
  for(theta in names(opt_theta$par)) param_i_fixed_theta[[theta]] <- opt_theta$par[[theta]]
  
  obj_fixed_theta <- MakeADFun(
    data = dat_i,
    parameters = param_i_fixed_theta,
    random = "phi_prev_minus_i",
    DLL = "model1index",
    silent = TRUE,
    map = map_fixed_theta
  )
  
  quad <- aghq::aghq(ff = obj_fixed_theta, k = 3, startingvalue = 0, control = aghq::default_control_tmb())
  pdf_and_cdf <- aghq::compute_pdf_and_cdf(quad)[[1]]
  
  return(pdf_and_cdf)
}

laplace_df <- lapply(1:dat$n, xi_laplace_marginal) %>%
  bind_rows(.id = "index") %>%
  mutate(index = as.numeric(index))

M <- 1000

samples_laplace <- lapply(1:dat$n, function(i) {
  laplace_df_i <- filter(laplace_df, index == i)
  samples <- approxfun(x = laplace_df_i$cdf, y = laplace_df_i$theta, ties = "ordered")(runif(M))
  data.frame("value" = samples)
}) %>%
  bind_rows(.id = "index")
```

Let's also get everything we need for the joint Gaussian marginal for each index:

```{r}
x_Sigma <- solve(x_Q)

gaussian_df <- lapply(1:dat$n, function(i) {
  x <- seq(-5, 5, length.out = 1000)
  mean_i <- as.numeric(x_mean[i])
  sd_i <- as.numeric(sqrt(x_Sigma[i, i]))
  data.frame(index = i, x = x, pdf = dnorm(x, mean = mean_i, sd = sd_i))
}) %>%
  bind_rows()

samples_gaussian <- lapply(1:dat$n, function(i) {
  mean_i <- as.numeric(x_mean[i])
  sd_i <- as.numeric(sqrt(x_Sigma[i, i]))
  samples <- rnorm(M, mean_i, sd_i)
  data.frame("value" = samples)
}) %>%
  bind_rows(.id = "index")
```

## Comparison

```{r}
samples_tmbstan <- extract(mcmc, pars = "phi_prev") %>%
  as.data.frame() %>%
  pivot_longer(
    cols = everything(),
    names_to = "index",
    names_prefix = "phi_prev.",
    names_transform = as.integer
  )

plot <- ggplot(samples_tmbstan, aes(x = value)) +
  geom_histogram(aes(y = ..density..), alpha = 0.8, fill = cbpalette[7]) +
  facet_wrap(~index) +
  theme_minimal() +
  labs(x = "phi_prev", y = "Posterior PDF")

plot2 <- plot +
  geom_line(data = gaussian_df, aes(x = x, y = pdf), col = cbpalette[1])
```


```{r}
plot3 <- plot2 +
  geom_line(data = laplace_df, aes(x = theta, y = pdf), col = cbpalette[2])

plot3
```

### KS test

```{r}
ks_gaussian_tmbstan <- lapply(1:length(mean), function(i) {
  samples_gaussian_i <- filter(samples_gaussian, index == i)$value
  samples_tmbstan_i <- filter(samples_tmbstan, index == i)$value
  inf.utils::ks_test(samples_gaussian_i, samples_tmbstan_i)
})

ks_laplace_tmbstan <- lapply(1:length(mean), function(i) {
  samples_laplace_i <- filter(samples_laplace, index == i)$value
  samples_tmbstan_i <- filter(samples_tmbstan, index == i)$value
  inf.utils::ks_test(samples_laplace_i, samples_tmbstan_i)
})

ks_results <- bind_rows(
  bind_rows(ks_gaussian_tmbstan, .id = "index") %>%
    mutate(type = "gaussian"),
  bind_rows(ks_laplace_tmbstan, .id = "index") %>%
    mutate(type = "laplace")
)

ks_results %>%
  select(index, D, type) %>%
  pivot_wider(
    names_from = type,
    values_from = D
  ) %>%
  ggplot(aes(x = laplace, y = gaussian)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    lims(x = c(0, 1), y = c(0, 1)) +
    labs(x = "KS(laplace, tmbstan)", y = "KS(gaussian, tmbstan)") +
    theme_minimal()
```


# Simplified INLA

In Section \@ref(full-laplace) we directly calculate the Laplace approximation to $\int p(x, y, \theta) \text{d} \theta_{-i}$.
Suppose that
$$
x \sim \mathcal{N}(\mu(\theta), Q(\theta)^{-1})
$$
then by the conditional distributions of a multivariate Gaussian distribution $x_{-i} \, | \, x_i$ is Gaussian with mean
$$
\mu(\theta)_{-i} + \Sigma(\theta)_{-i, i} \Sigma(\theta)^{-1}_{i, i} (x_i - \mu (\theta)_i) (\#eq:conditionalmean)
$$
and precision matrix
$$
Q(\theta)_{-i, -i} (\#eq:conditionalprecision)
$$

We follow Wood (2019) Section 2.
In this section we omit dependence on $\theta$ such that: $\mu = \mu(\theta)$, $Q = Q(\theta)$, $\mu(x_i) = \mu(x_i, \theta)$ and $Q(x_i) = Q(x_i, \theta)$.

Let $\mathcal{D}_{2j}$ denote a $2j \times 2j$ diagonal matrix with leading diagonal $-1, 1, -1, 1, \ldots$.
For example, with $j = 3$ we have:

```{r}
D2j <- function(j) {
  diag((-1)^{1:2 * j}, nrow = 2 * j)
}

D2j(3)
```

Let $u_0$ be a zero column matrix.
I am guessing that this means $u_j$ is a column matrix containing $j$ as each entry.
It's not specified what dimension $u_0$ or $u_j$ should be, but since the algorithm contains producing by $\mathcal{D}_{2j}$ presumably their lengths must be $2j$.

```{r}
uj <- function(j) {
 matrix(j, nrow = 2 * j, ncol = 1)
}

uj(3)
```

The algorithm contains $u_j \mathcal{D}_{2j} u_j^\top$.
If $u_j$ is a column matrix of dimension $\text{length}(u_j) \times 1$ then it's not going to be conformable with a $2j \times 2j$ matrix.
Is it actually a row matrix of dimension $1 \times \text{length}(u_j)$?
And $\text{length}(u_j) = 2j$ as discussed above?

Let $\Delta_j^i$ denote $\Delta_j$ with an extra zero inserted at element $i$.
But it hasn't been defined what $\Delta_j$ is yet.

## Step 1: Compute the Cholesky factor

The first task is to "compute the Cholesky factor of $Q_0 := Q_{-i, -i}$ by update of the Cholesky factor of $Q$".

```{r}
i <- 1

mm <- h$env$last.par
mean <- mm[h$env$random]
Q <- h$env$spHess(mm, random = TRUE)
Q0 <- Q[-i, -i] 

L0 <- chol(Q0)
```

We have that $Q_0 = L_0 {L_0}^\top$:

```{r}
sum(abs((L0 %*% t(L0)) - Q0)) < 10E-9
```

## Step 2. Loop

For each $x_i$ in a grid of evaluation points (how does one chose this grid?) repeat the following:

```{r}
spacing <- 0.05
x_grid <- seq(from = 0.5, to = 3.5, by = spacing)
```

### a)

Use Newton's method with fixed Hessian $Q_{-i, -i}$ to find $\mu(x_i)$ starting from \@ref(eq:conditionalmean).
We start by computing the conditional mean for the first grid point:

```{r}
Sigma <- solve(Q)
conditional_mean <- mean[-i] + Sigma[-i, i] * Sigma[i, i]^{-1} * (x_grid[1] - mean[i])
```

Note that for this model the latent field is IID, so the conditional mean is the same as the mean.
Now we use Newton's method to find $\mu(x_i)$.
This requires some function $f(\mu(x_i), \ldots) = 0$.

### b)

Compute a set of $J$ steps $\{\Delta_j\}$

```{r}
J <- 1
```

### c)

```{r}
j <- 1
```

For $j = 1, \ldots, J - 1$ compute
$$
q = Q_0 \Delta_j + u_j \mathcal{D}_{2j}u_j^\top \Delta_j
$$
and
$$
g = \Delta_x \log p(\mu(x_i) + \Delta_j^i, y, \theta).
$$
Then compute the matrix
$$
u_{j + 1} = \{q(\Delta_j^\top j)^{-1/2}, g_{-i}(\Delta_j^\top g_{-i})^{-1/2}, u_j\}.
$$

### d)

Compute the determinant approximation
$$
\det(Q_1) = \det(Q_0) \det(I_{2J} + u_J^\top Q_0^{-1} u_J \mathcal{D}_{2j}).
$$

### e)

Compute $\tilde p(x_i \, | \, \theta, y) = \ldots$.

## Step 3. Renormalise $\tilde p(x_i \, | \, \theta, y)$

Renormalise $\tilde p(x_i \, | \, \theta, y)$.

# Appendix {#appendix}

## Comparison of notation

| Description | Here          | Wood (2019) notation | Notes    |
|:------------|:--------------|:---------------------|:---------|
| Random effects | $x$        | $\beta$              | Stringer (2021) use $w$
| Random effects which maximise (the / some) objective for given $\theta$ | $\mu (\theta)$ | $\hat \beta$ | Wood omits dependence on $\theta$ |
| Random effects which maximise (the / some) objective for given $\theta$, subject to $x_i$ being fixed | $(x_i, \mu(x_i, \theta))$ | $\tilde \beta$ | We have $\mu(x_i, \theta))$ having dimension $n - 1$ |
| Hessian / precision matrix of the negative log-likelihood with respect to $x$ at $\mu(\theta)$ | $Q(\theta)$ | $H$ | Unsure if it's preferable to use Hessian or precision notation here |

## `model1.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1.cpp')}
```

## `model1index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1index.cpp')}
```

# Original computing environment

```{r}
sessionInfo()
```
