---
title: "Implementing simplified INLA into `aghq`"
author:
- name: Adam Howes
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    df_print: paged
    code_folding: show
    theme: lumen
abstract: |
  **Background** Wood (2020) simplify the INLA method by approximating the latent field posterior marginals using a reduced cost Laplace approximation which does not rely on sparsity assumptions. In spatio-temporal statistics, usually the latent field corresponds to spatio-temporal locations -- so these marginal posteriors are of central scientific interest.
  
  **Task** We implement the method in R making use of Template Model Builder (`TMB`) for Gaussian approximations, in a way compatible with the `aghq` package for adaptive Gauss-Hermite quadrature. We are interested in efficiently approximating latent field posterior marginals $p(x_i \, | \, \theta, y) \approx \tilde p(x_i \, | \, \theta, y)$ for each index $i$. To build up to implementing the simplified INLA approach, we will start with two simpler methods (1) taking the marginal of a joint Gaussian approximation (2) doing a full Laplace approximation.
---

```{r echo=FALSE}
cbpalette <- multi.utils::cbpalette()
```

# Background

The AGHQ/ELGM approximation to the joint posterior of the latent field is the following object
$$
\begin{equation}
\tilde p(x \, | \, y) = \sum_z \tilde p_\text{G}(x \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z), (\#eq:pxgiveny)
\end{equation}
$$
where $\theta(z) \in \Theta$ and $w_k(z) \in \mathbb{R}$ are AGHQ nodes and weights generated by "adaption" to $\tilde p_\text{LA}(\theta \, | \, y)$.
In particular, let
$$
\mathcal{Q}(1, k) = \{z_d \in \mathbb{R}: H_k(z_d) = 0\}
$$
be the set of nodes in one-dimension, exactly the zeros of the Hermite polynomials $H_k(z_d)$ for $k \in \mathbb{N}$, and
$$
w_k(z_d) = \frac{k!}{H_{k + 1}(z_d)^2 \times \phi(z_d)}, 
$$
be the associated weights.
Define $\mathcal{Q}(d, k) = \mathcal{Q}(1, k)^d$ to be the set of nodes $z$ in $d$-dimensions, of which there will be $k^d$.
Then $\theta(z) = \hat \theta + Lz$ where $\hat \theta$ is the mode (of $p_\text{LA}(\theta \, | \, y)$) and $L$ is the lower Cholesky triangle of the curvature (at the mode) such that $LL^\top = H^{-1}$.

This can be evaluated at any point, though in practice we work with it by drawing samples $x \sim p_\text{G}(x \, | \, \theta, y)$ using a method from sampling from a Gaussian given its mean and precision from Rue (in particular you solve a linear equation using the lower Cholesky triangle of the curvature at a randomly chosen quadrature point).
For the $i$th marginal, we just take the appropriate samples from the joint.
The approximation is therefore
$$
\begin{equation}
\tilde p(x_i \, | \, y) = \sum_z \tilde p_\text{G}(x_i \, | \, \theta(z), y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z), (\#eq:pxigiveny)
\end{equation}
$$
where $p_\text{G}(x_i \, | \, \theta, y)$ just takes the $i$th elements of the mean vector and $(i, i)$th element of the covariance matrix.

To understand how this is implemented in `aghq`, the two most important functions are `aghq::marginal_laplace_tmb` and `aghq::sample_marginal`.
`aghq::marginal_laplace_tmb` calculates an object representing $\tilde p(x \, | \, \theta, y)$.
`aghq::sample_marginal` draws samples from it.
For a line-by-line walkthrough of how these functions work, see the notebook [Code walkthrough](https://athowes.github.io/elgm-inf/walkthrough.html) (which uses the model from the notebook [Epilepsy example](https://athowes.github.io/elgm-inf/epil.html)).

The goal of this notebook is to create a new approximation swapping out $\tilde p_\text{G}(x_i \, | \, \theta, y)$ from the above for the simplified INLA approximation $p_{\text{SINLA}}(x_i \, | \, \theta, y)$ as follows
$$
\tilde p(x_i \, | \, \theta, y) = \sum_z \tilde p_{\text{SINLA}}(x_i \, | \, \theta, y) \tilde p_\text{LA}(\theta(z) \, | \, y) w_k(z).
$$
The reasons for doing this is that the new approximation should be more accurate than taking marginals of a joint Gaussian approximation.
At the extreme, you could just compute a Laplace approximation for each marginal, integrating out all other elements of the latent field, $x_{-i}$, together with the fixed effects $\theta$.
This would be an accurate approach, but not computationally feasible for large models.
What we hope to achieve is an approximation which gets to most of the accuracy benefits of the Laplace approximation approach, but without the high computational cost.

# Ground truth {#ground-truth}

To provide a ground truth to compare the methods that follow to, we generate "gold-standard" inferences using a relatively long run of MCMC as implemented by `tmbstan`.

We begin by loading in testing data, as generated by [`prev-anc-art_sim`](https://github.com/athowes/elgm-inf/tree/master/src/prev-anc-art_sim).
For more information about this model, see the [Prevalence, ANC, ART model inference case-study](https://athowes.github.io/elgm-inf/prev-anc-art.html).
`sim_data` contains many replicates of simulated data.
Here we only need one, so will will just take the first as given by:

```{r}
sim_data <- readRDS("depends/sim_data.rds")
data <- sim_data[[1]]
dat <- list(n = data$n, y_prev = data$y_prev, m_prev = data$m_prev)
dat
```

```{r}
compile("model1.cpp")
dyn.load(dynlib("model1"))
```

We create an objective function `h` (see Appendix for `TMB` template) integrating the random effects $x$ (which here correspond to the spatial random effects `phi_prev`).
The values of `param` intitialise the objective function:

```{r}
param <- list(
  beta_prev = -2,
  phi_prev = rep(0, data$n),
  log_sigma_phi_prev = -1
)

h <- MakeADFun(
  data = dat,
  parameters = param,
  random = "phi_prev",
  DLL = "model1",
  silent = TRUE
)
```

It is simple to fit the model using MCMC via the `tmbstan` package.
We use 4 `chains` each of 5000 `iter`:

```{r}
mcmc <- tmbstan::tmbstan(h, chains = 4, iter = 5000, refresh = 0)
```

The posterior marginal $p(\phi_1 \, | \, y)$ can be examined by extracting the corresponding Markov chains, and plotting a histogram of the draws:

```{r message=FALSE, warning=FALSE}
plot <- extract(mcmc, pars = "phi_prev[1]") %>%
  as.data.frame() %>%
  ggplot(aes(x = phi_prev.1.)) +
    geom_histogram(aes(y = ..density..), alpha = 0.8, fill = cbpalette[7]) +
    theme_minimal() +
    labs(x = "phi_prev[1]", y = "Posterior")

plot
```

# Marginal of joint Gaussian approximation {#gaussian-marginal}

Now we will compute Equation \@ref(eq:eq:pxgiveny) taking the marignals of a joint Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$.
This is the approach currently used in the `aghq` package (done implicitly, via sampling).
This approach is also used by `method = "gaussian"` in `R-INLA`.

## $\tilde p(\theta \, | \, y)$ 

The Gaussian approximation $p_\text{G}(x \, | \, \theta, y)$ is previously used in computing the Laplace approximation to $p(\theta \, | \, y)$
$$
\begin{equation}
\tilde p(\theta \, | \, y) \propto \frac{p(x, \theta, y)}{p_\text{G}(x \, | \, \theta, y)} \Big\rvert_{x = \mu(\theta)} = \frac{p(y, \mu(\theta), \theta)}{\det \left(\frac{1}{2 \pi} Q(\theta) \right)^{1/2}}, (\#eq:pthetagiveny)
\end{equation}
$$
such that one easy way to calculate it is to set `random = "x"` in the `TMB` template to integrate the latent field out, then recover the Gaussian approximation from there, as we have done in the function `h` above.

Let's optimise `h` to obtain estimates $\hat \theta$ which maximise $\tilde p(\theta \, | \, y)$:

```{r}
its <- 1000

opt <- nlminb(
  start = h$par,
  objective = h$fn,
  gradient = h$gr,
  control = list(iter.max = its, trace = 0)
)
```

The values `opt$par` below correspond to $\hat \theta$:

```{r}
opt$par
```

We can also look at the parameters -- that is, mean vector and precision matrix -- of the Gaussian approximation at the mode `opt$par` above.
Note that in `aghq`, what we would do is create a quadrature grid, then get the mean vector and precision matrix for each value of the quadrature grid over $\theta$.
By just sticking to the mode of $\theta$, we are effectively doing `aghq` with $k = 1$ in each dimension, i.e. one grid point, corresponding to empirical Bayes type of inference.

```{r}
#' Notice that last.par contains opt$par as the fixed effects
#' It's the last.par because the last thing we've done with the template is the optimsation
#' This seems a pretty shakey way to go about things, but here we are...
mm <- h$env$last.par

#' These are the indices of the random effects that we want
h$env$random

#' Note that I'm calling this mean, but because it's a Gaussian the mean is the same as the mode
mean <- mm[h$env$random]
length(mean) #' The length is 30

#' Hessian evaluated at the mean (or equivalently mode)
Q <- h$env$spHess(mm, random = TRUE)
dim(Q) #' With dimension [30 x 30]
image(Q)
```

## Reproducing output from `h`

As a check, we would like to reproduce `h` using the right hand side of Equation \@ref(eq:pthetagiveny).
To do this, we start by creating an objective function `f` (see Appendix for the `TMB` template) to evaluate $- \log p(x, \theta, y)$ (without any Laplace approximations!).
This is what we will be using for the numerator of Equation \@ref(eq:pthetagiveny):

```{r}
f <- MakeADFun(
  data = dat,
  parameters = param,
  DLL = "model1",
  silent = TRUE
)
```

Given $\theta$, we compute the `mean` and `Q` of $x$ (corresponding to `phi_prev`) using `h`.
We then concatenate and evaluate using `f` to get the numerator, multiplying by -1 because `f$fn` gives the negative log-likelihood, and we want the log-likelihood.
In the denominator, we use `determinant` on the log-scale as it is more numerically stable than e.g. `log(det(...))`.

```{r}
calculate_rhs <- function(theta) {
  #' Calculate mean and Q for those theta
  #' TMB template remembers what you've called -- spooky
  h$fn(theta)
  mm <- h$env$last.par
  mean <- mm[h$env$random]
  Q <- h$env$spHess(mm, random = TRUE)
  
  #' Append the calculated latent field mean
  param <- theta
  param$phi_prev <- as.numeric(mean)
  param <- param[c("beta_prev", "phi_prev", "log_sigma_phi_prev")]
  
  #' Evaluate the log-numerator of the RHS
  numer <- -1 * f$fn(unlist(param))
  
  #' Evaluate the log-denominator of the RHS
  denom <- 0.5 * (determinant(Q, logarithm = TRUE)$modulus - (nrow(Q) * log(2 * pi)))
  
  #' Return their difference
  numer - denom
}

theta <- param
theta$phi_prev <- NULL #' Setting the value of x to be NULL

calculate_rhs(theta)
```

As compared with what we got from `h`:

```{r}
-1 * h$fn(unlist(theta))
```

## $\tilde p(x_i \, | \, y)$

Now let's have a go at computing Equation \@ref(eq:pxigiveny) so that we can compare to the posterior distribution from `tmbstan` we obtained in Section \@ref(ground-truth).
As described above, we simplify matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{G}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{G}(x_i \, | \, \hat \theta, y)
\end{align}
$$
We can evaluate this for $i = 1$ via:

```{r}
i <- 1
mean_i <- as.numeric(mean[i])
var_i <- solve(Q)[i, i]
```

And compare it to our MCMC results as follows:

```{r message=FALSE, warning=FALSE}
plot2 <- plot +
  stat_function(
    data = data.frame(x = c(0.5, 3.5)), aes(x, col = "Joint Gaussian marginal"), fun = dnorm, n = 101, 
    args = list(mean = mean_i, sd = sqrt(var_i)), size = 1
  ) +
  labs(x = "prev_phi[1]", y = "Posterior", col = "Approximation") +
  scale_colour_manual(values = cbpalette) +
  theme_minimal()

plot2
```

# Full Laplace approximation {#full-laplace}

In this section we will continue to use the approximation \@ref(eq:pthetagiveny) for $\tilde p(\theta \, | \, y)$.

## $\tilde p(x_i \, | \, \theta, y)$

For the approximation to the latent field marginal posteriors, another approach we could take is to calculate the full Laplace approximation
$$
\begin{align*}
  \tilde p(x_i \, | \, \theta, y) &\propto \frac{p(x, \theta, y)}{p_\text{G}(x_{-i} \, | \, x_i, \theta, y)} \Big\rvert_{x_{-i} = \mu_{-i}(x_i, \theta)} (\#eq:pxigiventhetay) \\
  &= \frac{p(y, x_i, \mu_{-i}(x_i, \theta), \theta)}{\det(\frac{1}{2 \pi} Q_{-i}(x_i, \theta))^{1/2}}
\end{align*}
$$
integrating out $x_{-i}$ with a Gaussian approximation.
This is not practical in some settings as it involves recomputing the Gaussian approximation $p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ for each value of $(x_i, \theta)$ which can be too computationally expensive.
We will still go through the steps for implementing it here though for demonstration.
With of a bit of effort, it can be implemented in `TMB` by passing in data where elements $i$ and $-i$ of the latent field are explicitly named (see Appendix for the `TMB` template `model1index.cpp`):

```{r}
compile("model1index.cpp")
dyn.load(dynlib("model1index"))
```

The `prepare_dat` function takes `dat` and makes it compatible with `model1index` for particular choice of marginal `i`.

```{r}
prepare_dat <- function(dat, i) {
  dat[["y_prev_i"]] <- dat$y_prev[i]
  dat[["y_prev_minus_i"]] <- dat$y_prev[-i]
  dat[["m_prev_i"]] <- dat$m_prev[i]
  dat[["m_prev_minus_i"]] <- dat$m_prev[-i]
  dat[c("n", "y_prev_i", "y_prev_minus_i", "m_prev_i", "m_prev_minus_i")]
}
```

Now we can loop over indices $i$ doing the required Laplace approximations.
In the call to `MakeADFun` we set `random = "phi_prev_minus_i"` to integrate all of the random effects but that of the particular index.

```{r message=FALSE, warning=FALSE, results=FALSE}
objs <- list()
means <- list()
Qs <- list()

for(i in 1:dat$n) {
  dat_i <- prepare_dat(dat, i)

  #' Starting parameters for TMB are the same for every index of the loop
  param_i <- list(
    beta_prev = 0,
    phi_prev_i = 0,
    phi_prev_minus_i = rep(0, data$n - 1),
    log_sigma_phi_prev = 0
  )

  #' random are integrated out with a Laplace approximation
  obj <- MakeADFun(
    data = dat_i,
    parameters = param_i,
    random = "phi_prev_minus_i",
    DLL = "model1index",
    silent = TRUE
  )

  its <- 1000

  opt <- nlminb(
    start = obj$par,
    objective = obj$fn,
    gradient = obj$gr,
    control = list(iter.max = its, trace = 0)
  )

  #' Parameters of the Gaussian approximation
  mm <- obj$env$last.par
  mean <- mm[obj$env$random]
  Q <- obj$env$spHess(mm, random = TRUE)

  objs[[i]] <- obj
  means[[i]] <- mean
  Qs[[i]] <- Q
}
```

Let's consider the first marginal, that is $\tilde p_\text{LA}(x_1 \, | \, \theta, y)$.
The mean and precision matrix for the Gaussian approximation $\tilde p_\text{G}(x_{-i} \, | \, x_i, \theta, y)$ are:

```{r message=FALSE, warning=FALSE}
i <- 1
obj <- objs[[i]]
mean <- means[[i]] #' The length is 30 - 1
mean

Q <- Qs[[i]]
dim(Q) #' With dimension [(30 - 1) x (30 - 1)]
image(Q)
```

`obj$fn` should take as input $(x_i, \theta)$:

```{r}
transform_param <- function(param) {
  list(
    beta_prev = param$beta_prev,
    phi_prev_i = param$phi_prev[i],
    phi_prev_minus_i = param$phi_prev[-i],
    log_sigma_phi_prev = param$log_sigma_phi_prev
  )
}

transformed_param <- transform_param(param)
transformed_param$phi_prev_minus_i <- NULL #' Remove the integrated out effects
transformed_param

obj$fn(transformed_param)
```

## $\tilde p(x_i \, | \, y)$

Let's have a go at computing Equation \@ref(eq:pxigiveny) so that we can compare to the posterior distribution from `tmbstan`.
We simplify matters by considering just one quadrature point, $\hat \theta$, such that the sum over $z$ collapses to:
$$
\begin{align}
\tilde p(x_i \, | \, y) &= \tilde p_\text{LA}(x_i \, | \, \hat \theta, y) \tilde p_\text{LA}(\hat \theta \, | \, y) \\
&\propto \tilde p_\text{LA}(x_i \, | \, \hat \theta, y)
\end{align}
$$

Start by getting $\hat \theta$:

```{r}
opt <- nlminb(
  start = h$par,
  objective = h$fn,
  gradient = h$gr,
  control = list(iter.max = its, trace = 0)
)

#' How many equally spaced points do we want to evaluate the posterior at?
spacing <- 0.05
x_grid <- seq(from = 0.5, to = 3.5, by = spacing)
x_grid_length <- length(x_grid)

df <- opt$par %>%
  t() %>%
  as.data.frame() %>%
  slice(rep(1:n(), each = x_grid_length)) %>%
  mutate(
    phi_prev_i = x_grid
  ) %>%
  rowwise() %>%
  mutate(
    #' Note here: it's crucial that the arguments are in the right order!
    logpost = -obj$fn(c(beta_prev, phi_prev_i, log_sigma_phi_prev))
  )

ggplot(df, aes(x = phi_prev_i, y = logpost)) +
  geom_line() +
  theme_minimal() +
  labs(x = "phi_prev[1]", "log(posterior)")
```

What we've got here is the unnormalised log-posterior?
How can we go from that to a normalised posterior?
Going to do trapezoid rule here, but AGHQ would work better (even with a low number of points) because the function is close to being Gaussian.

$$
\begin{align}
\tilde f(x) &= \frac{f(x)}{\int f(x) \text{d}x} = \frac{f(x)}{C}
\implies \\
\log\tilde f(x) &= \log f(x) - \log C
\end{align}
$$

```{r}
safe_exp <- function(x) exp(x - max(x))

trapezoid_rule <- function(x, spacing) {
  0.5 * spacing * (2 * sum(x) - x[1] - x[2])
}

df$post <- safe_exp(df$logpost)
df$post_norm = df$post / trapezoid_rule(df$post, spacing)

plot3 <- plot2 +
  geom_line(data = df, aes(x = phi_prev_i, y = post, col = "Laplace"), size = 1)

plot3
```

So perhaps it's barely better than the Gaussian, but not really getting the real problem here.
Hypothesis would be that it's because we're doing empirical Bayes, and with more than one hyperparameter grid point we would do better.

# Simplified INLA

| Description | Here | Wood (2019) notation | Notes |
|-------------|---------------|----------------------|----------|
| Random effects | $x$ | $\beta$ | Stringer (2021) use $w$
| Random effects which maximise (the / some) objective for given $\theta$ | $\mu (\theta)$ | $\hat \beta$ | Wood omits dependence on $\theta$ |
| Random effects which maximise (the / some) objective for given $\theta$, subject to $x_i$ being fixed | $(x_i, \mu(x_i, \theta))$ | $\tilde \beta$ | We have $\mu(x_i, \theta))$ having dimension $n - 1$ |
| Hessian / precision matrix of the negative log-likelihood with respect to $x$ at $\mu(\theta)$ | $Q(\theta)$ | $H$ | Unsure if it's preferable to use Hessian or precision notation here |

In Section \@ref(full-laplace) we directly calculate the Laplace approximation to $\int p(x, y, \theta) \text{d} \theta_{-i}$.
Following Wood (2019), a cheaper approach would be to "base the denominator of Equation \@ref(eq:pxigiventhetay) on the conditional density implied by ? in which case the Hessian is constant and
$$
\mu(x_i, \theta) = \mu(\theta) + \Sigma(\theta)_{-i, i} \Sigma(\theta)^{-1}_{i, i} (x_i - \mu (\theta)_i) (\#eq:wood3)
$$
The proposed modification has two parts:

1. Use the numerically exact $\mu(x_i, \theta)$ rather than an approximation (as in simplified Laplace)
2. An approximation to the required Hessian

## Complete algorithm

In this section we omit dependence on $\theta$ such that: $\mu = \mu(\theta)$, $Q = Q(\theta)$, $\mu(x_i) = \mu(x_i, \theta)$ and $Q(x_i) = Q(x_i, \theta)$.

Want: $\tilde p(x_i \, | \, \theta, y)$

Let $\mathcal{D}_{2j}$ denote a $2j \times 2j$ diagonal matrix with leading diagonal $-1, 1, -1, 1, \ldots$.
Let $u_0$ be a zero column matrix.
Let $\Delta_j^i$ denote $\Delta_j$ with an extra zero inserted at element $i$.

1. Compute the Cholesky factor of $Q_0 := Q_{-i, -i}$ by update of the Cholesky factor of $Q$
2. For each $x_i$ in a grid of evaluation points (how does one chose this grid?) repeat the following steps a-e)
  a) Use Newton's method with fixed Hessian $Q_{-i, -i}$ to find $\mu(x_i)$ starting from \@ref(eq:wood3)
  b) Compute a set of $J$ steps $\{\Delta_j\}$
  c) For $j = 1, \ldots, J - 1$ compute
  $$
  q = Q_0 \Delta_j + u_j \mathcal{D}_{2j}u_j^\top \Delta_j
  $$
  and
  $$
  g = \Delta_x \log p(\mu(x_i) + \Delta_j^i, y, \theta).
  $$
  Then compute the matrix
  $$
  u_{j + 1} = \{q(\Delta_j^\top j)^{-1/2}, g_{-i}(\Delta_j^\top g_{-i})^{-1/2}, u_j\}.
  $$
  d) Compute the determinant approximation...
  e) Compute $\tilde p(x_i \, | \, \theta, y) = \ldots$
3. Renormalise $\tilde p(x_i \, | \, \theta, y)$

# Appendix

## `model1.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1.cpp')}
```

## `model1index.cpp`

```{cpp, echo=TRUE, eval=FALSE, output.var="ex", code=readLines('model1index.cpp')}
```
