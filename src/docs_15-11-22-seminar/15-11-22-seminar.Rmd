---
title: Integrated nested Laplace approximations for extended latent Gaussian models with application to the Naomi HIV model
subtitle: Waterloo SAS Student Seminar Series
author: Adam Howes
institute: Imperial College London
date: November 2022
bibliography: citations.bib
output:
  beamer_presentation:
    latex_engine: pdflatex
    highlight: haddock
    fig_width: 7 
    fig_height: 3
    includes:
        in_header: preamble.tex
---

# Motivation

* Surveillance of the HIV epidemic in sub-Saharan Africa
* Want to estimate indicators used for monitoring and response, including:
  * Prevalence $\rho$: the proportion of people who are HIV positive
  * Treatment coverage $\alpha$: the proportion of PLHIV on treatment
  * Incidence $\lambda$: the proportion of people newly infected 
* Aim to provide estimates at a district-level to enable \textcolor{hilit}{precision public health}

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
This is a challenging task! Data is noisy, sparse and biased $\implies$ compelling case for thoughtful Bayesian modelling 
\end{tcolorbox}
\end{center}

# A simple small-area model for prevalence

* Consider "small-areas" $i = 1, \ldots, n$ (e.g. districts of a country)
* Simple random sample household-survey^[In reality a complex survey design is used, often with urban rural stratification.] of size $m^\text{HS}_i$ where $y^\text{HS}_i$ people testing positive for HIV
* Could calculate direct estimates of prevalence by $y^\text{HS}_i / m^\text{HS}_i$

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
Because the survey is powered at a national-level, the $m^\text{HS}_i$ are small and direct estimates would be noisy $\implies$ use a model to smooth estimates
\end{tcolorbox}
\end{center}

# A simple small-area model for prevalence

* We can use a binomial logistic regression of the form:
\begin{align*}
y^\text{HS}_i &\sim \text{Bin}(m^\text{HS}_i, \rho^\text{HS}_i), \\
\text{logit}(\rho^\text{HS}_i) &\sim g(\vartheta^\text{HS}), \quad i = 1, \ldots, n,
\end{align*}
* We usually set up $g$ as a Gaussian spatial smoother
* This allows for \textcolor{hilit}{pooling of information} between districts

#

```{r echo=FALSE, warning=FALSE, out.width="350px", fig.cap = "The Besag model, $\\phi_i \\, | \\, \\phi_{-i} \\sim \\mathcal{N} \\left(\\frac{1}{n_{\\delta i}} \\sum_{j: j \\sim i} \\phi_j, \\frac{1}{n_{\\delta i}\\tau_\\phi} \\right)$."}

areas <- filter(sf::read_sf("zwe_areas.geojson"), area_level == 2)

areas %>%
  filter(area_level == 2) %>%
  bsae::plot_graph(add_geography = TRUE)
```

# Latent Gaussian models

* Three-stage Bayesian hierarchical model
\begin{alignat*}{2}
  &\text{(Observations)}     &   \y &\sim p(\y \, | \, \x), \\
  &\text{(Latent field)}     &   \x &\sim p(\x \, | \, \btheta), \\
  &\text{(Hyperparameters)}  &   \qquad \btheta &\sim p(\btheta),
\end{alignat*}
where $\y = (y_1, \ldots, y_n)$, $\x = (x_1, \ldots, x_N)$, $\btheta = (\theta_1, \ldots, \theta_m)$
* Interested in learning both $(\btheta, \x)$ from data $\y$
* If the middle layer is Gaussian, then it's a \textcolor{hilit}{latent Gaussian model}
\begin{equation*}
  \text{(Latent field)} \qquad  p(\x \, | \, \btheta) = \mathcal{N}(\x  \, | \, \bm{\mu}(\btheta), \bm{Q}(\btheta)^{-1}).
\end{equation*}
* Latent field is typically indexed by spatiotemporal location, such that $N > m$

# Limitations of household surveys

* Household surveys cost millions to run so they don't happen very often
* e.g. DHS include 5k-30k households, and occurs around every 5 years

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
The snapshot they provide can be quite out of date, and difficult to base effective policy on $\implies$ need to use routinely collected data to help here
\end{tcolorbox}
\end{center}

# Adding ANC surveillance

* Pregnant women attending antenatal care clinics are routinely tested for HIV, to avoid mother-to-child transmission. This data source is:
 1. More \textcolor{hilit}{real-time} than household surveys -- can be collected e.g. monthly
 2. More \textcolor{hilit}{biased} than household surveys -- attendees are not representative
* If the this bias is consistent, we can still ANC data to supplement our model

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
$\implies$ model the level using the household survey data, and the trend using the ANC data
\end{tcolorbox}
\end{center}

# Adding ANC surveillance

* Suppose of $m^\text{ANC}_i$ ANC attendees, $y^\text{ANC}_i$ are HIV positive, and model
\begin{align*}
y^\text{ANC}_i &\sim \text{Bin}(m^\text{ANC}_i, \rho^\text{ANC}_i), \\
\text{logit}(\rho^\text{ANC}_i) &= \text{logit}(\rho^\text{HS}_i) + b_i, \\
b_i &\sim \mathcal{N}(\beta_b, \sigma_b^2),
\end{align*}
* This is similar to using $\rho^\text{ANC}_i$ as a covariate in the model for household survey prevalence, but this way takes into account sampling variation

# Adding ART coverage

* Also interested in what proportion $\alpha_i$ of people living with HIV are receiving treatment, which may also be informative about prevalence
* If we record $A_i$ attendees from a known population of $N_i$ in each district, then this can be modelled by
\begin{align*}
A_i &\sim \text{Bin}(N_i, \rho^\text{HS}_i \alpha_i), \\
\text{logit}(\alpha_i) &\sim \mathcal{N}(\beta_\alpha, \sigma_\alpha^2).
\end{align*}
* To be more sophisticated, you can also model the movement of people to receive treatment in districts other than the one they live in

# Naomi evidence synthesis model

:::::::::::::: {.columns}

::: {.column width=.65}

* Combining these three modules is the basis of the Naomi evidence synthesis model
* Used by countries to produce HIV estimates in a yearly process supported by UNAIDS
* Can't run long MCMC in this setting, so we \textcolor{hilit}{require fast, accurate, approximations}
* It's a complicated model, and requires something more flexible than `R-INLA`
* Currently using a package called Template Model Builder `TMB`

:::

::: {.column width=.35}

```{r, echo=FALSE, fig.cap="A supermodel", out.width = '65%'}
knitr::include_graphics("naomi_hex.png")
```

:::

::::::::::::::

# 

```{r, echo=FALSE, fig.cap="Example of the user interface from https://naomi.unaids.org/", out.width = '70%'}
knitr::include_graphics("naomi_user.png")
```

# Template Model Builder

* `TMB` [@kristensen2015tmb] is an R package which implements the Laplace approximation for latent variable models
* I use "Laplace approximation" to mean approximating the normalising constant with Laplace's method^[Rather than approximating the posterior with a Gaussian, which I call a Gaussian approximation.]
* To get started with `TMB`, write your $f(\x, \btheta)$ in `TMB`'s C++ syntax
* As pseudo-Bayesians, we choose (something proportional to) the log-posterior
$$
f(\x, \btheta) = - \log p(\y \, | \, \x, \btheta) p(\x \, | \, \btheta) p(\btheta)
$$
* For example, for the model $\y \sim \mathcal{N}(\mu, 1)$ with $p(\mu) \propto 1$ then the `TMB` user template looks as follows

# 

```{cpp eval=FALSE}
#include <TMB.hpp>

template <class Type>
Type objective_function<Type>::operator()() {
  // Define data e.g.
  DATA_VECTOR(y);
  // Define parameters e.g.
  PARAMETER(mu);
  // Calculate negative log-likelihood e.g.
  nll = Type(0.0);
  nll -= dnorm(y, mu, 1, true).sum()
  return(nll);
}
```

# Template Model Builder

* We can use `TMB` to obtain the Laplace approximation
$$
\tilde p_\text{LA}(\btheta \, | \, \y) \propto \frac{p(\y, \x, \btheta)}{\tilde p_\text{G}(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \bm{\mu}(\btheta)} \label{eq:hypermarginal}
$$
* Integrate out a Gaussian approximation $\tilde p_\text{G}(\x \, | \, \btheta, \y)$ to the latent field
* `TMB` uses automatic differentiation [@griewank2008evaluating] via `CppAD` to do this, as well as help with numerical optimisation routines
* We then optimise this to obtain a mode $\hat \btheta$, and a Hessian $\bm{H}$ at the mode

# Integrated Nested Laplace Approximation

* Integrated nested Laplace approximation (INLA) [@rue2009approximate; @blangiardo2015spatial] is an approach to approximate inference which builds on the Laplace approximation
* Goal is to approximate \textcolor{hilit}{posterior marginals} $\{\tilde p(x_i \, | \, \y)\}_{i = 1}^n$ and $\{\tilde p(\theta_j \, | \, \y)\}_{j = 1}^m$
\begin{align}
  p(x_i \, | \, \y) &= \int p(x_i, \btheta \, | \, \y) \text{d} \btheta = \int p(x_i \, | \, \btheta, \y) p(\btheta \, | \, \y) \text{d}\btheta, \quad i = 1, \dots, N, \label{eq:inla1} \\
  p(\theta_j \, | \, \y) &= \int p(\btheta \, | \, \y) \text{d}\btheta_{-j} \quad j = 1, \ldots, m. \label{eq:inla2}
\end{align}
* To do so, we require the approximations $\tilde p(\btheta \, | \, \y)$ and $\tilde p(x_i \, | \, \btheta, \y)$
* There are four steps as to how the method works (bare with me!)

# Step 1

1) First Laplace approximate hyperparameter posterior
\begin{equation}
\tilde p_\text{LA}(\btheta \, | \, \y) \propto \frac{p(\y, \x, \btheta)}{\tilde p_\text{G}(\x \, | \, \btheta, \y)} \Big\rvert_{\x = \bm{\mu}(\btheta)} \label{eq:hypermarginal}
\end{equation}
which can be marginalised to get $\tilde p(\theta_j \, | \, \y)$
* Notice that this is the same object we had been working with in `TMB`
* We will use this approximation \textcolor{hilit}{nested} within integrals like this one
$$
\int p(x_i, \btheta \, | \, \y) \text{d} \btheta = \int p(x_i \, | \, \btheta, \y) \tilde p_\text{LA}(\btheta \, | \, \y) \text{d}\btheta
$$
hence the name INLA

# Step 2

2) In both Equations \eqref{eq:inla1} and \eqref{eq:inla2} we want to integrate w.r.t. $\btheta$, so choose integration nodes and weights $\{ \btheta(\z), \omega(\z) \}_{\z \in \mathcal{Z}}$
  * For low $m$ `R-INLA` uses a grid-strategy
  * For larger $m$ this becomes too expensive and `R-INLA` uses a CCD design
  * We plan to use adaptive Gaussian Hermite quadrature (AGHQ), which has recently been shown to have theoretical guarantees [@bilodeau2021stochastic] and is implemented in the `aghq` R package [@stringer2021implementing]
  
#

```{r message=FALSE, echo=FALSE, out.height="200px", fig.cap=paste("An illustration of the \\texttt{R-INLA} grid method for selecting integration nodes using a toy bivariate Gaussian distribution for $\\btheta$. Start at the mode and work outwards along the eigenvectors until the density drops sufficiently low. \\label{fig:grid}")}
knitr::include_graphics("depends/inla-grid.pdf")
```

# Adaptive Gaussian Hermite Quadrature

* Gauss-Hermite quadrature is one way to pick nodes $\z \in \mathcal{Q}(m, k)$ and weights $\omega(\z): \mathcal{Q}(m, k) \to \mathbb{R}$, based on the theory of polynomial interpolation
* The \textcolor{hilit}{adaptive} part means that it uses the location (mode) and curvature (Hessian) of the target (posterior) so that $\btheta(\z) = \hat \btheta + \bm{L}\z$
* Works particularly well when the integrand is pretty Gaussian
* Use $k$ quadrature nodes per dimension, e.g. if $k = 3$ then $3^m$ total nodes

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
Key benefits: no manual tuning, works well (and starting to get some theory) in statistical contexts
\end{tcolorbox}
\end{center}
#

```{r message=FALSE, echo=FALSE, out.height="200px", fig.cap=paste("One dimensional example of AGHQ with $3^1 = 3$ nodes. If $k$ is odd then the mode is always included. \\label{fig:aghq-1d}")}
knitr::include_graphics("depends/aghq-1d.pdf")
```

#

```{r message=FALSE, echo=FALSE, out.height="200px", fig.cap=paste("Two dimensional example of AGHQ with $3^2 = 9$ nodes. Here we use the product rule so that the points in 2D are just 1D x 1D. \\label{fig:aghq-2d}")}
knitr::include_graphics("depends/aghq-2d.pdf")
```

# Step 3

3) Choose approximation for $\tilde p(x_i \, | \, \btheta, \y)$
  * Simplest version [@rue2007approximate] is to marginalise $\tilde p_\text{G}(\x \, | \, \btheta, \y)$
\begin{equation}
\tilde p_\text{G}(x_i \, | \, \btheta, \y) = \mathcal{N}(x_i \, | \, \mu_i(\btheta), 1 / q_i(\btheta))
\end{equation}
  * In `R-INLA`, the above is referred to as `method = "gaussian"` 
  * This is also what is currently used in `aghq`

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
There are more accurate (and complicated) versions which I will talk briefly about in a minute!
\end{tcolorbox}
\end{center}

# Step 4

4) Finally, use quadrature to combine
  * our approximation $\tilde p_\text{LA}(\btheta \, | \, \y)$ from Step 1,
  * some choice of integration nodes and weights $\{ \btheta(\z), \omega(\z) \}$ Step 2,
  * some choice of approximation $\tilde p(x_i \, | \, \btheta, \y)$ from Step 3
to give
\begin{equation}
  \tilde p(x_i \, | \, \y) = 
  \sum_{\z \in \mathcal{Z}} \tilde p(x_i \, | \, \btheta(\z), \y) \times \tilde p_\text{LA}(\btheta(\z) \, | \, \y) \times \omega(\z)
\end{equation}

# Using a Laplace approximation for Step 3

* Previously had been taking the marginals of $\tilde p_\text{G}(\x \, | \, \btheta, \y)$
* Alternative: calculate \textcolor{hilit}{a new Laplace approximation} for each $x_i$
$$
\tilde p_\text{LA}(x_i, \btheta, \y) = \frac{p(x_i, \x_{-i}, \btheta, \y)}{\tilde p_\text{G}(\x_{-i} \, | \, x_i, \btheta, \y)} \Big\rvert_{\x_{-i} = \bm{\mu}_{-i}(x_i, \btheta)}
$$
where $\tilde p_\text{G}(\x \, | \, \btheta, \y) = \mathcal{N}(\x \, | \, \bm{\mu}_{-i}(x_i, \btheta), \bm{Q}_{-i}(x_i, \btheta)^{-1})$
* Problem: $N$ can be big, and we will need to recalculate this for each $(x_i, \btheta)$
* Ideas like using $\bm{\mu}(\btheta)_{-i}$ to initialise Newton optimisation to find $\bm{\mu}_{-i}(x_i, \btheta)$ could help 

# Cheaper approximate approximations

* @rue2009approximate found a way to do this in a cheaper and more approximate way based on assuming a \textcolor{hilit}{sparse} precision for $\x$
  * a.k.a. that $\x$ is a Gaussian Markov random field (GMRF)
* @wood2020simplified extended their approximation to work for the case when $\x$ does not have a sparse precision

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
Plan: see how long a naive version without these modifications takes, then use this work to get speed-ups as required
\end{tcolorbox}
\end{center}

# Epilepsy example

* Replication of example from Section 5.2. of @rue2009approximate, and previously from BUGS manual
* Patients $i = 1, \ldots, 59$ each either assigned treatment $\texttt{Trt}_i = 1$ or placebo $\texttt{Trt}_i = 0$ to help with seizures
* Visits to clinics $j = 1, \ldots, 4$ times with $y_{ij}$ the number of seizures of the $i$th person in the two weeks proceeding their $j$th visit to the clinic
* Covariates age $\texttt{Age}_i$, baseline seizure counts $\texttt{Base}_i$ and an indicator for the final clinic visit $\texttt{V}_4$

\begin{center}
\begin{tcolorbox}[width=0.9\textwidth, colframe={title}, colback={white}, title={}]
Notebook for this example at \texttt{athowes.github.io/elgm-inf/epil}
\end{tcolorbox}
\end{center}

# Epilepsy example

The model is a Poisson GLMM:
\begin{align*}
  y_{ij}          &\sim \text{Poisson}(\lambda_{ij}), \\
  \lambda_{ij}    &= e^{\eta_{ij}}, \\
  \eta_{ij}       &= \beta_0 + \beta_{\texttt{Base}} \log(\texttt{Baseline}_j / 4) + \beta_{\texttt{Trt}} \texttt{Trt}_i +
                     \beta_{\texttt{Trt} \times \texttt{Base}} \texttt{Trt}_i \times \log(\texttt{Baseline}_j / 4) \\ 
                  &+ \beta_{\texttt{Age}} \log(\texttt{Age}_i) + \beta_{\texttt{V}_4} {\texttt{V}_4}_j +
                     \epsilon_i + \nu_{ij}, \quad i=1:59, \quad j=1:4, \\
  \beta           &\sim \mathcal{N}(0, 100^2), \quad \forall \beta, \\
  \epsilon_i      &\sim \mathcal{N}(0, 1/\tau_\epsilon), \\
  \nu_{ij}        &\sim \mathcal{N}(0, 1/\tau_\nu), \\
  \tau_\epsilon   &\sim \Gamma(0.001, 0.001), \\
  \tau_\nu        &\sim \Gamma(0.001, 0.001).
\end{align*}

# Inference

Implement the following inference procedures:

1. HMC NUTS via `tmbstan` and `TMB`
2. Grid with Gaussian marginals via `R-INLA`
3. Grid with simplified Laplace marginals via `R-INLA`
4. Grid with Laplace marginals via `R-INLA`
5. EB with Gaussian marginals via `TMB`
6. AGHQ with Gaussian marginals via `aghq` and `TMB`
7. **EB with Laplace marginals via `aghq` and `TMB`**^[I'm working on AGHQ with Gaussian marginals via `aghq` and `TMB`. I am using the `aghq` package, just with $k = 1$ corresponding to EB]

# Results

# Comparison approach

* You could look at the summaries like the mean and standard deviation of each of the posterior marginals but it's better to compare the whole posterior distributions
* One way to do this is via Kolmogorov-Smirnov statistics, which give the maximum difference between two empirical CDFs

# Prevalence, ANC, ART example

* Notebook for this example at `athowes.github.io/elgm-inf/prev-anc-art`
* Simulate data from model with all three components and particular (known) parameter values

# Inference

Implement the following inference procedures:

1. HMC NUTS via `tmbstan` and `TMB`
2. EB with Gaussian marginals via `TMB`
3. AGHQ with Gaussian marginals via `aghq` and `TMB`

* All of these approaches share the same C++ template, so the models are identical! This is often very difficult to ensure, so we're very fortunate here^[i.e. thanks to Kasper and Alex for making `tmbstan` and ``aghq` respectively!]

# Results

# Conclusions

> My main comment is that several aspects of the computational machineery that is presented by Rue and his colleagues **could benefit from the use of a numerical technique known as automatic differentiation (AD)** ... By the use of AD one could obtain a system that is automatic from a user's perspective... the benefit would be a fast, flexible and easy-to-use system for doing Bayesian analysis in models with Gaussian latent variables 

-- Hans J. Skaug (coauthor of `TMB`), RSS discussion of @rue2009approximate

# Conclusions

* Hopeful that we'll give fast, accurate inferences for Naomi!
* Implementation as a part of `aghq` combining simplified INLA and AGHQ, enabled by automatic differentiation, will provide flexible use of the method
  * Will be of interest to advanced users of `R-INLA` who would like specify models outside a formula interface (similar to users of `brms` v.s. Stan)
  * This describes many in the HIV inference group `hiv-inference.org`^[See `athowes.github.io/inla-sandbox/` for some examples of understanding `R-INLA` internals.]

# Thanks for listening!

* Joint work with Alex Stringer (Waterloo) and my supervisors Seth Flaxman (Oxford) and Jeff Eaton (Imperial)
* The code for this project is at `github.com/athowes/elgm-inf`
* You can find me online at `athowes.github.io`

# References {.allowframebreaks}
