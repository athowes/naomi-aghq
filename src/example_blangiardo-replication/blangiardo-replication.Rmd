---
title: "Blangiardo replication"
author: "Adam Howes"
output: html_document
---

Step-by-step example of the INLA method.
The model for i.i.d. observations $y = (y_1, \ldots, y_n)$ is as follows
\begin{align}
y_i \, | \, x, \theta &\sim \mathcal{N}(x, 1/\theta), \\
x &\sim \mathcal{N}(x_0 = -3, 1/\tau_0 = 4), \\
\theta &\sim \text{Gamma}(a = 1.6, b = 0.4).
\end{align}

## Set-up

```{r message=FALSE, warning=FALSE}
midblue <- "#3D9BD0"
midgreen <- "#00855A"
midpink <- "#B3608E"
```

Load the data:

```{r}
y <- c(1.2697, 7.7637, 2.2532, 3.4557, 4.1776, 6.4320, -3.6623, 7.7567, 5.9032, 7.2671,
       -2.3447, 8.0160, 3.5013, 2.8495, 0.6467, 3.2371, 5.8573, -3.3749, 4.1507, 4.3092,
       11.7327, 2.6174, 9.4942, -2.7639, -1.5859, 3.6986, 2.4544, -0.3294, 0.2329, 5.2846)

n <- length(y)
y_bar <- mean(y)
```

```{r, fig.height=2, fig.cap="The observations (ordered by index, though this has no interpretation). The prior (mean shown as dashed line) mismatches with the data."}
data.frame(index = 1:30, y = y) %>%
  ggplot(aes(x = index, y = y)) +
  geom_point(shape = 4) +
  labs(title = "The data") +
  geom_hline(yintercept = -3, col = "#666666", lty = "dashed")
```

Parameters of the prior distribution:

  ```{r}
x_0 <- -3
tau_0 <- 1/4
a <- 1.6
b <- 0.4
```

## `R-INLA`

```{r}
formula <- y ~ 1
dat <- list(y = y)

theta_prior <- list(prec = list(prior = "loggamma", param = c(a, b)))

fit <- inla(formula,
            data = dat,
            control.family = list(hyper = theta_prior),
            control.fixed = list(mean.intercept = x_0, prec.intercept = tau_0))
```

## R implementation

Exact distribution of $x$ given $\theta$:

  ```{r}
inner_loop <- function(theta) {
  tau_n <- n * theta + tau_0
  x_n <- (theta * n * y_bar + tau_0 * x_0) / tau_n
  return(list(x_n = x_n, tau_n = tau_n))
}
```

Functions with $\theta > 0$ constraint and on the $\log(\theta)$ scale to avoid this constraint:

```{r}
nl_full_conditional_x <- function(x, theta, log_input = FALSE) {
  if(log_input == TRUE) theta <- exp(theta) # Convert from log(theta) to theta
  par <- inner_loop(theta)
  return(-dnorm(x, par$x_n, sqrt(1 / par$tau_n), log = TRUE))
}

nl_post_marginal_theta <- function(theta, log_input = FALSE) {
  target <- 0
  if(log_input == TRUE) {
    target <- target + theta # Increment by Jacobian correction (theta is l_theta here)
    theta <- exp(theta) # Convert from log(theta) to theta
  }
  par <- inner_loop(theta)

  # (^)
  target <- target + -0.5 * log(par$tau_n) +
    dgamma(theta, shape = a, rate = b, log = TRUE) +  # theta prior
    dnorm(par$x_n, x_0, sqrt(1 / tau_0), log = TRUE) +  # x prior
    sum(dnorm(y, par$x_n, sqrt(1 / theta), log = TRUE)) # y likelihood

  return(-target)
}
```

Note that the code above in the section marked `(^)` is very similar to that in `blangiardo.cpp`.
Recall that the Laplace approximation $\tilde p(\theta \, | \, y)$, which in this instance coincides with the exact posterior, is given by
\begin{equation}
\tilde p(\theta \, | \, y) \propto \frac{p(y, \mu^\star(\theta), \theta)}{\det(Q(\theta))^{1/2}}.
\end{equation}
This form can be seen in `(^)`:

  * $p(y, \mu^\star(\theta), \theta)$ corresponds to the calls to `dgamma`, `dnorm` and sum of `dnorm` evaluated at $\mu^\star(\theta)$ which here is simply `par$x_n` (for the Normal distribution the mean of the distribution is the same as the mode).
* The logarithm of $1 / \det(Q(\theta))^{1/2}$ when $\theta$ is a scalar is given by `-0.5 * log(par$tau_n)`.

It should be the case that `inner_loop` produces parameter values for the latent field which are those found by `TMB` using the optimisation inner loop (hence the name).

```{r}
de_nl <- function(f, ...) exp(-f(...)) # Versions which are not negative logarithms

full_conditional_x <- function(x, theta, log_input = FALSE) {
  de_nl(nl_full_conditional_x, x, theta, log_input = FALSE)
}

post_marginal_theta <- function(theta, log_input = FALSE) {
  de_nl(nl_post_marginal_theta, theta, log_input = FALSE)
}
```

Simple grids (one dimensional):

```{r}
eval_grid <- function(grid = NULL, uniform = FALSE, K = NULL, min = NULL, max = NULL, f) {
  if(uniform) {
    grid <- seq(min, max, length.out = K)
  }
  df <- data.frame(input = grid, output = sapply(grid, f))
  df <- mutate(df, norm_output = output / sum(output))
  return(df)
}

blangiardo_theta <- eval_grid(uniform = TRUE,
                              K = 25, min = 0.001, max = 0.3,
                              f = post_marginal_theta)

dense_theta <- eval_grid(uniform = TRUE,
                         K = 500, min = 0.001, max = 0.3,
                         f = post_marginal_theta)
```

```{r fig.height = 3, fig.cap = "True posterior marginal of theta (blue line) overlaid with the choice of integration points. This naive grid places too many points in regions of the parameter space without much posterior density."}
ggplot(dense_theta) +
  geom_line(aes(x = input, y = output), col = midblue) +
  geom_point(data = blangiardo_theta, aes(x = input, y = output), shape = 4) +
  theme(axis.text.y=element_blank()) +
  labs(title = "Blangiardo integration points for the hyperparameters")
```

Optimisation on the log scale (to avoid constrained optimisation):

```{r}
its <- 1000
r_nlminb <- nlminb(start = 0,
                   objective = nl_post_marginal_theta,
                   log_input = TRUE,
                   control = list(iter.max = its, trace = 0))

r_optim <- optim(par = 0,
                 fn = nl_post_marginal_theta,
                 log_input = TRUE,
                 method = "Brent",
                 lower = -100, # Have to specify upper and lower bounds
                 upper = 100, # when using optimize (1D function)
                 control = list(maxit = its, trace = 0))

c(r_nlminb$par, r_optim$par) # The same
r_opt <- r_nlminb
```

For INLA's grid strategy, starting from the mode, we take steps of size $\delta_z$ checking that each point meets the criteria
\begin{equation}
  \log \tilde p(\theta(0) \, | \, y) - \log \tilde p(\theta(z) \, | \, y) < \delta_\pi. \label{eq:criteria}
\end{equation}
In the one dimensional case there is no need to do the $z$-parametrisation.
Choosing $\delta_z$ and $\delta_\pi$ based upon manual tuning (how does INLA select these numbers in general?):

```{r}
delta_z <- 0.05
delta_pi <- 3
```

The following is very inefficient R programming but just an idea as to how it could be done:

```{r fig.height = 3, fig.cap = "The INLA method grid points. This looks better but it still looks a little strange probably because of the reparametrisation. Then there is also the task of choosing the values of $\\delta_z$ and $\\delta_\\pi$."}
# Increasing
points <- c(r_opt$par) # On the log theta scale

i <- 0
condition <- TRUE
while(condition) {
  i <- i + 1
  proposal <- r_opt$par + i * delta_z
  statistic <- nl_post_marginal_theta(theta = proposal, log_input = TRUE) -
               nl_post_marginal_theta(theta = r_opt$par, log_input = TRUE)
  condition <- (statistic < delta_pi)
  if(condition){
    points <- c(points, proposal)
  }
}

# Decreasing
i <- 0
condition <- TRUE
while(condition) {
  i <- i + 1
  proposal <- r_opt$par - i * delta_z
  statistic <- nl_post_marginal_theta(theta = proposal, log_input = TRUE) -
               nl_post_marginal_theta(theta = r_opt$par, log_input = TRUE)
  condition <- (statistic < delta_pi)
  if(condition){
    points <- c(points, proposal)
  }
}

inla_theta <- eval_grid(exp(points), f = post_marginal_theta)

ggplot(dense_theta) +
  geom_line(aes(x = input, y = output), col = midblue) +
  geom_point(data = inla_theta, aes(x = input, y = output), shape = 4) +
  theme(axis.text.y=element_blank()) +
  labs(title = "INLA integration points for the hyperparameters") +
  annotate("text", x = 0.175, y = 5e-41,
           label = "Now the points are concentrated \n where there is higher density")
```

One of the INLA vignettes discusses how the user can set their own integration points--see `browseVignettes(package="INLA")`.
The integration points that INLA uses can be found using `fit$joint.hyper`, which we can add on to the above plot:

```{r}
internals_inla_theta <- data.frame(
  input = exp(fit$joint.hyper$`Log precision for the Gaussian observations`),
  output = exp(fit$joint.hyper$`Log posterior density`)
)

# Different normalising constant?
ggplot(dense_theta) +
  geom_line(aes(x = input, y = output), col = midblue) +
  geom_point(data = internals_inla_theta, aes(x = input, y = output), shape = 4) +
  theme(axis.text.y=element_blank()) +
  labs(title = "INLA integration points for the hyperparameters") +
  annotate("text", x = 0.175, y = 5e-41,
           label = "Now the points are concentrated \n where there is higher density")
```

We take the set of points defined by `inla_theta` to be our $\{\theta^{(k)}\}$.
$K$ is given by `length(points)` which equals `r length(points)`.

Define a function `nl_joint_post` by taking the negative log of the joint posterior $p(x, \theta \, | \, y) = p(x \, | \, \theta, y) p(\theta \, | \, y)$ to give
\begin{equation}
- \log p(x, \theta \, | \, y) = - \log p(x \, | \, \theta, y) - \log p(\theta \, | \, y).
\end{equation}

```{r}
nl_joint_post <- function(x, theta, log_input = FALSE) {
  nl_full_conditional_x(x, theta, log_input) +
  nl_post_marginal_theta(theta, log_input)
}
```

For any given input $x$ you can do quadrature according to
\begin{equation}
  \tilde p(x \, | \, y) =
  \sum_{k = 1}^K p(x \, | \, \theta^{(k)}, y) \times p(\theta^{(k)} \, | \, y) \times \Delta^{(k)},
\end{equation}
but how should the range of $x$ be chosen?
Blangiardo again use a naive grid for demonstration:

```{r}
blangiardo_x <- seq(-8, 5, length.out = 50)
```

Modify `nl_joint_post` to accept and return vectors using the base R function `Vectorize`, then apply the outer product:

```{r}
v_nl_joint_post <- Vectorize(nl_joint_post, vectorize.args = c("x", "theta"))
nl_joint_post <- outer(blangiardo_x, points, v_nl_joint_post, log_input = TRUE)
```

50 rows (the number of $x$ integration points) and 26 columns (the number of $\theta$ integration points):

```{r}
dim(nl_joint_post)
```

The $x$ and $y$-axis do not correspond to values, just to indicies:

```{r}
image(nl_joint_post)
```

Following the book:

```{r}
nl_post_marginal_x <- rowSums(nl_joint_post)
plot(blangiardo_x, nl_post_marginal_x)
blangiardo_x[[which.min(nl_post_marginal_x)]]
```

## `TMB`

C++ for the negative log joint posterior:

```{cpp, code=readLines("blangiardo.cpp"), eval=FALSE}
```

```{r results=FALSE}
TMB::compile("blangiardo.cpp")
dyn.load(dynlib("blangiardo"))

param <- list(x = 0, l_theta = 0)
```

`h` with Laplace approximation is a function of $\dim(\theta)$ inputs, integrating out $x$ by specifying `random = c("x")`:

```{r}
h <- MakeADFun(data = dat,
               parameters = param,
               random = c("x"),
               DLL = "blangiardo",
               hessian = TRUE)
```

`f` without Laplace approximation is a function of $\dim(\theta) + \dim(x)$ inputs:

```{r}
f <- MakeADFun(data = dat,
               parameters = param,
               DLL = "blangiardo",
               hessian = TRUE)
```

Optimisation using `nlminb` and `optim` gets the same results:

```{r}
its <- 1000

tmb_nlminb <- nlminb(start = h$par,
                     objective = h$fn,
                     gradient = h$gr,
                     control = list(iter.max = its, trace = 0))

tmb_optim <- optim(par = h$par,
                   fn = h$fn,
                   gr = h$gn,
                   method = "Brent",
                   lower = -100, # Have to specify upper and lower bounds
                   upper = 100, # when using optimize (1D function)
                   control = list(maxit = its, trace = 0))

c(tmb_optim$par, tmb_nlminb$par) # The same

tmb_opt <- tmb_nlminb

sd_out <- sdreport(h, par.fixed = tmb_opt$par, getJointPrecision = TRUE)
```

The mode and comparison (I started by making the mistake of exponentiating `fit$summary.hyperpar$mode` rather than the internal version--on the log scale, as with R and `TMB`--which, because the transformation is non-linear, is not the same).

```{r}
kable_data <- data.frame(c(exp(fit$internal.summary.hyperpar$mode),
                           exp(r_opt$par),
                           exp(tmb_opt$par)))

rownames(kable_data) <- c("$\\texttt{R-INLA}$", "R", "$\\texttt{TMB}$")
colnames(kable_data) <- c("Posterior mode")

kableExtra::kable(kable_data, booktabs = TRUE, escape = FALSE, align = "c")
```

Compare the variances for $\log(\theta)$:

```{r}
sd_out$cov.fixed # TMB
fit$internal.summary.hyperpar$sd^2 # INLA
```

Compare $x$:

```{r}
sd_out$par.random # TMB
fit$summary.fixed # INLA

plot(fit$marginals.fixed$`(Intercept)`, type = "l")
abline(v = sd_out$par.random)
```
